{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2_burguers_inf.ipynb",
      "provenance": [],
      "mount_file_id": "1p2QAEGT6uVtKq9w6GhZ38yd0VgsREGyb",
      "authorship_tag": "ABX9TyO5m0tTL9hqgbuVX6KjpXjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloacs/DeepCFD/blob/main/Navier_inf_tf2.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDPHl8PZHsbz"
      },
      "source": [
        "# Installing **latex** & pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m11J46MtBwJO"
      },
      "source": [
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended  \n",
        "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip \n",
        "! unzip type1cm.zip -d /tmp/type1cm \n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm \n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm \n",
        "! sudo texhash \n",
        "!apt install cm-super\n",
        "pip install --upgrade pyDOE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgDvEO_7HnSn"
      },
      "source": [
        "# FIGURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfAE7ns_ORH"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "def saveResultDir(save_path, save_hp):\n",
        "    now = datetime.now()\n",
        "    scriptName =  os.path.splitext(os.path.basename(sys.argv[0]))[0]\n",
        "    resDir = os.path.join(save_path, \"results\", f\"{now.strftime('%Y%m%d-%H%M%S')}-{scriptName}\")\n",
        "    os.mkdir(resDir)\n",
        "    print(\"Saving results to directory \", resDir)\n",
        "    savefig(os.path.join(resDir, \"graph\"))\n",
        "    with open(os.path.join(resDir, \"hp.json\"), \"w\") as f:\n",
        "        json.dump(save_hp, f)\n",
        "\n",
        "\n",
        "# MIT License\n",
        "# \n",
        "# Copyright (c) 2018 maziarraissi\n",
        "# \n",
        "# https://github.com/maziarraissi/PINNs\n",
        "\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 10,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots))\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def savefig(filename, crop = True):\n",
        "    if crop == True:\n",
        "#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        # plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.png'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "#        plt.savefig('{}.pgf'.format(filename))\n",
        "        plt.savefig('{}.pdf'.format(filename))\n",
        "        # plt.savefig('{}.eps'.format(filename))\n",
        "        plt.savefig('{}.png'.format(filename))\n",
        "\n",
        "## Simple plot\n",
        "#fig, ax  = newfig(1.0)\n",
        "#\n",
        "#def ema(y, a):\n",
        "#    s = []\n",
        "#    s.append(y[0])\n",
        "#    for t in range(1, len(y)):\n",
        "#        s.append(a * y[t] + (1-a) * s[t-1])\n",
        "#    return np.array(s)\n",
        "#    \n",
        "#y = [0]*200\n",
        "#y.extend([20]*(1000-len(y)))\n",
        "#s = ema(y, 0.01)\n",
        "#\n",
        "#ax.plot(s)\n",
        "#ax.set_xlabel('X Label')\n",
        "#ax.set_ylabel('EMA')\n",
        "#\n",
        "#savefig('ema')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W7M8nkHgi8"
      },
      "source": [
        "# INFORMATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njniDEGr77xp"
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, hp):\n",
        "        print(\"Hyperparameters:\")\n",
        "        print(json.dumps(hp, indent=2))\n",
        "        print()\n",
        "\n",
        "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.prev_time = self.start_time\n",
        "        self.frequency = hp[\"log_frequency\"]\n",
        "\n",
        "    def get_epoch_duration(self):\n",
        "        now = time.time()\n",
        "        edur = datetime.fromtimestamp(now - self.prev_time) \\\n",
        "            .strftime(\"%S.%f\")[:-5]\n",
        "        self.prev_time = now\n",
        "        return edur\n",
        "\n",
        "    def get_elapsed(self):\n",
        "        return datetime.fromtimestamp(time.time() - self.start_time) \\\n",
        "                .strftime(\"%M:%S\")\n",
        "\n",
        "    def get_error_u(self):\n",
        "        return self.error_fn()\n",
        "\n",
        "    def set_error_fn(self, error_fn):\n",
        "        self.error_fn = error_fn\n",
        "\n",
        "    def log_train_start(self, model, model_description=False):\n",
        "        print(\"\\nTraining started\")\n",
        "        print(\"================\")\n",
        "        self.model = model\n",
        "        if model_description:\n",
        "            print(model.summary())\n",
        "\n",
        "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "        if epoch % self.frequency == 0:\n",
        "            name = 'nt_epoch' if is_iter else 'tf_epoch'\n",
        "            print(f\"{name} = {epoch:6d}  \" +\n",
        "                  f\"elapsed = {self.get_elapsed()} \" +\n",
        "                  f\"(+{self.get_epoch_duration()})  \" +\n",
        "                  f\"loss = {loss:.4e}  \" + custom)\n",
        "\n",
        "    def log_train_opt(self, name):\n",
        "        print(f\"-- Starting {name} optimization --\")\n",
        "\n",
        "    def log_train_end(self, epoch, custom=\"\"):\n",
        "        print(\"==================\")\n",
        "        print(f\"Training finished (epoch {epoch}): \" +\n",
        "              f\"duration = {self.get_elapsed()}  \" +\n",
        "              f\"error = {self.get_error_u():.4e}  \" + custom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CblmTR6zHOgB"
      },
      "source": [
        "Defining the costum NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI14rDnv78rH"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, hp, logger, ub, lb):\n",
        "\n",
        "        layers = hp[\"layers\"]\n",
        "\n",
        "        # Setting up the optimizers with the hyper-parameters\n",
        "        self.nt_config = Struct()\n",
        "        self.nt_config.learningRate = hp[\"nt_lr\"]\n",
        "        self.nt_config.maxIter = hp[\"nt_epochs\"]\n",
        "        self.nt_config.nCorrection = hp[\"nt_ncorr\"]\n",
        "        self.nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
        "        self.tf_epochs = hp[\"tf_epochs\"]\n",
        "        self.tf_optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp[\"tf_lr\"],\n",
        "            beta_1=hp[\"tf_b1\"],\n",
        "            epsilon=hp[\"tf_eps\"])\n",
        "\n",
        "        self.dtype = \"float64\"\n",
        "        # Descriptive Keras model\n",
        "        tf.keras.backend.set_floatx(self.dtype)\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "        self.model.add(tf.keras.layers.Lambda(\n",
        "            lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "        for width in layers[1:-1]:\n",
        "            self.model.add(tf.keras.layers.Dense(\n",
        "                width, activation=tf.nn.tanh,\n",
        "                kernel_initializer=\"glorot_normal\"))\n",
        "        self.model.add(tf.keras.layers.Dense(\n",
        "                layers[-1], activation=None,\n",
        "                kernel_initializer=\"glorot_normal\"))\n",
        "\n",
        "        # Computing the sizes of weights/biases for future decomposition\n",
        "        self.sizes_w = []\n",
        "        self.sizes_b = []\n",
        "        for i, width in enumerate(layers):\n",
        "            if i != 1:\n",
        "                self.sizes_w.append(int(width * layers[1]))\n",
        "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "        self.logger = logger\n",
        "\n",
        "    # Defining custom loss\n",
        "    # @tf.function\n",
        "    def loss(self, u, u_pred):\n",
        "        return tf.reduce_mean(tf.square(u - u_pred))\n",
        "\n",
        "    # @tf.function\n",
        "    def grad(self, X, u):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = self.loss(u, self.model(X))\n",
        "        grads = tape.gradient(loss_value, self.wrap_training_variables())\n",
        "        return loss_value, grads\n",
        "\n",
        "    def wrap_training_variables(self):\n",
        "        var = self.model.trainable_variables\n",
        "        return var\n",
        "\n",
        "    def get_params(self, numpy=False):\n",
        "        return []\n",
        "\n",
        "    def get_weights(self, convert_to_tensor=True):\n",
        "        w = []\n",
        "        for layer in self.model.layers[1:]:\n",
        "            weights_biases = layer.get_weights()\n",
        "            weights = weights_biases[0].flatten()\n",
        "            biases = weights_biases[1]\n",
        "            w.extend(weights)\n",
        "            w.extend(biases)\n",
        "        if convert_to_tensor:\n",
        "            w = self.tensor(w)\n",
        "        return w\n",
        "\n",
        "    def set_weights(self, w):\n",
        "        for i, layer in enumerate(self.model.layers[1:]):\n",
        "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "            weights = w[start_weights:end_weights]\n",
        "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "            weights_biases = [weights, biases]\n",
        "            layer.set_weights(weights_biases)\n",
        "\n",
        "    def get_loss_and_flat_grad(self, X, u):\n",
        "        def loss_and_flat_grad(w):\n",
        "            with tf.GradientTape() as tape:\n",
        "                self.set_weights(w)\n",
        "                loss_value = self.loss(u, self.model(X))\n",
        "            grad = tape.gradient(loss_value, self.wrap_training_variables())\n",
        "            grad_flat = []\n",
        "            for g in grad:\n",
        "                grad_flat.append(tf.reshape(g, [-1]))\n",
        "            grad_flat = tf.concat(grad_flat, 0)\n",
        "            return loss_value, grad_flat\n",
        "\n",
        "        return loss_and_flat_grad\n",
        "\n",
        "    def tf_optimization(self, X_u, u):\n",
        "        self.logger.log_train_opt(\"Adam\")\n",
        "        for epoch in range(self.tf_epochs):\n",
        "            loss_value = self.tf_optimization_step(X_u, u)\n",
        "            self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    # @tf.function\n",
        "    def tf_optimization_step(self, X_u, u):\n",
        "        loss_value, grads = self.grad(X_u, u)\n",
        "        self.tf_optimizer.apply_gradients(\n",
        "                zip(grads, self.wrap_training_variables()))\n",
        "        return loss_value\n",
        "\n",
        "    def nt_optimization(self, X_u, u):\n",
        "        self.logger.log_train_opt(\"LBFGS\")\n",
        "        loss_and_flat_grad = self.get_loss_and_flat_grad(X_u, u)\n",
        "        # tfp.optimizer.lbfgs_minimize(\n",
        "        #   loss_and_flat_grad,\n",
        "        #   initial_position=self.get_weights(),\n",
        "        #   num_correction_pairs=nt_config.nCorrection,\n",
        "        #   max_iterations=nt_config.maxIter,\n",
        "        #   f_relative_tolerance=nt_config.tolFun,\n",
        "        #   tolerance=nt_config.tolFun,\n",
        "        #   parallel_iterations=6)\n",
        "        self.nt_optimization_steps(loss_and_flat_grad)\n",
        "\n",
        "    def nt_optimization_steps(self, loss_and_flat_grad):\n",
        "        lbfgs(loss_and_flat_grad,\n",
        "              self.get_weights(),\n",
        "              self.nt_config, Struct(), True,\n",
        "              lambda epoch, loss, is_iter:\n",
        "              self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    def fit(self, X_u, u):\n",
        "        self.logger.log_train_start(self)\n",
        "\n",
        "        # Creating the tensors\n",
        "        X_u = self.tensor(X_u)\n",
        "        u = self.tensor(u)\n",
        "\n",
        "        # Optimizing\n",
        "        self.tf_optimization(X_u, u)\n",
        "        self.nt_optimization(X_u, u)\n",
        "\n",
        "        self.logger.log_train_end(self.tf_epochs + self.nt_config.maxIter)\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        u_pred = self.model(X_star)\n",
        "        return u_pred.numpy()\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def tensor(self, X):\n",
        "        return tf.convert_to_tensor(X, dtype=self.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FX1xijuHI4W"
      },
      "source": [
        "## Defining **lbfgs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PRe016z_GqJ"
      },
      "source": [
        "#%% Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ET5AgMQGkyR"
      },
      "source": [
        "# Utilities:\n",
        "\n",
        "\n",
        "1.   Prepare data function\n",
        "2.   Plotting function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yYBxFmN8g58"
      },
      "source": [
        "#%% Utils for Burger's equation\n",
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
        "\n",
        "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    # x = np.load(\"1d-burgers/data/burgers_x.npy\")[:, None]\n",
        "    # t = np.load(\"1d-burgers/data/burgers_t.npy\")[:, None]\n",
        "    # Exact_u = np.load(\"1d-burgers/data/burgers_u.npy\").T\n",
        "\n",
        "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
        "      dt = t[idx_t_1] - t[idx_t_0]\n",
        "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "        \n",
        "      # Boudanry data\n",
        "      x_1 = np.vstack((lb, ub))\n",
        "      \n",
        "      # Test data\n",
        "      x_star = x\n",
        "      u_star = Exact_u[idx_t_1,:]\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
        "      IRK_times = tmp[q**2+q:]\n",
        "\n",
        "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    if N_0 != None and N_1 != None:\n",
        "      Exact_u = Exact_u.T\n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "          \n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
        "      x_1 = x[idx_x,:]\n",
        "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
        "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
        "      \n",
        "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
        "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
        "      IRK_alpha = weights[0:-1,:]\n",
        "      IRK_beta = weights[-1:,:] \n",
        "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
        "\n",
        "    if N_f == None:\n",
        "      lb = X_star.min(axis=0)\n",
        "      ub = X_star.max(axis=0) \n",
        "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=-1) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "    # Getting the highest boundary conditions (x=1) \n",
        "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "    uu3 = Exact_u[:,-1:]\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "    u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, save_path=None, save_hp=None):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "\n",
        "  # Creating the figures\n",
        "  fig, ax = newfig(1.0, 1.1)\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 3)\n",
        "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])    \n",
        "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "\n",
        "  if save_path != None and save_hp != None:\n",
        "      saveResultDir(save_path, save_hp)\n",
        "\n",
        "  else:\n",
        "    plt.show()\n",
        "\n",
        "def plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, save_path=None, save_hp=None):\n",
        "  fig, ax = newfig(1.0, 1.2)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  ####### Row 0: h(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/2 + 0.1, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "  \n",
        "  h = ax.imshow(Exact_u.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x_star.min(), x_star.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "      \n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  leg = ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  \n",
        "  ####### Row 1: h(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/2-0.05, bottom=0.15, left=0.15, right=0.85, wspace=0.5)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[idx_t_0,:], 'b-', linewidth = 2) \n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_0]), fontsize = 10)\n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.8, -0.3), ncol=2, frameon=False)\n",
        "\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x, Exact_u[idx_t_1,:], 'b-', linewidth = 2, label = 'Exact') \n",
        "  ax.plot(x_star, u_1_pred, 'r--', linewidth = 2, label = 'Prediction')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_1]), fontsize = 10)    \n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  \n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.1, -0.3), ncol=2, frameon=False)\n",
        "\n",
        "  if save_path != None and save_hp != None:\n",
        "      saveResultDir(save_path, save_hp)\n",
        "\n",
        "  else:\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "  ub, lb, u_1_pred, Exact, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy,\n",
        "  x, t, save_path=None, save_hp=None):  \n",
        "  fig, ax = newfig(1.0, 1.5)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3+0.05, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "      \n",
        "  h = ax.imshow(Exact, interpolation='nearest', cmap='rainbow',\n",
        "                extent=[t_star.min(),t_star.max(), lb[0], ub[0]],\n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "  \n",
        "  line = np.linspace(x_star.min(), x_star.max(), 2)[:,None]\n",
        "  ax.plot(t_star[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1.0)\n",
        "  ax.plot(t_star[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1.0)    \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/3-0.1, bottom=1-2/3, left=0.15, right=0.85, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x_star,Exact[:,idx_t_0][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_0], u_0.shape[0]), fontsize = 10)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x_star,Exact[:,idx_t_1][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_1, u_1, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_1], u_1.shape[0]), fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(-0.3, -0.3), ncol=2, frameon=False)\n",
        "  \n",
        "  gs2 = gridspec.GridSpec(1, 2)\n",
        "  gs2.update(top=1-2/3-0.05, bottom=0, left=0.15, right=0.85, wspace=0.0)\n",
        "  \n",
        "  ax = plt.subplot(gs2[0, 0])\n",
        "  ax.axis('off')\n",
        "  nu = 0.01/np.pi\n",
        "  s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x + %.6f u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & ' % (nu)\n",
        "  s2 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "  s3 = r'Identified PDE (1\\% noise) & '\n",
        "  s4 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "  s5 = r'\\end{tabular}$'\n",
        "  s = s1+s2+s3+s4+s5\n",
        "  ax.text(-0.1,0.2,s)\n",
        "\n",
        "  if save_path != None and save_hp != None:\n",
        "      saveResultDir(save_path, save_hp)\n",
        "\n",
        "  else:\n",
        "    plt.show()\n",
        "\n",
        "def plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy):\n",
        "    fig, ax = newfig(1.0, 1.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    \n",
        "    ####### Row 0: u(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')    \n",
        "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])    \n",
        "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 3: Identified PDE ##################    \n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "    ax = plt.subplot(gs2[:, :])\n",
        "    ax.axis('off')\n",
        "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "    s3 = r'Identified PDE (1\\% noise) & '\n",
        "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "    s5 = r'\\end{tabular}$'\n",
        "    s = s1+s2+s3+s4+s5\n",
        "    ax.text(0.1,0.1,s)\n",
        "    plt.show()\n",
        "    savefig('./content/Burgers_identification')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2CZd9yt_rot"
      },
      "source": [
        "# RUnning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5uz8YCt1hn2O",
        "outputId": "b91ea54a-7779-45a5-e902-a1874d5b4a11"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "# HYPER PARAMETERS\n",
        "\n",
        "hp = {}\n",
        "# Data size on the solution u\n",
        "hp[\"N_u\"] = 100\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "hp[\"N_f\"] = 10000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "hp[\"layers\"] = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "hp[\"tf_epochs\"] = 120\n",
        "hp[\"tf_lr\"] = 0.03\n",
        "hp[\"tf_b1\"] = 0.9\n",
        "hp[\"tf_eps\"] = None\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "hp[\"nt_epochs\"] = 1000\n",
        "hp[\"nt_lr\"] = 0.6\n",
        "hp[\"nt_ncorr\"] = 50\n",
        "hp[\"log_frequency\"] = 10\n",
        "\n",
        "# %% DEFINING THE MODEL\n",
        "\n",
        "\n",
        "class BurgersInformedNN(NeuralNetwork):\n",
        "    def __init__(self, hp, logger, X_f, ub, lb, nu):\n",
        "        super().__init__(hp, logger, ub, lb)\n",
        "\n",
        "        self.nu = nu\n",
        "\n",
        "        # Separating the collocation coordinates\n",
        "        self.x_f = self.tensor(X_f[:, 0:1])\n",
        "        self.t_f = self.tensor(X_f[:, 1:2])\n",
        "\n",
        "    # Defining custom loss\n",
        "    def loss(self, u, u_pred):\n",
        "        f_pred = self.f_model()\n",
        "        return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "            tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "    # The actual PINN\n",
        "    def f_model(self):\n",
        "        # Using the new GradientTape paradigm of TF2.0,\n",
        "        # which keeps track of operations to get the gradient at runtime\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watching the two inputs we’ll need later, x and t\n",
        "            tape.watch(self.x_f)\n",
        "            tape.watch(self.t_f)\n",
        "            # Packing together the inputs\n",
        "            X_f = tf.stack([self.x_f[:, 0], self.t_f[:, 0]], axis=1)\n",
        "\n",
        "            # Getting the prediction\n",
        "            u = self.model(X_f)\n",
        "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "            u_x = tape.gradient(u, self.x_f)\n",
        "\n",
        "        # Getting the other derivatives\n",
        "        u_xx = tape.gradient(u_x, self.x_f)\n",
        "        u_t = tape.gradient(u, self.t_f)\n",
        "\n",
        "        # Letting the tape go\n",
        "        del tape\n",
        "\n",
        "        nu = self.get_params(numpy=True)\n",
        "\n",
        "        # Buidling the PINNs\n",
        "        return u_t + u*u_x - nu*u_xx\n",
        "\n",
        "    def get_params(self, numpy=False):\n",
        "        return self.nu\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        u_star = self.model(X_star)\n",
        "        f_star = self.f_model()\n",
        "        return u_star.numpy(), f_star.numpy()\n",
        "\n",
        "# %% TRAINING THE MODEL\n",
        "\n",
        "\n",
        "# Getting the data\n",
        "path = os.path.join(\"/content/drive/MyDrive/PINNs-TF2.0/1d-burgers/data\", \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "    X_u_train, u_train, X_f, ub, lb = prep_data(\n",
        "        path, hp[\"N_u\"], hp[\"N_f\"], noise=0.0)\n",
        "\n",
        "# Creating the model\n",
        "logger = Logger(hp)\n",
        "pinn = BurgersInformedNN(hp, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
        "\n",
        "# Defining the error function for the logger and training\n",
        "def error():\n",
        "    u_pred, _ = pinn.predict(X_star)\n",
        "    return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train)\n",
        "\n",
        "# Getting the model predictions\n",
        "u_pred, _ = pinn.predict(X_star)\n",
        "\n",
        "\n",
        "# %% PLOTTING\n",
        "plot_inf_cont_results(X_star, u_pred.flatten(), X_u_train, u_train,\n",
        "                      Exact_u, X, T, x, t)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "{\n",
            "  \"N_u\": 100,\n",
            "  \"N_f\": 10000,\n",
            "  \"layers\": [\n",
            "    2,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    1\n",
            "  ],\n",
            "  \"tf_epochs\": 120,\n",
            "  \"tf_lr\": 0.03,\n",
            "  \"tf_b1\": 0.9,\n",
            "  \"tf_eps\": null,\n",
            "  \"nt_epochs\": 1000,\n",
            "  \"nt_lr\": 0.6,\n",
            "  \"nt_ncorr\": 50,\n",
            "  \"log_frequency\": 10\n",
            "}\n",
            "\n",
            "TensorFlow version: 2.4.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "-- Starting Adam optimization --\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:00 (+00.1)  loss = 2.4713e-01  \n",
            "tf_epoch =     10  elapsed = 00:00 (+00.6)  loss = 2.6044e-01  \n",
            "tf_epoch =     20  elapsed = 00:01 (+00.6)  loss = 1.6198e-01  \n",
            "tf_epoch =     30  elapsed = 00:02 (+00.6)  loss = 1.2458e-01  \n",
            "tf_epoch =     40  elapsed = 00:02 (+00.6)  loss = 1.0602e-01  \n",
            "tf_epoch =     50  elapsed = 00:03 (+00.6)  loss = 8.9587e-02  \n",
            "tf_epoch =     60  elapsed = 00:04 (+00.6)  loss = 7.5692e-02  \n",
            "tf_epoch =     70  elapsed = 00:04 (+00.6)  loss = 6.7965e-02  \n",
            "tf_epoch =     80  elapsed = 00:05 (+00.6)  loss = 6.2576e-02  \n",
            "tf_epoch =     90  elapsed = 00:06 (+00.6)  loss = 6.0161e-02  \n",
            "tf_epoch =    100  elapsed = 00:06 (+00.6)  loss = 9.9555e-02  \n",
            "tf_epoch =    110  elapsed = 00:07 (+00.6)  loss = 8.0400e-02  \n",
            "-- Starting LBFGS optimization --\n",
            "nt_epoch =     10  elapsed = 00:09 (+01.5)  loss = 5.7541e-02  \n",
            "nt_epoch =     20  elapsed = 00:10 (+00.9)  loss = 5.5123e-02  \n",
            "nt_epoch =     30  elapsed = 00:11 (+00.9)  loss = 4.9098e-02  \n",
            "nt_epoch =     40  elapsed = 00:12 (+01.0)  loss = 4.4087e-02  \n",
            "nt_epoch =     50  elapsed = 00:13 (+01.1)  loss = 4.0265e-02  \n",
            "nt_epoch =     60  elapsed = 00:14 (+01.2)  loss = 3.7140e-02  \n",
            "nt_epoch =     70  elapsed = 00:15 (+01.1)  loss = 3.4224e-02  \n",
            "nt_epoch =     80  elapsed = 00:16 (+01.2)  loss = 3.1015e-02  \n",
            "nt_epoch =     90  elapsed = 00:18 (+01.2)  loss = 2.7645e-02  \n",
            "nt_epoch =    100  elapsed = 00:19 (+01.1)  loss = 2.6141e-02  \n",
            "nt_epoch =    110  elapsed = 00:20 (+01.1)  loss = 2.4054e-02  \n",
            "nt_epoch =    120  elapsed = 00:21 (+01.2)  loss = 2.1925e-02  \n",
            "nt_epoch =    130  elapsed = 00:22 (+01.1)  loss = 1.8096e-02  \n",
            "nt_epoch =    140  elapsed = 00:24 (+01.2)  loss = 1.5799e-02  \n",
            "nt_epoch =    150  elapsed = 00:25 (+01.2)  loss = 1.4138e-02  \n",
            "nt_epoch =    160  elapsed = 00:26 (+01.1)  loss = 1.1496e-02  \n",
            "nt_epoch =    170  elapsed = 00:27 (+01.1)  loss = 9.5007e-03  \n",
            "nt_epoch =    180  elapsed = 00:28 (+01.1)  loss = 7.7142e-03  \n",
            "nt_epoch =    190  elapsed = 00:30 (+01.2)  loss = 6.6437e-03  \n",
            "nt_epoch =    200  elapsed = 00:31 (+01.1)  loss = 5.6255e-03  \n",
            "nt_epoch =    210  elapsed = 00:32 (+01.1)  loss = 4.8068e-03  \n",
            "nt_epoch =    220  elapsed = 00:33 (+01.1)  loss = 4.0847e-03  \n",
            "nt_epoch =    230  elapsed = 00:34 (+01.1)  loss = 3.4518e-03  \n",
            "nt_epoch =    240  elapsed = 00:36 (+01.1)  loss = 3.1420e-03  \n",
            "nt_epoch =    250  elapsed = 00:37 (+01.2)  loss = 2.8294e-03  \n",
            "nt_epoch =    260  elapsed = 00:38 (+01.2)  loss = 2.5882e-03  \n",
            "nt_epoch =    270  elapsed = 00:39 (+01.1)  loss = 2.4101e-03  \n",
            "nt_epoch =    280  elapsed = 00:40 (+01.1)  loss = 2.1300e-03  \n",
            "nt_epoch =    290  elapsed = 00:41 (+01.1)  loss = 2.0670e-03  \n",
            "nt_epoch =    300  elapsed = 00:43 (+01.1)  loss = 2.0208e-03  \n",
            "nt_epoch =    310  elapsed = 00:44 (+01.1)  loss = 1.9409e-03  \n",
            "nt_epoch =    320  elapsed = 00:45 (+01.1)  loss = 1.8888e-03  \n",
            "nt_epoch =    330  elapsed = 00:46 (+01.1)  loss = 1.8413e-03  \n",
            "nt_epoch =    340  elapsed = 00:47 (+01.1)  loss = 1.9741e-03  \n",
            "nt_epoch =    350  elapsed = 00:49 (+01.2)  loss = 1.7790e-03  \n",
            "nt_epoch =    360  elapsed = 00:50 (+01.1)  loss = 1.7027e-03  \n",
            "nt_epoch =    370  elapsed = 00:51 (+01.1)  loss = 1.5875e-03  \n",
            "nt_epoch =    380  elapsed = 00:52 (+01.1)  loss = 1.4426e-03  \n",
            "nt_epoch =    390  elapsed = 00:53 (+01.1)  loss = 1.3688e-03  \n",
            "nt_epoch =    400  elapsed = 00:55 (+01.1)  loss = 1.3065e-03  \n",
            "nt_epoch =    410  elapsed = 00:56 (+01.1)  loss = 1.2554e-03  \n",
            "nt_epoch =    420  elapsed = 00:57 (+01.1)  loss = 1.2026e-03  \n",
            "nt_epoch =    430  elapsed = 00:58 (+01.1)  loss = 1.1580e-03  \n",
            "nt_epoch =    440  elapsed = 00:59 (+01.1)  loss = 1.1082e-03  \n",
            "nt_epoch =    450  elapsed = 01:00 (+01.1)  loss = 1.0237e-03  \n",
            "nt_epoch =    460  elapsed = 01:02 (+01.1)  loss = 1.0045e-03  \n",
            "nt_epoch =    470  elapsed = 01:03 (+01.1)  loss = 9.4985e-04  \n",
            "nt_epoch =    480  elapsed = 01:04 (+01.1)  loss = 9.2627e-04  \n",
            "nt_epoch =    490  elapsed = 01:05 (+01.1)  loss = 8.7407e-04  \n",
            "nt_epoch =    500  elapsed = 01:06 (+01.1)  loss = 8.3320e-04  \n",
            "nt_epoch =    510  elapsed = 01:08 (+01.1)  loss = 8.1360e-04  \n",
            "nt_epoch =    520  elapsed = 01:09 (+01.1)  loss = 7.7849e-04  \n",
            "nt_epoch =    530  elapsed = 01:10 (+01.1)  loss = 7.4036e-04  \n",
            "nt_epoch =    540  elapsed = 01:11 (+01.1)  loss = 6.9701e-04  \n",
            "nt_epoch =    550  elapsed = 01:12 (+01.1)  loss = 6.6471e-04  \n",
            "nt_epoch =    560  elapsed = 01:13 (+01.1)  loss = 6.3278e-04  \n",
            "nt_epoch =    570  elapsed = 01:15 (+01.2)  loss = 6.1612e-04  \n",
            "nt_epoch =    580  elapsed = 01:16 (+01.1)  loss = 5.7690e-04  \n",
            "nt_epoch =    590  elapsed = 01:17 (+01.1)  loss = 5.5671e-04  \n",
            "nt_epoch =    600  elapsed = 01:18 (+01.1)  loss = 5.4187e-04  \n",
            "nt_epoch =    610  elapsed = 01:19 (+01.1)  loss = 5.2347e-04  \n",
            "nt_epoch =    620  elapsed = 01:20 (+01.1)  loss = 5.0867e-04  \n",
            "nt_epoch =    630  elapsed = 01:22 (+01.1)  loss = 4.9267e-04  \n",
            "nt_epoch =    640  elapsed = 01:23 (+01.1)  loss = 4.7391e-04  \n",
            "nt_epoch =    650  elapsed = 01:24 (+01.1)  loss = 4.6142e-04  \n",
            "nt_epoch =    660  elapsed = 01:25 (+01.1)  loss = 4.4284e-04  \n",
            "nt_epoch =    670  elapsed = 01:26 (+01.1)  loss = 4.3491e-04  \n",
            "nt_epoch =    680  elapsed = 01:28 (+01.1)  loss = 4.2423e-04  \n",
            "nt_epoch =    690  elapsed = 01:29 (+01.1)  loss = 4.1596e-04  \n",
            "nt_epoch =    700  elapsed = 01:30 (+01.1)  loss = 3.9852e-04  \n",
            "nt_epoch =    710  elapsed = 01:31 (+01.1)  loss = 3.7858e-04  \n",
            "nt_epoch =    720  elapsed = 01:32 (+01.1)  loss = 3.7116e-04  \n",
            "nt_epoch =    730  elapsed = 01:33 (+01.1)  loss = 3.6484e-04  \n",
            "nt_epoch =    740  elapsed = 01:35 (+01.1)  loss = 3.5423e-04  \n",
            "nt_epoch =    750  elapsed = 01:36 (+01.1)  loss = 3.4665e-04  \n",
            "nt_epoch =    760  elapsed = 01:37 (+01.1)  loss = 3.3520e-04  \n",
            "nt_epoch =    770  elapsed = 01:38 (+01.1)  loss = 3.2806e-04  \n",
            "nt_epoch =    780  elapsed = 01:39 (+01.1)  loss = 3.2293e-04  \n",
            "nt_epoch =    790  elapsed = 01:40 (+01.1)  loss = 3.1631e-04  \n",
            "nt_epoch =    800  elapsed = 01:42 (+01.1)  loss = 3.1297e-04  \n",
            "nt_epoch =    810  elapsed = 01:43 (+01.1)  loss = 3.0635e-04  \n",
            "nt_epoch =    820  elapsed = 01:44 (+01.1)  loss = 2.9881e-04  \n",
            "nt_epoch =    830  elapsed = 01:45 (+01.2)  loss = 2.9134e-04  \n",
            "nt_epoch =    840  elapsed = 01:46 (+01.1)  loss = 2.8446e-04  \n",
            "nt_epoch =    850  elapsed = 01:48 (+01.1)  loss = 2.7653e-04  \n",
            "nt_epoch =    860  elapsed = 01:49 (+01.1)  loss = 2.7224e-04  \n",
            "nt_epoch =    870  elapsed = 01:50 (+01.1)  loss = 2.6367e-04  \n",
            "nt_epoch =    880  elapsed = 01:51 (+01.1)  loss = 2.6826e-04  \n",
            "nt_epoch =    890  elapsed = 01:52 (+01.1)  loss = 2.5729e-04  \n",
            "nt_epoch =    900  elapsed = 01:54 (+01.1)  loss = 2.5209e-04  \n",
            "nt_epoch =    910  elapsed = 01:55 (+01.1)  loss = 2.4798e-04  \n",
            "nt_epoch =    920  elapsed = 01:56 (+01.1)  loss = 2.4349e-04  \n",
            "nt_epoch =    930  elapsed = 01:57 (+01.1)  loss = 2.3971e-04  \n",
            "nt_epoch =    940  elapsed = 01:58 (+01.1)  loss = 2.3577e-04  \n",
            "nt_epoch =    950  elapsed = 01:59 (+01.1)  loss = 2.3225e-04  \n",
            "nt_epoch =    960  elapsed = 02:01 (+01.1)  loss = 2.3096e-04  \n",
            "nt_epoch =    970  elapsed = 02:02 (+01.1)  loss = 2.2662e-04  \n",
            "nt_epoch =    980  elapsed = 02:03 (+01.1)  loss = 2.2209e-04  \n",
            "nt_epoch =    990  elapsed = 02:04 (+01.1)  loss = 2.2019e-04  \n",
            "==================\n",
            "Training finished (epoch 1120): duration = 02:05  error = 2.1561e-02  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 388.543x264.146 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLL85HUR_otN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}