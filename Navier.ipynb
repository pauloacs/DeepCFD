{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CÃ³pia de Navier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lhEn6qcwbmwV",
        "U9NYRbjDdGlA",
        "ouHFdWPTz0Y4"
      ],
      "mount_file_id": "11fbFTcssPYcEsrqyzGR6ZX2JQv4cVYzp",
      "authorship_tag": "ABX9TyPbr1ofTWUXELPou7bmIj+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloacs/DeepCFD/blob/main/Navier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkp1-UcwRdhH"
      },
      "source": [
        "import contextlib\r\n",
        "import functools\r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import scipy as sp\r\n",
        "from six.moves import urllib\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "\r\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhEn6qcwbmwV"
      },
      "source": [
        "# CUSTOM_lbfgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts-MsMIzbbnB"
      },
      "source": [
        "#%% Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p64VmVabtvg"
      },
      "source": [
        "# NN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcFGFUBrlLDZ"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class NeuralNetwork(object):\r\n",
        "    def __init__(self, hp, logger, ub, lb):\r\n",
        "\r\n",
        "        layers = hp[\"layers\"]\r\n",
        "\r\n",
        "        # Setting up the optimizers with the hyper-parameters\r\n",
        "        # self.nt_config = Struct()\r\n",
        "        # self.nt_config.learningRate = hp[\"nt_lr\"]\r\n",
        "        # self.nt_config.maxIter = hp[\"nt_epochs\"]\r\n",
        "        # self.nt_config.nCorrection = hp[\"nt_ncorr\"]\r\n",
        "        # self.nt_config.tolFun = 1.0 * np.finfo(float).eps\r\n",
        "        self.nt_learningRate = hp[\"nt_lr\"]\r\n",
        "        self.nt_maxIter = hp[\"nt_epochs\"]\r\n",
        "        self.nt_nCorrection = hp[\"nt_ncorr\"]\r\n",
        "        self.nt_tolFun = 1.0 * np.finfo(float).eps\r\n",
        "        self.tf_epochs = hp[\"tf_epochs\"]\r\n",
        "        self.tf_optimizer = tf.keras.optimizers.Adam(\r\n",
        "            learning_rate=hp[\"tf_lr\"],\r\n",
        "            beta_1=hp[\"tf_b1\"],\r\n",
        "            epsilon=hp[\"tf_eps\"])\r\n",
        "\r\n",
        "        self.dtype = \"float64\"\r\n",
        "        # Descriptive Keras model\r\n",
        "        tf.keras.backend.set_floatx(self.dtype)\r\n",
        "        self.model = tf.keras.Sequential()\r\n",
        "        self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\r\n",
        "        self.model.add(tf.keras.layers.Lambda(\r\n",
        "            lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\r\n",
        "        for width in layers[1:-1]:\r\n",
        "            self.model.add(tf.keras.layers.Dense(\r\n",
        "                width, activation=tf.nn.tanh,\r\n",
        "                kernel_initializer=\"glorot_normal\"))\r\n",
        "        self.model.add(tf.keras.layers.Dense(\r\n",
        "                layers[-1], activation=None,\r\n",
        "                kernel_initializer=\"glorot_normal\"))\r\n",
        "\r\n",
        "        # Computing the sizes of weights/biases for future decomposition\r\n",
        "        self.sizes_w = []\r\n",
        "        self.sizes_b = []\r\n",
        "        for i, width in enumerate(layers):\r\n",
        "            if i != 1:\r\n",
        "                self.sizes_w.append(int(width * layers[1]))\r\n",
        "                self.sizes_b.append(int(width if i != 0 else layers[1]))\r\n",
        "\r\n",
        "        self.logger = logger\r\n",
        "\r\n",
        "    # Defining custom loss\r\n",
        "    # @tf.function\r\n",
        "    # def loss(self, X, u, v):\r\n",
        "    #     return tf.reduce_mean(tf.square(u - u_pred)) + tf.reduce_mean(tf.square(v - v_pred))\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def grad(self, X, u, v):\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            loss_value = self.loss(X, u, v)\r\n",
        "        grads = tape.gradient(loss_value, self.wrap_training_variables())\r\n",
        "        return loss_value, grads\r\n",
        "\r\n",
        "    def wrap_training_variables(self):\r\n",
        "        var = self.model.trainable_variables\r\n",
        "        return var\r\n",
        "\r\n",
        "    def get_params(self, numpy=False):\r\n",
        "        return []\r\n",
        "\r\n",
        "    def get_weights(self, convert_to_tensor=True):\r\n",
        "        w = []\r\n",
        "        for layer in self.model.layers[1:]:\r\n",
        "            weights_biases = layer.get_weights()\r\n",
        "            weights = weights_biases[0].flatten()\r\n",
        "            biases = weights_biases[1]\r\n",
        "            w.extend(weights)\r\n",
        "            w.extend(biases)\r\n",
        "        if convert_to_tensor:\r\n",
        "            w = self.tensor(w)\r\n",
        "        return w\r\n",
        "\r\n",
        "    def set_weights(self, w):\r\n",
        "        for i, layer in enumerate(self.model.layers[1:]):\r\n",
        "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\r\n",
        "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\r\n",
        "            weights = w[start_weights:end_weights]\r\n",
        "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\r\n",
        "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\r\n",
        "            biases = w[end_weights:end_weights + self.sizes_b[i]]\r\n",
        "            weights_biases = [weights, biases]\r\n",
        "            layer.set_weights(weights_biases)\r\n",
        "\r\n",
        "    # def make_val_and_grad_fn(value_fn):\r\n",
        "    #   @functools.wraps(value_fn)\r\n",
        "    #   def val_and_grad(w):\r\n",
        "    #     self.set_weights(w)\r\n",
        "    #     return tfp.math.value_and_gradient(value_fn,self.wrap_training_variables())\r\n",
        "    #   return val_and_grad\r\n",
        "\r\n",
        "    def get_loss_and_flat_grad(self, X, u, v):\r\n",
        "        def loss_and_flat_grad(w):\r\n",
        "            with tf.GradientTape() as tape:\r\n",
        "                self.set_weights(w)\r\n",
        "                loss_value = self.loss(X, u, v)\r\n",
        "            grad = tape.gradient(loss_value, self.wrap_training_variables())\r\n",
        "            grad_flat = []\r\n",
        "            for g in grad:\r\n",
        "                grad_flat.append(tf.reshape(g, [-1]))\r\n",
        "            grad_flat = tf.concat(grad_flat, 0)\r\n",
        "            return loss_value, grad_flat\r\n",
        "\r\n",
        "        return loss_and_flat_grad\r\n",
        "\r\n",
        "    \r\n",
        "    def tf_optimization(self, X_u, u, v):\r\n",
        "        self.logger.log_train_opt(\"Adam\")\r\n",
        "        for epoch in range(self.tf_epochs):\r\n",
        "            loss_value = self.tf_optimization_step(X_u, u, v)\r\n",
        "            self.logger.log_train_epoch(epoch, loss_value)\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def tf_optimization_step(self, X_u, u, v):\r\n",
        "        loss_value, grads = self.grad(X_u, u, v)\r\n",
        "        self.tf_optimizer.apply_gradients(\r\n",
        "                zip(grads, self.wrap_training_variables()))\r\n",
        "        return loss_value\r\n",
        "    \r\n",
        "    #@tf.function\r\n",
        "    def nt_optimization(self, X, u, v):\r\n",
        "        self.logger.log_train_opt(\"LBFGS\")\r\n",
        "        loss_and_flat_grad = self.get_loss_and_flat_grad( X, u, v)\r\n",
        "        tfp.optimizer.lbfgs_minimize(\r\n",
        "          loss_and_flat_grad,\r\n",
        "          initial_position=self.get_weights(),\r\n",
        "          num_correction_pairs=self.nt_nCorrection,\r\n",
        "          max_iterations=self.nt_maxIter,\r\n",
        "          f_relative_tolerance=self.nt_tolFun,\r\n",
        "          tolerance=self.nt_tolFun)\r\n",
        "        self.nt_optimization_steps(loss_and_flat_grad)\r\n",
        "\r\n",
        "    # def nt_optimization_steps(self, loss_and_flat_grad):\r\n",
        "    #     lbfgs(loss_and_flat_grad,\r\n",
        "    #           self.get_weights(),\r\n",
        "    #           self.nt_config, Struct(), True,\r\n",
        "    #           lambda epoch, loss, is_iter:\r\n",
        "    #           self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\r\n",
        "\r\n",
        "    def np_value(tensor):\r\n",
        "      \"\"\"Get numpy value out of possibly nested tuple of tensors.\"\"\"\r\n",
        "      if isinstance(tensor, tuple):\r\n",
        "        return type(tensor)(*(np_value(t) for t in tensor))\r\n",
        "      else:\r\n",
        "        return self.tensor(tensor.numpy())\r\n",
        "\r\n",
        "    def run(optimizer):\r\n",
        "      \"\"\"Run an optimizer and measure it's evaluation time.\"\"\"\r\n",
        "      optimizer()  # Warmup.\r\n",
        "      with timed_execution():\r\n",
        "        result = optimizer()\r\n",
        "      return self.np_value(result)\r\n",
        "\r\n",
        "    def fit(self, X_u, u, v):\r\n",
        "        self.logger.log_train_start(self)\r\n",
        "\r\n",
        "        # Creating the tensors\r\n",
        "        X_u = self.tensor(X_u)\r\n",
        "        u = self.tensor(u)\r\n",
        "        v = self.tensor(v)\r\n",
        "\r\n",
        "        # Optimizing\r\n",
        "        self.tf_optimization(X_u, u, v)\r\n",
        "        self.run(self.nt_optimization(X,u,v))\r\n",
        "        #self.nt_optimization(X_u, u, v)\r\n",
        "\r\n",
        "        self.logger.log_train_end(self.tf_epochs + self.nt_maxIter)\r\n",
        "\r\n",
        "    def predict(self, X_star):\r\n",
        "        u_pred = self.model(X_star)\r\n",
        "        return u_pred.numpy()\r\n",
        "\r\n",
        "    def summary(self):\r\n",
        "        return self.model.summary()\r\n",
        "\r\n",
        "    def tensor(self, X):\r\n",
        "        return tf.convert_to_tensor(X, dtype=self.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9NYRbjDdGlA"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-_yerEMedfH"
      },
      "source": [
        "# Load Data\n",
        "import scipy.io\n",
        "\n",
        "def prep_data(path, N_train):\n",
        "\n",
        "  data = scipy.io.loadmat(path)\n",
        "        \n",
        "  U_star = data['U_star'] # N x 2 x T\n",
        "  P_star = data['p_star'] # N x T\n",
        "  t_star = data['t'] # T x 1\n",
        "  X_star = data['X_star'] # N x 2\n",
        "\n",
        "  N = X_star.shape[0]\n",
        "  T = t_star.shape[0]\n",
        "\n",
        "  # Rearrange Data \n",
        "  XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "  YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "  TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "  UU = U_star[:,0,:] # N x T\n",
        "  VV = U_star[:,1,:] # N x T\n",
        "  PP = P_star # N x T\n",
        "\n",
        "  x = XX.flatten()[:,None] # NT x 1\n",
        "  y = YY.flatten()[:,None] # NT x 1\n",
        "  t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "  u = UU.flatten()[:,None] # NT x 1\n",
        "  v = VV.flatten()[:,None] # NT x 1\n",
        "  p = PP.flatten()[:,None] # NT x 1\n",
        "  X = np.concatenate([x, y, t], 1)\n",
        "        \n",
        "  lb = X.min(0)\n",
        "  ub = X.max(0)\n",
        "\n",
        "  #TRaining DAta\n",
        "  idx = np.random.choice(N*T, N_train, replace=False)\n",
        "  x_train = x[idx,:]\n",
        "  y_train = y[idx,:]\n",
        "  t_train = t[idx,:]\n",
        "  u_train = u[idx,:]\n",
        "  v_train = v[idx,:]\n",
        "  X_train= np.concatenate([x_train, y_train, t_train], 1)\n",
        "\n",
        "  # Boudanry data\n",
        "  x_1 = np.vstack((lb, ub))\n",
        "  \n",
        "  #Test DATA\n",
        "  snap = np.array([100])\n",
        "  x_star = X_star[:,0:1]\n",
        "  y_star = X_star[:,1:2]\n",
        "  t_star = TT[:,snap]\n",
        "\n",
        "  u_star = U_star[:,0,snap]\n",
        "  v_star = U_star[:,1,snap]\n",
        "  p_star = P_star[:,snap]\n",
        "  X_star= np.concatenate([x_train, y_train, t_train], 1)\n",
        "\n",
        "  return X, X_train, u_train , v_train , x_1, X_star, u_star, v_star, p_star, ub, lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6DYzM3GdMfh"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7iYtOUtg89-"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsWWHtwtdAR0"
      },
      "source": [
        "#HYPER PARAMETERS\n",
        "hp = {}\n",
        "# Data size on the solution u\n",
        "hp[\"N_u\"] = 5000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "hp[\"layers\"] = [3, 20, 20, 20, 20, 20, 20, 2]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "hp[\"tf_epochs\"] = 10\n",
        "hp[\"tf_lr\"] = 0.003\n",
        "hp[\"tf_b1\"] = 0.9\n",
        "hp[\"tf_eps\"] = None\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "hp[\"nt_epochs\"] = 10\n",
        "hp[\"nt_lr\"] = 0.8\n",
        "hp[\"nt_ncorr\"] = 50\n",
        "hp[\"log_frequency\"] = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly9bdTk2dV3J"
      },
      "source": [
        "#DEFINE THE MODEL\n",
        "\n",
        "class NavierStokesInformedNN(NeuralNetwork):\n",
        "  def __init__(self, hp, logger, ub, lb):\n",
        "    super().__init__(hp, logger, ub, lb)\n",
        "\n",
        "    # Defining the two additional trainable variables for identification\n",
        "    self.lambda_1 = tf.Variable([1.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([1.0], dtype=self.dtype)\n",
        "\n",
        "  def f_model(self,X):\n",
        "\n",
        "    l1, l2 = self.get_params()\n",
        "    \n",
        "    # Separating the collocation coordinates\n",
        "    x_f = tf.convert_to_tensor(X[:, 0:1], dtype=self.dtype)\n",
        "    y_f = tf.convert_to_tensor(X[:, 1:2], dtype=self.dtype)\n",
        "    t_f = tf.convert_to_tensor(X[:, 2:3], dtype=self.dtype)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      #Watching the 3 inputs\n",
        "      tape.watch(x_f)\n",
        "      tape.watch(y_f)\n",
        "      tape.watch(t_f)\n",
        "      \n",
        "      #Packing the inputs\n",
        "      X_f = tf.stack([x_f[:,0], y_f[:,0], t_f[:,0]], axis=1)\n",
        "\n",
        "      #Getting the prediction\n",
        "      psi_and_p = self.model(X_f)\n",
        "      psi = psi_and_p[:,0:1]\n",
        "      p = psi_and_p[:,1:2]\n",
        "      \n",
        "      #Getting velocities \n",
        "      u = tape.gradient(psi, y_f)\n",
        "      v = tape.gradient(tf.negative(psi), x_f)\n",
        "      \n",
        "      #Deriving inside\n",
        "      u_x = tape.gradient(u, x_f)\n",
        "      u_y = tape.gradient(u, y_f)\n",
        "      v_x = tape.gradient(v, x_f)\n",
        "      v_y = tape.gradient(v, y_f)\n",
        "    \n",
        "    u_t = tape.gradient(u, t_f)\n",
        "    v_t = tape.gradient(v, t_f)\n",
        "\n",
        "    u_xx = tape.gradient(u_x, x_f)\n",
        "    u_yy = tape.gradient(u_y, y_f)\n",
        "    \n",
        "    v_xx = tape.gradient(v_x, x_f)\n",
        "    v_yy = tape.gradient(v_y, y_f)\n",
        "  \n",
        "    p_x = tape.gradient(p, x_f)\n",
        "    p_y = tape.gradient(p, y_f)\n",
        "    \n",
        "    del tape\n",
        "\n",
        "    f_u = u_t + l1*(u*u_x + v*u_y) + p_x - l2*(u_xx + u_yy) \n",
        "    f_v = v_t + l1*(u*v_x + v*v_y) + p_y - l2*(v_xx + v_yy)\n",
        "    \n",
        "    return u, v, p, f_u, f_v\n",
        "\n",
        "  def loss(self,X, u, v):\n",
        "    u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.f_model(self.X)\n",
        "    return tf.reduce_sum(tf.square(u - u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(v - v_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(f_u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(f_v_pred))\n",
        "\n",
        "\n",
        "  def wrap_training_variables(self):\n",
        "    var = self.model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = super().get_weights(convert_to_tensor=False)\n",
        "    w.extend(self.lambda_1.numpy())\n",
        "    w.extend(self.lambda_2.numpy())\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    super().set_weights(w)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = self.lambda_2\n",
        "    if numpy:\n",
        "        return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def fit(self, X, u, v):\n",
        "    self.X =  tf.convert_to_tensor(X, dtype=self.dtype)\n",
        "    super().fit(X, u, v)\n",
        "\n",
        "  def predict(self, X): \n",
        "    u_star, v_star, p_star, _ , _ = self.f_model(X)\n",
        "    return u_star, v_star, p_star\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHFdWPTz0Y4"
      },
      "source": [
        "# LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csR7qBcaz2fV"
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, hp):\n",
        "        print(\"Hyperparameters:\")\n",
        "        print(json.dumps(hp, indent=2))\n",
        "        print()\n",
        "\n",
        "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.prev_time = self.start_time\n",
        "        self.frequency = hp[\"log_frequency\"]\n",
        "\n",
        "    def get_epoch_duration(self):\n",
        "        now = time.time()\n",
        "        edur = datetime.fromtimestamp(now - self.prev_time) \\\n",
        "            .strftime(\"%S.%f\")[:-5]\n",
        "        self.prev_time = now\n",
        "        return edur\n",
        "\n",
        "    def get_elapsed(self):\n",
        "        return datetime.fromtimestamp(time.time() - self.start_time) \\\n",
        "                .strftime(\"%M:%S\")\n",
        "\n",
        "    def get_error_u(self):\n",
        "        return self.error_fn()\n",
        "\n",
        "    def set_error_fn(self, error_fn):\n",
        "        self.error_fn = error_fn\n",
        "\n",
        "    def log_train_start(self, model, model_description=False):\n",
        "        print(\"\\nTraining started\")\n",
        "        print(\"================\")\n",
        "        self.model = model\n",
        "        if model_description:\n",
        "            print(model.summary())\n",
        "\n",
        "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "        if epoch % self.frequency == 0:\n",
        "            name = 'nt_epoch' if is_iter else 'tf_epoch'\n",
        "            print(f\"{name} = {epoch:6d}  \" +\n",
        "                  f\"elapsed = {self.get_elapsed()} \" +\n",
        "                  f\"(+{self.get_epoch_duration()})  \" +\n",
        "                  f\"loss = {loss:.4e}  \" + custom)\n",
        "\n",
        "    def log_train_opt(self, name):\n",
        "        print(f\"-- Starting {name} optimization --\")\n",
        "\n",
        "    def log_train_end(self, epoch, custom=\"\"):\n",
        "        print(\"==================\")\n",
        "        print(f\"Training finished (epoch {epoch}): \" +\n",
        "              f\"duration = {self.get_elapsed()}  \" +\n",
        "              f\"error = {self.get_error_u():.4e}  \" + custom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dJ1CESikfZq"
      },
      "source": [
        "# TRAIN THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P-XtP31Ckhmx",
        "outputId": "cdc103c4-e88c-4944-8a26-808ba10522b1"
      },
      "source": [
        "# Getting the data\n",
        "path='/cylinder_nektar_wake.mat'\n",
        "X, X_train, u_train , v_train , x_1, X_star, u_star, v_star, p_star, ub, lb = prep_data(path, hp[\"N_u\"])\n",
        "lambdas_star = (1.0, 100) #true values\n",
        "\n",
        "# Creating the model\n",
        "logger = Logger(hp)\n",
        "pinn = NavierStokesInformedNN(hp, logger, ub, lb)\n",
        "\n",
        "\n",
        "# Defining the error function and training\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_train, u_train, v_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "{\n",
            "  \"N_u\": 5000,\n",
            "  \"layers\": [\n",
            "    3,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    2\n",
            "  ],\n",
            "  \"tf_epochs\": 10,\n",
            "  \"tf_lr\": 0.003,\n",
            "  \"tf_b1\": 0.9,\n",
            "  \"tf_eps\": null,\n",
            "  \"nt_epochs\": 10,\n",
            "  \"nt_lr\": 0.8,\n",
            "  \"nt_ncorr\": 50,\n",
            "  \"log_frequency\": 10\n",
            "}\n",
            "\n",
            "TensorFlow version: 2.4.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "-- Starting Adam optimization --\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_13/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_13/bias:0'] when minimizing the loss.\n",
            "tf_epoch =      0  elapsed = 00:06 (+06.9)  loss = 5.7296e+03  \n",
            "-- Starting LBFGS optimization --\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d33ab2dcbf0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_lambda_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merror_lambda_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-48d656efdb50>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, u, v)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fcffa27e30a0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, v)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Optimizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m#self.nt_optimization(X_u, u, v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fcffa27e30a0>\u001b[0m in \u001b[0;36mnt_optimization\u001b[0;34m(self, X, u, v)\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_maxIter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m           \u001b[0mf_relative_tolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_tolFun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           tolerance=self.nt_tolFun)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_optimization_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_and_flat_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/optimizer/lbfgs.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(value_and_gradients_function, initial_position, previous_optimizer_results, num_correction_pairs, tolerance, x_tolerance, f_relative_tolerance, initial_inverse_hessian_estimate, max_iterations, parallel_iterations, stopping_condition, max_line_search_iterations, name)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                          \u001b[0minitial_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                                          \u001b[0mnum_correction_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                                          tolerance)\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_optimizer_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/optimizer/lbfgs.py\u001b[0m in \u001b[0;36m_get_initial_state\u001b[0;34m(value_and_gradients_function, initial_position, num_correction_pairs, tolerance)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0minitial_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       tolerance)\n\u001b[0m\u001b[1;32m    300\u001b[0m   \u001b[0mempty_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_empty_queue_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_correction_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_deltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_deltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/optimizer/bfgs_utils.py\u001b[0m in \u001b[0;36mget_initial_state_args\u001b[0;34m(value_and_gradients_function, initial_position, grad_tolerance, control_inputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0;31m# This is a gradient-based convergence check.  We only do it for finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;31m# objective values because we assume the gradient reported at a position with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fcffa27e30a0>\u001b[0m in \u001b[0;36mloss_and_flat_grad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mgrad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mgrad_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mgrad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_flat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8371\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8372\u001b[0m       return reshape_eager_fallback(\n\u001b[0;32m-> 8373\u001b[0;31m           tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   8374\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8375\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   8391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8392\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreshape_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8393\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8394\u001b[0m   \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8395\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         tensor = ops.convert_to_tensor(\n\u001b[0;32m--> 274\u001b[0;31m             t, dtype, preferred_dtype=default_dtype, ctx=ctx)\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m       \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy2_M8tD_qX"
      },
      "source": [
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\r\n",
        "u_pred, v_pred, p_pred = pinn.predict(X_star)\r\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNPfEmc0gcmK"
      },
      "source": [
        "print(\"l1: \", lambda_1_pred)\r\n",
        "print(\"l2: \", lambda_2_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wl1p69FF52Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}