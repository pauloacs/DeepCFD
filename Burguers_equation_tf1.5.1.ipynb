{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Burguers_equation.ipynb",
      "provenance": [],
      "mount_file_id": "1gn2OCkcEGMYuepLFcPXNR-nSgCA3MmLv",
      "authorship_tag": "ABX9TyOuZTyiw9UeaPhybAuVm7wN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloacs/DeepCFD/blob/main/Burguers_equation_tf1.5.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeWVBhf1VxlH",
        "outputId": "7d776637-8c48-42a7-8ce1-c4a14645a708"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XbfkU7BeziQ",
        "outputId": "35a2ce02-95a3-4448-8af1-3efb0b7865de"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3vdKIIWXLP8",
        "outputId": "91b607a8-d705-4b44-9553-af6167dbdbc7"
      },
      "source": [
        "#install latex\n",
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended  \n",
        "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip \n",
        "! unzip type1cm.zip -d /tmp/type1cm \n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm \n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm \n",
        "! sudo texhash \n",
        "!apt install cm-super"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base\n",
            "Suggested packages:\n",
            "  fonts-noto poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum debhelper gv\n",
            "  | postscript-viewer perl-tk xpdf-reader | pdf-viewer texlive-latex-base-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-lmodern fonts-noto-mono libcupsfilters1\n",
            "  libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpotrace0 libptexenc1 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13\n",
            "  lmodern poppler-data t1utils tex-common texlive-base texlive-binaries\n",
            "  texlive-latex-base texlive-latex-recommended\n",
            "0 upgraded, 24 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 68.4 MB of archives.\n",
            "After this operation, 223 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.14 [5,092 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [2,265 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Fetched 68.4 MB in 4s (17.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 24.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../02-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../03-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../04-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../05-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../06-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../09-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../10-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../11-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../12-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../13-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../14-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../15-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../16-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../17-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../18-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../19-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../20-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../21-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../22-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../23-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-lato fonts-texgyre ghostscript gsfonts javascript-common libjs-jquery\n",
            "  libruby2.5 preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-pictures texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  ghostscript-x apache2 | lighttpd | httpd ri ruby-dev bundler\n",
            "  texlive-fonts-recommended-doc python-pygments icc-profiles\n",
            "  libfile-which-perl libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  dot2tex prerex ruby-tcltk | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  dvipng fonts-lato fonts-texgyre ghostscript gsfonts javascript-common\n",
            "  libjs-jquery libruby2.5 preview-latex-style rake ruby ruby-did-you-mean\n",
            "  ruby-minitest ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration tex-gyre texlive-fonts-recommended texlive-latex-extra\n",
            "  texlive-pictures texlive-plain-generic tipa\n",
            "0 upgraded, 24 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 69.8 MB of archives.\n",
            "After this operation, 221 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [51.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.7 [48.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.7 [3,068 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 69.8 MB in 4s (18.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 24.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-lato.\n",
            "(Reading database ... 153776 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../01-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../02-dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../03-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../04-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../05-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../06-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../07-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../08-ruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../09-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../10-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../11-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../12-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../13-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../14-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../15-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../16-libruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../17-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../18-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../19-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../20-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../21-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../22-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../23-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "--2021-02-21 11:34:23--  http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip\n",
            "Resolving mirrors.ctan.org (mirrors.ctan.org)... 5.35.249.60\n",
            "Connecting to mirrors.ctan.org (mirrors.ctan.org)|5.35.249.60|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://mirrors.concertpass.com/tex-archive/macros/latex/contrib/type1cm.zip [following]\n",
            "--2021-02-21 11:34:24--  https://mirrors.concertpass.com/tex-archive/macros/latex/contrib/type1cm.zip\n",
            "Resolving mirrors.concertpass.com (mirrors.concertpass.com)... 162.219.248.112\n",
            "Connecting to mirrors.concertpass.com (mirrors.concertpass.com)|162.219.248.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 328566 (321K) [application/zip]\n",
            "Saving to: ‘type1cm.zip’\n",
            "\n",
            "type1cm.zip         100%[===================>] 320.87K  1.54MB/s    in 0.2s    \n",
            "\n",
            "2021-02-21 11:34:25 (1.54 MB/s) - ‘type1cm.zip’ saved [328566/328566]\n",
            "\n",
            "Archive:  type1cm.zip\n",
            "   creating: /tmp/type1cm/type1cm/\n",
            "  inflating: /tmp/type1cm/type1cm/type1cm.fdd  \n",
            "  inflating: /tmp/type1cm/type1cm/type1cm.ins  \n",
            "  inflating: /tmp/type1cm/type1cm/type1cm.txt  \n",
            "  inflating: /tmp/type1cm/type1cm/type1cm-doc.pdf  \n",
            "  inflating: /tmp/type1cm/type1cm/type1cm-doc.tex  \n",
            "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian) (preloaded format=latex)\n",
            " restricted \\write18 enabled.\n",
            "entering extended mode\n",
            "(./type1cm.ins\n",
            "LaTeX2e <2017-04-15>\n",
            "Babel <3.18> and hyphenation patterns for 3 language(s) loaded.\n",
            "(/usr/share/texlive/texmf-dist/tex/latex/base/docstrip.tex\n",
            "Utility: `docstrip' 2.5e <2014/09/29>\n",
            "English documentation    <2017/03/13>\n",
            "\n",
            "**********************************************************\n",
            "* This program converts documented macro-files into fast *\n",
            "* loadable files by stripping off (nearly) all comments! *\n",
            "**********************************************************\n",
            "\n",
            "********************************************************\n",
            "* No Configuration file found, using default settings. *\n",
            "********************************************************\n",
            "\n",
            "(./type1cm.ins\n",
            "\n",
            "Generating file(s) ./type1cm.sty \n",
            "\n",
            "Processing file type1cm.fdd (package,ams) -> type1cm.sty\n",
            "Lines  processed: 410\n",
            "Comments removed: 25\n",
            "Comments  passed: 7\n",
            "Codelines passed: 263\n",
            "\n",
            ") ) )\n",
            "No pages of output.\n",
            "Transcript written on type1cm.log.\n",
            "texhash: Updating /usr/local/share/texmf/ls-R... \n",
            "texhash: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "texhash: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "texhash: Updating /var/lib/texmf/ls-R... \n",
            "texhash: Done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal pfb2t1c2pfb\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal pfb2t1c2pfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 24.5 MB of archives.\n",
            "After this operation, 59.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Fetched 24.5 MB in 2s (11.7 MB/s)\n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "(Reading database ... 172320 files and directories currently installed.)\n",
            "Preparing to unpack .../cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "Running mktexlsr. This may take some time... done.\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jZNjOtzFSkBj",
        "outputId": "573b99ae-f979-42a0-d7a8-e95325e30037"
      },
      "source": [
        "\"\"\"\n",
        "@author: Maziar Raissi\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/PINNs-master/Utilities')\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import time\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, X, u, layers, lb, ub):\n",
        "        \n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        \n",
        "        self.x = X[:,0:1]\n",
        "        self.t = X[:,1:2]\n",
        "        self.u = u\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NNs\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
        "        \n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
        "                \n",
        "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
        "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
        "        \n",
        "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_pred))\n",
        "        \n",
        "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
        "                                                                method = 'L-BFGS-B', \n",
        "                                                                options = {'maxiter': 50000,\n",
        "                                                                           'maxfun': 50000,\n",
        "                                                                           'maxcor': 50,\n",
        "                                                                           'maxls': 50,\n",
        "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
        "    \n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "            \n",
        "    def net_u(self, x, t):  \n",
        "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
        "        return u\n",
        "    \n",
        "    def net_f(self, x, t):\n",
        "        lambda_1 = self.lambda_1        \n",
        "        lambda_2 = tf.exp(self.lambda_2)\n",
        "        u = self.net_u(x,t)\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def callback(self, loss, lambda_1, lambda_2):\n",
        "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
        "        \n",
        "        \n",
        "    def train(self, nIter):\n",
        "        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                lambda_1_value = self.sess.run(self.lambda_1)\n",
        "                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
        "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
        "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.optimizer.minimize(self.sess,\n",
        "                                feed_dict = tf_dict,\n",
        "                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
        "                                loss_callback = self.callback)\n",
        "        \n",
        "        \n",
        "    def predict(self, X_star):\n",
        "        \n",
        "        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
        "        \n",
        "        return u_star, f_star\n",
        "\n",
        "    \n",
        "if __name__ == \"__main__\": \n",
        "     \n",
        "    nu = 0.01/np.pi\n",
        "\n",
        "    N_u = 2000\n",
        "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "    \n",
        "    data = scipy.io.loadmat('/content/drive/MyDrive/PINNs-master/appendix/Data/burgers_shock.mat')\n",
        "    \n",
        "    t = data['t'].flatten()[:,None]\n",
        "    x = data['x'].flatten()[:,None]\n",
        "    Exact = np.real(data['usol']).T\n",
        "    \n",
        "    X, T = np.meshgrid(x,t)\n",
        "    \n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "    u_star = Exact.flatten()[:,None]              \n",
        "\n",
        "    # Doman bounds\n",
        "    lb = X_star.min(0)\n",
        "    ub = X_star.max(0)    \n",
        "    \n",
        "    ######################################################################\n",
        "    ######################## Noiseles Data ###############################\n",
        "    ######################################################################\n",
        "    noise = 0.0            \n",
        "             \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "    \n",
        "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "    model.train(0)\n",
        "    \n",
        "    u_pred, f_pred = model.predict(X_star)\n",
        "            \n",
        "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    \n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "        \n",
        "    lambda_1_value = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value = model.sess.run(model.lambda_2)\n",
        "    lambda_2_value = np.exp(lambda_2_value)\n",
        "    \n",
        "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "    error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
        "    \n",
        "    print('Error u: %e' % (error_u))    \n",
        "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
        "    print('Error l2: %.5f%%' % (error_lambda_2))  \n",
        "    \n",
        "    ######################################################################\n",
        "    ########################### Noisy Data ###############################\n",
        "    ######################################################################\n",
        "    noise = 0.01        \n",
        "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "        \n",
        "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "    model.train(10000)\n",
        "    \n",
        "    u_pred, f_pred = model.predict(X_star)\n",
        "        \n",
        "    lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
        "    lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
        "            \n",
        "    error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
        "    error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n",
        "    \n",
        "    print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
        "    print('Error lambda_2: %f%%' % (error_lambda_2_noisy))                               "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss: 2.091399e-06, l1: 0.99975, l2: 0.00320\n",
            "Loss: 2.090648e-06, l1: 0.99975, l2: 0.00320\n",
            "Loss: 2.090331e-06, l1: 0.99975, l2: 0.00320\n",
            "Loss: 2.089385e-06, l1: 0.99975, l2: 0.00320\n",
            "Loss: 2.088753e-06, l1: 0.99974, l2: 0.00320\n",
            "Loss: 2.090692e-06, l1: 0.99976, l2: 0.00320\n",
            "Loss: 2.088265e-06, l1: 0.99975, l2: 0.00320\n",
            "Loss: 2.087396e-06, l1: 0.99973, l2: 0.00320\n",
            "Loss: 2.086143e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.085190e-06, l1: 0.99970, l2: 0.00320\n",
            "Loss: 2.084693e-06, l1: 0.99970, l2: 0.00320\n",
            "Loss: 2.084555e-06, l1: 0.99973, l2: 0.00320\n",
            "Loss: 2.084183e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.083340e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.082509e-06, l1: 0.99972, l2: 0.00320\n",
            "Loss: 2.081626e-06, l1: 0.99972, l2: 0.00320\n",
            "Loss: 2.081254e-06, l1: 0.99972, l2: 0.00320\n",
            "Loss: 2.080539e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.079714e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.078348e-06, l1: 0.99971, l2: 0.00320\n",
            "Loss: 2.077428e-06, l1: 0.99970, l2: 0.00320\n",
            "Loss: 2.076293e-06, l1: 0.99969, l2: 0.00320\n",
            "Loss: 2.075630e-06, l1: 0.99969, l2: 0.00320\n",
            "Loss: 2.075196e-06, l1: 0.99968, l2: 0.00320\n",
            "Loss: 2.074661e-06, l1: 0.99968, l2: 0.00320\n",
            "Loss: 2.074399e-06, l1: 0.99967, l2: 0.00320\n",
            "Loss: 2.073811e-06, l1: 0.99967, l2: 0.00320\n",
            "Loss: 2.073441e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.072903e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.072812e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.072259e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.071922e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.071193e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.070587e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.071130e-06, l1: 0.99963, l2: 0.00320\n",
            "Loss: 2.070150e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.069434e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.068677e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.068077e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.067452e-06, l1: 0.99963, l2: 0.00320\n",
            "Loss: 2.066662e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.065591e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.065154e-06, l1: 0.99967, l2: 0.00320\n",
            "Loss: 2.064527e-06, l1: 0.99968, l2: 0.00320\n",
            "Loss: 2.064053e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.063446e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.062544e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.061333e-06, l1: 0.99963, l2: 0.00320\n",
            "Loss: 2.060514e-06, l1: 0.99964, l2: 0.00320\n",
            "Loss: 2.059173e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.058087e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.056640e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.055421e-06, l1: 0.99966, l2: 0.00320\n",
            "Loss: 2.054601e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.053572e-06, l1: 0.99963, l2: 0.00320\n",
            "Loss: 2.054573e-06, l1: 0.99960, l2: 0.00320\n",
            "Loss: 2.052816e-06, l1: 0.99962, l2: 0.00320\n",
            "Loss: 2.051560e-06, l1: 0.99960, l2: 0.00320\n",
            "Loss: 2.049753e-06, l1: 0.99959, l2: 0.00320\n",
            "Loss: 2.048306e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.047261e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.045634e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.044731e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.043985e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.043494e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.042702e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.041593e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.049107e-06, l1: 0.99965, l2: 0.00320\n",
            "Loss: 2.041297e-06, l1: 0.99959, l2: 0.00320\n",
            "Loss: 2.040347e-06, l1: 0.99959, l2: 0.00320\n",
            "Loss: 2.039855e-06, l1: 0.99959, l2: 0.00320\n",
            "Loss: 2.039678e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.038729e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.038357e-06, l1: 0.99957, l2: 0.00320\n",
            "Loss: 2.037638e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.037166e-06, l1: 0.99955, l2: 0.00320\n",
            "Loss: 2.039405e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.036933e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.036506e-06, l1: 0.99955, l2: 0.00320\n",
            "Loss: 2.035988e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.034982e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.034436e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.037410e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.034159e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.033476e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.032988e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.032509e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.032105e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.031034e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.031144e-06, l1: 0.99958, l2: 0.00320\n",
            "Loss: 2.030577e-06, l1: 0.99956, l2: 0.00320\n",
            "Loss: 2.030108e-06, l1: 0.99955, l2: 0.00320\n",
            "Loss: 2.029565e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.029279e-06, l1: 0.99953, l2: 0.00320\n",
            "Loss: 2.028489e-06, l1: 0.99952, l2: 0.00320\n",
            "Loss: 2.027835e-06, l1: 0.99952, l2: 0.00320\n",
            "Loss: 2.026902e-06, l1: 0.99953, l2: 0.00320\n",
            "Loss: 2.026334e-06, l1: 0.99952, l2: 0.00320\n",
            "Loss: 2.025098e-06, l1: 0.99954, l2: 0.00320\n",
            "Loss: 2.023336e-06, l1: 0.99953, l2: 0.00320\n",
            "Loss: 2.021649e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.020924e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 2.019899e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.018502e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.020823e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 2.018052e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.016688e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 2.015866e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 2.015289e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 2.014708e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.014182e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 2.013945e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 2.013669e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 2.012785e-06, l1: 0.99949, l2: 0.00319\n",
            "Loss: 2.012213e-06, l1: 0.99949, l2: 0.00319\n",
            "Loss: 2.011197e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.010609e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.009973e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 2.009029e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 2.008392e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 2.007314e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 2.006645e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.005508e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.005592e-06, l1: 0.99956, l2: 0.00319\n",
            "Loss: 2.004538e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 2.002924e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 2.001755e-06, l1: 0.99949, l2: 0.00319\n",
            "Loss: 2.001326e-06, l1: 0.99949, l2: 0.00319\n",
            "Loss: 2.000910e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.000243e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.999170e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 2.001932e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.998842e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.998066e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.997587e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.997317e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.997069e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.996828e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.996385e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.996181e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.995817e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.995013e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.995374e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.994672e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.994043e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.993241e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.993215e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.992845e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.992263e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.991642e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.991112e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 2.006778e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.990862e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.989907e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.989502e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.988342e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.987639e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.987262e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.986774e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.985841e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.984835e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.984095e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.984928e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.983504e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.982499e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.980920e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 1.980684e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.979583e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.978029e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.974837e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.972888e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.971351e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.969380e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.967750e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.965770e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.963700e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.962827e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.960758e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.960233e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.959619e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.958596e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.957270e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.955846e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.954867e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.953879e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.953078e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.951453e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.950476e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.949909e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.949436e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.949257e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.948707e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.947996e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.947642e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.947059e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.946772e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.945900e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.952292e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.945632e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.944872e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.944394e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.943992e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.943578e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.943746e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.943357e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.942635e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.941806e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.940980e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.940104e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.938853e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.937091e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.935869e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.934755e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.934279e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.933979e-06, l1: 0.99975, l2: 0.00319\n",
            "Loss: 1.933225e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.932421e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.931709e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.930675e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.929860e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.928724e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.928010e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.927194e-06, l1: 0.99975, l2: 0.00319\n",
            "Loss: 1.926196e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.925467e-06, l1: 0.99975, l2: 0.00319\n",
            "Loss: 1.924836e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.924028e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.923115e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.922150e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.920675e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.919214e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.918373e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.917519e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.916559e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.915713e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.914367e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.913892e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.913395e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.912424e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.911402e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.910720e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.909384e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.909661e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.908591e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.907505e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.906538e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.906058e-06, l1: 0.99964, l2: 0.00319\n",
            "Loss: 1.905341e-06, l1: 0.99964, l2: 0.00319\n",
            "Loss: 1.904673e-06, l1: 0.99963, l2: 0.00319\n",
            "Loss: 1.903945e-06, l1: 0.99963, l2: 0.00319\n",
            "Loss: 1.903551e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.905233e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.903371e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.903105e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.902544e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.901983e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.900653e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.899780e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.908325e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.899496e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.898538e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 1.897218e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.895352e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 1.893744e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.892817e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.892196e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.891694e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.891882e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.891466e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.891057e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.890337e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 1.890121e-06, l1: 0.99949, l2: 0.00319\n",
            "Loss: 1.888564e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.887591e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.886306e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.900156e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.885813e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.884365e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.882913e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.882235e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.881510e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.881099e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.879924e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.892930e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.879670e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.878907e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.878454e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.877842e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.877180e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.876410e-06, l1: 0.99942, l2: 0.00319\n",
            "Loss: 1.875667e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.875066e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.874685e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.873839e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.872445e-06, l1: 0.99944, l2: 0.00319\n",
            "Loss: 1.871749e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.871150e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.870629e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.870144e-06, l1: 0.99942, l2: 0.00319\n",
            "Loss: 1.868549e-06, l1: 0.99941, l2: 0.00319\n",
            "Loss: 1.866968e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.864431e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.890360e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.864078e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.862043e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.860996e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.860008e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.859164e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.857644e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.856386e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.854322e-06, l1: 0.99933, l2: 0.00319\n",
            "Loss: 1.852572e-06, l1: 0.99930, l2: 0.00319\n",
            "Loss: 1.851364e-06, l1: 0.99928, l2: 0.00319\n",
            "Loss: 1.849333e-06, l1: 0.99929, l2: 0.00319\n",
            "Loss: 1.848000e-06, l1: 0.99928, l2: 0.00319\n",
            "Loss: 1.847271e-06, l1: 0.99931, l2: 0.00319\n",
            "Loss: 1.846430e-06, l1: 0.99932, l2: 0.00319\n",
            "Loss: 1.845592e-06, l1: 0.99931, l2: 0.00319\n",
            "Loss: 1.844477e-06, l1: 0.99932, l2: 0.00319\n",
            "Loss: 1.842394e-06, l1: 0.99933, l2: 0.00319\n",
            "Loss: 1.843414e-06, l1: 0.99933, l2: 0.00319\n",
            "Loss: 1.841622e-06, l1: 0.99933, l2: 0.00319\n",
            "Loss: 1.840542e-06, l1: 0.99934, l2: 0.00319\n",
            "Loss: 1.839247e-06, l1: 0.99935, l2: 0.00319\n",
            "Loss: 1.837529e-06, l1: 0.99935, l2: 0.00319\n",
            "Loss: 1.839588e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.836791e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.835552e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.833852e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.833584e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.832322e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.831483e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.830893e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.830408e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.829741e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.832335e-06, l1: 0.99935, l2: 0.00319\n",
            "Loss: 1.829105e-06, l1: 0.99936, l2: 0.00319\n",
            "Loss: 1.828453e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.826574e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.825903e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.825508e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.824608e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.823922e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.822953e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.822026e-06, l1: 0.99937, l2: 0.00319\n",
            "Loss: 1.820468e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.819204e-06, l1: 0.99938, l2: 0.00319\n",
            "Loss: 1.816960e-06, l1: 0.99940, l2: 0.00319\n",
            "Loss: 1.815695e-06, l1: 0.99942, l2: 0.00319\n",
            "Loss: 1.814704e-06, l1: 0.99942, l2: 0.00319\n",
            "Loss: 1.812937e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.811703e-06, l1: 0.99943, l2: 0.00319\n",
            "Loss: 1.810278e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.823042e-06, l1: 0.99942, l2: 0.00319\n",
            "Loss: 1.809874e-06, l1: 0.99945, l2: 0.00319\n",
            "Loss: 1.809317e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.808109e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.806872e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.805731e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.806835e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.805484e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.804593e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.804168e-06, l1: 0.99947, l2: 0.00319\n",
            "Loss: 1.803528e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.802654e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.802186e-06, l1: 0.99946, l2: 0.00319\n",
            "Loss: 1.800989e-06, l1: 0.99948, l2: 0.00319\n",
            "Loss: 1.800376e-06, l1: 0.99950, l2: 0.00319\n",
            "Loss: 1.799458e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.798660e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.796977e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.794952e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.792694e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.790856e-06, l1: 0.99964, l2: 0.00319\n",
            "Loss: 1.789538e-06, l1: 0.99964, l2: 0.00319\n",
            "Loss: 1.788062e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.786864e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.785795e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.784826e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.784278e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.783267e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.785689e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.782684e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.781676e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.780607e-06, l1: 0.99975, l2: 0.00319\n",
            "Loss: 1.779885e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.779447e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.777929e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.777229e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.776201e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.774917e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.774039e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.773569e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.772581e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.771680e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.770563e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.769884e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.769439e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.769017e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.768332e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.767307e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.765676e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.764893e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.763359e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.762646e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.761159e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.759344e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.757586e-06, l1: 0.99995, l2: 0.00319\n",
            "Loss: 1.756395e-06, l1: 0.99996, l2: 0.00319\n",
            "Loss: 1.755691e-06, l1: 0.99995, l2: 0.00319\n",
            "Loss: 1.754536e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.753302e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.757903e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.753226e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.752418e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.752006e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.751596e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.750811e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.750262e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.750155e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.749745e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.749260e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.748930e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.748671e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.748141e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.746844e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.745504e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.744697e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.744089e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.743770e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.742825e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.741710e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.739673e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.738007e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.737226e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.736488e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.735781e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.735067e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.734368e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.733228e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.732196e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.732278e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.731779e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.731365e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.730856e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.730522e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.730256e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.730066e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.729607e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.729080e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.728779e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.729006e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.728612e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.728222e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.727777e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.727309e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.727183e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.726576e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.727293e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.726228e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.725500e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.724672e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.724184e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.723714e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.723761e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.723607e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.723265e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.722862e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.722596e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.722497e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.722097e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.721605e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.721210e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.721286e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.720887e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.720733e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.720468e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.720210e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.720207e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.720023e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.719524e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.718999e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.718711e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.717930e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.717306e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.717074e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.716909e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.716560e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.716175e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.715822e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.715580e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.715178e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.714387e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.713691e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.712774e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.711814e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.711397e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.710948e-06, l1: 0.99972, l2: 0.00319\n",
            "Loss: 1.709955e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.709504e-06, l1: 0.99974, l2: 0.00319\n",
            "Loss: 1.709003e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.708783e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.708519e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.707403e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.706906e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.706339e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.705626e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.704890e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.705902e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.704348e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.703583e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.702752e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.702011e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.701157e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.700315e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.699674e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.699096e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.698067e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.697227e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.696605e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.696151e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.695851e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.695396e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.694742e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.693927e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.693189e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.692481e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.691932e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.691889e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.691643e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.691257e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.690898e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.690012e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.689265e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.695703e-06, l1: 0.99996, l2: 0.00319\n",
            "Loss: 1.688960e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.688378e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.688158e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.687745e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.687122e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.686843e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.686226e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.685292e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.684582e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.684291e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.684591e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.683460e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.683094e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.682497e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.681734e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.680234e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.678453e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.676808e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.674597e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.672348e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.669061e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.667046e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.666266e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.665461e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.664293e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.663199e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.661660e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.660001e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.659055e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.658029e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.656748e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.655720e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.654316e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.652785e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.650661e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.649694e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.649078e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.648835e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.647954e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.647743e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.646841e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.646342e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.645706e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.645634e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.644829e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.644189e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.643534e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.643232e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.642671e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.641819e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.645792e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.641256e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.640488e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.639539e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.638713e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.637747e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.637001e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.636697e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.635989e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.635536e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.634928e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.634242e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.633481e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.632380e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.631378e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.630344e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.629700e-06, l1: 0.99982, l2: 0.00319\n",
            "Loss: 1.629251e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.628734e-06, l1: 0.99984, l2: 0.00319\n",
            "Loss: 1.627856e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.626765e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.625951e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.625689e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.625156e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.624805e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.624240e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.623768e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.624043e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.623587e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.623136e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.622380e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.622015e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.621283e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.620549e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.619896e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.619295e-06, l1: 0.99993, l2: 0.00319\n",
            "Loss: 1.618665e-06, l1: 0.99992, l2: 0.00319\n",
            "Loss: 1.617853e-06, l1: 0.99991, l2: 0.00319\n",
            "Loss: 1.616990e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.615982e-06, l1: 0.99990, l2: 0.00319\n",
            "Loss: 1.615115e-06, l1: 0.99989, l2: 0.00319\n",
            "Loss: 1.616018e-06, l1: 0.99986, l2: 0.00319\n",
            "Loss: 1.614642e-06, l1: 0.99988, l2: 0.00319\n",
            "Loss: 1.613955e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.613312e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.612634e-06, l1: 0.99987, l2: 0.00319\n",
            "Loss: 1.612254e-06, l1: 0.99985, l2: 0.00319\n",
            "Loss: 1.611617e-06, l1: 0.99983, l2: 0.00319\n",
            "Loss: 1.610890e-06, l1: 0.99981, l2: 0.00319\n",
            "Loss: 1.610178e-06, l1: 0.99980, l2: 0.00319\n",
            "Loss: 1.609334e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.608326e-06, l1: 0.99979, l2: 0.00319\n",
            "Loss: 1.606967e-06, l1: 0.99978, l2: 0.00319\n",
            "Loss: 1.606339e-06, l1: 0.99977, l2: 0.00319\n",
            "Loss: 1.605813e-06, l1: 0.99976, l2: 0.00319\n",
            "Loss: 1.604716e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.603920e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.602533e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.600618e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.599202e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.598294e-06, l1: 0.99973, l2: 0.00319\n",
            "Loss: 1.597713e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.596783e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.596454e-06, l1: 0.99971, l2: 0.00319\n",
            "Loss: 1.595984e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.595660e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.595283e-06, l1: 0.99968, l2: 0.00319\n",
            "Loss: 1.594818e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.594648e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.594505e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.593737e-06, l1: 0.99970, l2: 0.00319\n",
            "Loss: 1.593078e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.592429e-06, l1: 0.99969, l2: 0.00319\n",
            "Loss: 1.592077e-06, l1: 0.99967, l2: 0.00319\n",
            "Loss: 1.591737e-06, l1: 0.99966, l2: 0.00319\n",
            "Loss: 1.591422e-06, l1: 0.99965, l2: 0.00319\n",
            "Loss: 1.590392e-06, l1: 0.99963, l2: 0.00319\n",
            "Loss: 1.589744e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.589102e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.588428e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.587831e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.587349e-06, l1: 0.99963, l2: 0.00319\n",
            "Loss: 1.586674e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.585982e-06, l1: 0.99963, l2: 0.00319\n",
            "Loss: 1.585529e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.584560e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.584126e-06, l1: 0.99962, l2: 0.00319\n",
            "Loss: 1.583239e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.582544e-06, l1: 0.99960, l2: 0.00319\n",
            "Loss: 1.581839e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.581397e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.581134e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.580395e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.579491e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.577860e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.576563e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.575534e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.574813e-06, l1: 0.99960, l2: 0.00319\n",
            "Loss: 1.574115e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.573604e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.573199e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.572641e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.571913e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.570592e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.569337e-06, l1: 0.99956, l2: 0.00319\n",
            "Loss: 1.568396e-06, l1: 0.99956, l2: 0.00319\n",
            "Loss: 1.567635e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.567224e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.566437e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.565861e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.565195e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.564056e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.562988e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.562246e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.561878e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.561509e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.560911e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.560470e-06, l1: 0.99954, l2: 0.00319\n",
            "Loss: 1.560154e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.559715e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.559694e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.559492e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.559226e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.558586e-06, l1: 0.99956, l2: 0.00319\n",
            "Loss: 1.558092e-06, l1: 0.99956, l2: 0.00319\n",
            "Loss: 1.557452e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.556915e-06, l1: 0.99957, l2: 0.00319\n",
            "Loss: 1.556152e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.555780e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.555304e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.555068e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.554414e-06, l1: 0.99960, l2: 0.00319\n",
            "Loss: 1.553679e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.553031e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.552913e-06, l1: 0.99961, l2: 0.00319\n",
            "Loss: 1.552618e-06, l1: 0.99960, l2: 0.00319\n",
            "Loss: 1.552149e-06, l1: 0.99959, l2: 0.00319\n",
            "Loss: 1.551085e-06, l1: 0.99958, l2: 0.00319\n",
            "Loss: 1.549800e-06, l1: 0.99955, l2: 0.00319\n",
            "Loss: 1.549346e-06, l1: 0.99951, l2: 0.00319\n",
            "Loss: 1.548881e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548630e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548266e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548358e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548360e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548354e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548333e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548266e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548266e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548225e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548225e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548225e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548222e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548222e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548222e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548221e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548221e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548221e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548326e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.548221e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 3.339823e-04, l1: 0.99799, l2: 0.00316\n",
            "Loss: 2.342215e-06, l1: 0.99937, l2: 0.00318\n",
            "Loss: 1.548028e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.547911e-06, l1: 0.99953, l2: 0.00319\n",
            "Loss: 1.547769e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547590e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547522e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547505e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547490e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "Loss: 1.547457e-06, l1: 0.99952, l2: 0.00319\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
            "  Objective function value: 0.000002\n",
            "  Number of iterations: 5619\n",
            "  Number of functions evaluations: 6104\n",
            "Error u: 8.157560e-04\n",
            "Error l1: 0.04834%\n",
            "Error l2: 0.13781%\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "It: 0, Loss: 4.023e-01, Lambda_1: 0.001, Lambda_2: 0.002480, Time: 1.28\n",
            "It: 10, Loss: 3.000e-01, Lambda_1: -0.003, Lambda_2: 0.002487, Time: 0.27\n",
            "It: 20, Loss: 2.598e-01, Lambda_1: -0.008, Lambda_2: 0.002508, Time: 0.27\n",
            "It: 30, Loss: 2.482e-01, Lambda_1: -0.020, Lambda_2: 0.002537, Time: 0.26\n",
            "It: 40, Loss: 2.436e-01, Lambda_1: -0.032, Lambda_2: 0.002571, Time: 0.29\n",
            "It: 50, Loss: 2.349e-01, Lambda_1: -0.044, Lambda_2: 0.002605, Time: 0.28\n",
            "It: 60, Loss: 2.216e-01, Lambda_1: -0.055, Lambda_2: 0.002642, Time: 0.29\n",
            "It: 70, Loss: 1.944e-01, Lambda_1: -0.064, Lambda_2: 0.002676, Time: 0.27\n",
            "It: 80, Loss: 1.461e-01, Lambda_1: -0.067, Lambda_2: 0.002683, Time: 0.28\n",
            "It: 90, Loss: 9.770e-02, Lambda_1: -0.060, Lambda_2: 0.002638, Time: 0.29\n",
            "It: 100, Loss: 7.170e-02, Lambda_1: -0.041, Lambda_2: 0.002582, Time: 0.27\n",
            "It: 110, Loss: 5.985e-02, Lambda_1: -0.018, Lambda_2: 0.002524, Time: 0.28\n",
            "It: 120, Loss: 5.041e-02, Lambda_1: 0.005, Lambda_2: 0.002468, Time: 0.26\n",
            "It: 130, Loss: 4.353e-02, Lambda_1: 0.024, Lambda_2: 0.002422, Time: 0.27\n",
            "It: 140, Loss: 4.003e-02, Lambda_1: 0.039, Lambda_2: 0.002393, Time: 0.27\n",
            "It: 150, Loss: 3.852e-02, Lambda_1: 0.048, Lambda_2: 0.002380, Time: 0.30\n",
            "It: 160, Loss: 3.769e-02, Lambda_1: 0.051, Lambda_2: 0.002377, Time: 0.27\n",
            "It: 170, Loss: 3.704e-02, Lambda_1: 0.052, Lambda_2: 0.002380, Time: 0.26\n",
            "It: 180, Loss: 3.649e-02, Lambda_1: 0.052, Lambda_2: 0.002385, Time: 0.28\n",
            "It: 190, Loss: 3.602e-02, Lambda_1: 0.052, Lambda_2: 0.002390, Time: 0.27\n",
            "It: 200, Loss: 3.560e-02, Lambda_1: 0.053, Lambda_2: 0.002395, Time: 0.26\n",
            "It: 210, Loss: 3.521e-02, Lambda_1: 0.054, Lambda_2: 0.002398, Time: 0.27\n",
            "It: 220, Loss: 3.483e-02, Lambda_1: 0.055, Lambda_2: 0.002402, Time: 0.28\n",
            "It: 230, Loss: 3.445e-02, Lambda_1: 0.057, Lambda_2: 0.002405, Time: 0.28\n",
            "It: 240, Loss: 3.410e-02, Lambda_1: 0.059, Lambda_2: 0.002409, Time: 0.27\n",
            "It: 250, Loss: 3.369e-02, Lambda_1: 0.060, Lambda_2: 0.002413, Time: 0.26\n",
            "It: 260, Loss: 3.329e-02, Lambda_1: 0.062, Lambda_2: 0.002417, Time: 0.28\n",
            "It: 270, Loss: 3.287e-02, Lambda_1: 0.064, Lambda_2: 0.002421, Time: 0.28\n",
            "It: 280, Loss: 3.244e-02, Lambda_1: 0.066, Lambda_2: 0.002425, Time: 0.27\n",
            "It: 290, Loss: 3.198e-02, Lambda_1: 0.068, Lambda_2: 0.002430, Time: 0.28\n",
            "It: 300, Loss: 3.150e-02, Lambda_1: 0.070, Lambda_2: 0.002435, Time: 0.27\n",
            "It: 310, Loss: 3.099e-02, Lambda_1: 0.073, Lambda_2: 0.002440, Time: 0.26\n",
            "It: 320, Loss: 3.047e-02, Lambda_1: 0.076, Lambda_2: 0.002445, Time: 0.27\n",
            "It: 330, Loss: 2.993e-02, Lambda_1: 0.079, Lambda_2: 0.002451, Time: 0.29\n",
            "It: 340, Loss: 2.938e-02, Lambda_1: 0.082, Lambda_2: 0.002457, Time: 0.26\n",
            "It: 350, Loss: 2.882e-02, Lambda_1: 0.085, Lambda_2: 0.002464, Time: 0.25\n",
            "It: 360, Loss: 2.826e-02, Lambda_1: 0.089, Lambda_2: 0.002470, Time: 0.26\n",
            "It: 370, Loss: 2.770e-02, Lambda_1: 0.094, Lambda_2: 0.002478, Time: 0.28\n",
            "It: 380, Loss: 2.714e-02, Lambda_1: 0.098, Lambda_2: 0.002485, Time: 0.27\n",
            "It: 390, Loss: 2.660e-02, Lambda_1: 0.103, Lambda_2: 0.002492, Time: 0.27\n",
            "It: 400, Loss: 2.607e-02, Lambda_1: 0.108, Lambda_2: 0.002499, Time: 0.27\n",
            "It: 410, Loss: 2.556e-02, Lambda_1: 0.113, Lambda_2: 0.002506, Time: 0.27\n",
            "It: 420, Loss: 2.507e-02, Lambda_1: 0.119, Lambda_2: 0.002513, Time: 0.27\n",
            "It: 430, Loss: 2.459e-02, Lambda_1: 0.124, Lambda_2: 0.002519, Time: 0.27\n",
            "It: 440, Loss: 2.413e-02, Lambda_1: 0.130, Lambda_2: 0.002525, Time: 0.27\n",
            "It: 450, Loss: 2.369e-02, Lambda_1: 0.136, Lambda_2: 0.002531, Time: 0.29\n",
            "It: 460, Loss: 2.335e-02, Lambda_1: 0.142, Lambda_2: 0.002536, Time: 0.26\n",
            "It: 470, Loss: 2.294e-02, Lambda_1: 0.148, Lambda_2: 0.002542, Time: 0.28\n",
            "It: 480, Loss: 2.248e-02, Lambda_1: 0.154, Lambda_2: 0.002547, Time: 0.26\n",
            "It: 490, Loss: 2.210e-02, Lambda_1: 0.160, Lambda_2: 0.002553, Time: 0.27\n",
            "It: 500, Loss: 2.174e-02, Lambda_1: 0.166, Lambda_2: 0.002559, Time: 0.26\n",
            "It: 510, Loss: 2.137e-02, Lambda_1: 0.172, Lambda_2: 0.002565, Time: 0.28\n",
            "It: 520, Loss: 2.102e-02, Lambda_1: 0.178, Lambda_2: 0.002570, Time: 0.27\n",
            "It: 530, Loss: 2.107e-02, Lambda_1: 0.184, Lambda_2: 0.002576, Time: 0.26\n",
            "It: 540, Loss: 2.041e-02, Lambda_1: 0.191, Lambda_2: 0.002582, Time: 0.27\n",
            "It: 550, Loss: 2.013e-02, Lambda_1: 0.197, Lambda_2: 0.002588, Time: 0.28\n",
            "It: 560, Loss: 1.973e-02, Lambda_1: 0.203, Lambda_2: 0.002594, Time: 0.27\n",
            "It: 570, Loss: 1.939e-02, Lambda_1: 0.209, Lambda_2: 0.002601, Time: 0.26\n",
            "It: 580, Loss: 1.906e-02, Lambda_1: 0.216, Lambda_2: 0.002607, Time: 0.27\n",
            "It: 590, Loss: 1.873e-02, Lambda_1: 0.223, Lambda_2: 0.002613, Time: 0.27\n",
            "It: 600, Loss: 1.839e-02, Lambda_1: 0.229, Lambda_2: 0.002619, Time: 0.26\n",
            "It: 610, Loss: 1.824e-02, Lambda_1: 0.236, Lambda_2: 0.002625, Time: 0.27\n",
            "It: 620, Loss: 1.816e-02, Lambda_1: 0.243, Lambda_2: 0.002631, Time: 0.28\n",
            "It: 630, Loss: 1.746e-02, Lambda_1: 0.250, Lambda_2: 0.002637, Time: 0.27\n",
            "It: 640, Loss: 1.722e-02, Lambda_1: 0.257, Lambda_2: 0.002644, Time: 0.26\n",
            "It: 650, Loss: 1.689e-02, Lambda_1: 0.264, Lambda_2: 0.002653, Time: 0.27\n",
            "It: 660, Loss: 1.658e-02, Lambda_1: 0.271, Lambda_2: 0.002661, Time: 0.27\n",
            "It: 670, Loss: 1.628e-02, Lambda_1: 0.278, Lambda_2: 0.002670, Time: 0.27\n",
            "It: 680, Loss: 1.600e-02, Lambda_1: 0.285, Lambda_2: 0.002679, Time: 0.27\n",
            "It: 690, Loss: 1.570e-02, Lambda_1: 0.292, Lambda_2: 0.002690, Time: 0.29\n",
            "It: 700, Loss: 1.542e-02, Lambda_1: 0.299, Lambda_2: 0.002700, Time: 0.27\n",
            "It: 710, Loss: 1.626e-02, Lambda_1: 0.306, Lambda_2: 0.002711, Time: 0.27\n",
            "It: 720, Loss: 1.491e-02, Lambda_1: 0.313, Lambda_2: 0.002722, Time: 0.27\n",
            "It: 730, Loss: 1.472e-02, Lambda_1: 0.320, Lambda_2: 0.002736, Time: 0.27\n",
            "It: 740, Loss: 1.435e-02, Lambda_1: 0.326, Lambda_2: 0.002752, Time: 0.26\n",
            "It: 750, Loss: 1.408e-02, Lambda_1: 0.333, Lambda_2: 0.002766, Time: 0.28\n",
            "It: 760, Loss: 1.378e-02, Lambda_1: 0.340, Lambda_2: 0.002781, Time: 0.28\n",
            "It: 770, Loss: 1.349e-02, Lambda_1: 0.347, Lambda_2: 0.002797, Time: 0.27\n",
            "It: 780, Loss: 1.320e-02, Lambda_1: 0.354, Lambda_2: 0.002814, Time: 0.27\n",
            "It: 790, Loss: 1.291e-02, Lambda_1: 0.361, Lambda_2: 0.002830, Time: 0.28\n",
            "It: 800, Loss: 1.264e-02, Lambda_1: 0.368, Lambda_2: 0.002847, Time: 0.28\n",
            "It: 810, Loss: 1.249e-02, Lambda_1: 0.375, Lambda_2: 0.002861, Time: 0.28\n",
            "It: 820, Loss: 1.389e-02, Lambda_1: 0.382, Lambda_2: 0.002875, Time: 0.27\n",
            "It: 830, Loss: 1.227e-02, Lambda_1: 0.388, Lambda_2: 0.002893, Time: 0.28\n",
            "It: 840, Loss: 1.176e-02, Lambda_1: 0.394, Lambda_2: 0.002912, Time: 0.32\n",
            "It: 850, Loss: 1.141e-02, Lambda_1: 0.401, Lambda_2: 0.002927, Time: 0.28\n",
            "It: 860, Loss: 1.111e-02, Lambda_1: 0.408, Lambda_2: 0.002942, Time: 0.28\n",
            "It: 870, Loss: 1.087e-02, Lambda_1: 0.414, Lambda_2: 0.002956, Time: 0.28\n",
            "It: 880, Loss: 1.062e-02, Lambda_1: 0.421, Lambda_2: 0.002970, Time: 0.28\n",
            "It: 890, Loss: 1.039e-02, Lambda_1: 0.428, Lambda_2: 0.002985, Time: 0.26\n",
            "It: 900, Loss: 1.016e-02, Lambda_1: 0.435, Lambda_2: 0.002998, Time: 0.27\n",
            "It: 910, Loss: 1.015e-02, Lambda_1: 0.442, Lambda_2: 0.003012, Time: 0.27\n",
            "It: 920, Loss: 1.216e-02, Lambda_1: 0.449, Lambda_2: 0.003020, Time: 0.27\n",
            "It: 930, Loss: 2.139e-02, Lambda_1: 0.455, Lambda_2: 0.003022, Time: 0.27\n",
            "It: 940, Loss: 1.545e-02, Lambda_1: 0.459, Lambda_2: 0.003018, Time: 0.29\n",
            "It: 950, Loss: 1.065e-02, Lambda_1: 0.459, Lambda_2: 0.003034, Time: 0.28\n",
            "It: 960, Loss: 1.006e-02, Lambda_1: 0.459, Lambda_2: 0.003048, Time: 0.26\n",
            "It: 970, Loss: 9.951e-03, Lambda_1: 0.459, Lambda_2: 0.003063, Time: 0.28\n",
            "It: 980, Loss: 9.447e-03, Lambda_1: 0.462, Lambda_2: 0.003073, Time: 0.27\n",
            "It: 990, Loss: 9.315e-03, Lambda_1: 0.466, Lambda_2: 0.003080, Time: 0.27\n",
            "It: 1000, Loss: 9.104e-03, Lambda_1: 0.470, Lambda_2: 0.003087, Time: 0.26\n",
            "It: 1010, Loss: 8.950e-03, Lambda_1: 0.475, Lambda_2: 0.003093, Time: 0.26\n",
            "It: 1020, Loss: 8.828e-03, Lambda_1: 0.480, Lambda_2: 0.003100, Time: 0.28\n",
            "It: 1030, Loss: 8.702e-03, Lambda_1: 0.484, Lambda_2: 0.003106, Time: 0.27\n",
            "It: 1040, Loss: 8.583e-03, Lambda_1: 0.489, Lambda_2: 0.003113, Time: 0.26\n",
            "It: 1050, Loss: 8.467e-03, Lambda_1: 0.493, Lambda_2: 0.003120, Time: 0.27\n",
            "It: 1060, Loss: 8.354e-03, Lambda_1: 0.498, Lambda_2: 0.003127, Time: 0.27\n",
            "It: 1070, Loss: 8.243e-03, Lambda_1: 0.502, Lambda_2: 0.003134, Time: 0.26\n",
            "It: 1080, Loss: 8.135e-03, Lambda_1: 0.506, Lambda_2: 0.003141, Time: 0.27\n",
            "It: 1090, Loss: 8.028e-03, Lambda_1: 0.511, Lambda_2: 0.003149, Time: 0.27\n",
            "It: 1100, Loss: 7.924e-03, Lambda_1: 0.515, Lambda_2: 0.003156, Time: 0.27\n",
            "It: 1110, Loss: 7.821e-03, Lambda_1: 0.519, Lambda_2: 0.003164, Time: 0.27\n",
            "It: 1120, Loss: 7.720e-03, Lambda_1: 0.523, Lambda_2: 0.003171, Time: 0.27\n",
            "It: 1130, Loss: 7.621e-03, Lambda_1: 0.527, Lambda_2: 0.003179, Time: 0.29\n",
            "It: 1140, Loss: 7.523e-03, Lambda_1: 0.531, Lambda_2: 0.003186, Time: 0.27\n",
            "It: 1150, Loss: 7.427e-03, Lambda_1: 0.535, Lambda_2: 0.003194, Time: 0.27\n",
            "It: 1160, Loss: 7.333e-03, Lambda_1: 0.539, Lambda_2: 0.003202, Time: 0.29\n",
            "It: 1170, Loss: 7.240e-03, Lambda_1: 0.543, Lambda_2: 0.003210, Time: 0.27\n",
            "It: 1180, Loss: 7.148e-03, Lambda_1: 0.547, Lambda_2: 0.003218, Time: 0.26\n",
            "It: 1190, Loss: 7.058e-03, Lambda_1: 0.551, Lambda_2: 0.003226, Time: 0.27\n",
            "It: 1200, Loss: 6.969e-03, Lambda_1: 0.555, Lambda_2: 0.003234, Time: 0.27\n",
            "It: 1210, Loss: 6.881e-03, Lambda_1: 0.559, Lambda_2: 0.003242, Time: 0.26\n",
            "It: 1220, Loss: 6.794e-03, Lambda_1: 0.563, Lambda_2: 0.003250, Time: 0.27\n",
            "It: 1230, Loss: 6.708e-03, Lambda_1: 0.566, Lambda_2: 0.003258, Time: 0.27\n",
            "It: 1240, Loss: 6.623e-03, Lambda_1: 0.570, Lambda_2: 0.003266, Time: 0.28\n",
            "It: 1250, Loss: 6.539e-03, Lambda_1: 0.574, Lambda_2: 0.003275, Time: 0.27\n",
            "It: 1260, Loss: 6.456e-03, Lambda_1: 0.577, Lambda_2: 0.003283, Time: 0.28\n",
            "It: 1270, Loss: 6.497e-03, Lambda_1: 0.581, Lambda_2: 0.003292, Time: 0.27\n",
            "It: 1280, Loss: 6.473e-03, Lambda_1: 0.585, Lambda_2: 0.003299, Time: 0.27\n",
            "It: 1290, Loss: 9.281e-03, Lambda_1: 0.589, Lambda_2: 0.003303, Time: 0.27\n",
            "It: 1300, Loss: 1.515e-02, Lambda_1: 0.591, Lambda_2: 0.003297, Time: 0.26\n",
            "It: 1310, Loss: 1.344e-01, Lambda_1: 0.589, Lambda_2: 0.003276, Time: 0.27\n",
            "It: 1320, Loss: 8.400e-02, Lambda_1: 0.588, Lambda_2: 0.003241, Time: 0.26\n",
            "It: 1330, Loss: 3.798e-02, Lambda_1: 0.581, Lambda_2: 0.003238, Time: 0.27\n",
            "It: 1340, Loss: 2.053e-02, Lambda_1: 0.571, Lambda_2: 0.003249, Time: 0.27\n",
            "It: 1350, Loss: 1.398e-02, Lambda_1: 0.559, Lambda_2: 0.003266, Time: 0.26\n",
            "It: 1360, Loss: 1.080e-02, Lambda_1: 0.549, Lambda_2: 0.003282, Time: 0.27\n",
            "It: 1370, Loss: 8.906e-03, Lambda_1: 0.542, Lambda_2: 0.003299, Time: 0.28\n",
            "It: 1380, Loss: 8.336e-03, Lambda_1: 0.538, Lambda_2: 0.003310, Time: 0.27\n",
            "It: 1390, Loss: 7.785e-03, Lambda_1: 0.537, Lambda_2: 0.003316, Time: 0.27\n",
            "It: 1400, Loss: 7.591e-03, Lambda_1: 0.537, Lambda_2: 0.003320, Time: 0.28\n",
            "It: 1410, Loss: 7.428e-03, Lambda_1: 0.538, Lambda_2: 0.003324, Time: 0.26\n",
            "It: 1420, Loss: 7.314e-03, Lambda_1: 0.540, Lambda_2: 0.003326, Time: 0.28\n",
            "It: 1430, Loss: 7.208e-03, Lambda_1: 0.542, Lambda_2: 0.003328, Time: 0.27\n",
            "It: 1440, Loss: 7.114e-03, Lambda_1: 0.544, Lambda_2: 0.003330, Time: 0.27\n",
            "It: 1450, Loss: 7.027e-03, Lambda_1: 0.546, Lambda_2: 0.003332, Time: 0.27\n",
            "It: 1460, Loss: 6.946e-03, Lambda_1: 0.548, Lambda_2: 0.003334, Time: 0.27\n",
            "It: 1470, Loss: 6.870e-03, Lambda_1: 0.551, Lambda_2: 0.003337, Time: 0.27\n",
            "It: 1480, Loss: 6.799e-03, Lambda_1: 0.553, Lambda_2: 0.003339, Time: 0.28\n",
            "It: 1490, Loss: 6.731e-03, Lambda_1: 0.555, Lambda_2: 0.003342, Time: 0.27\n",
            "It: 1500, Loss: 6.667e-03, Lambda_1: 0.557, Lambda_2: 0.003344, Time: 0.27\n",
            "It: 1510, Loss: 6.605e-03, Lambda_1: 0.559, Lambda_2: 0.003347, Time: 0.27\n",
            "It: 1520, Loss: 6.546e-03, Lambda_1: 0.561, Lambda_2: 0.003350, Time: 0.29\n",
            "It: 1530, Loss: 6.489e-03, Lambda_1: 0.564, Lambda_2: 0.003353, Time: 0.28\n",
            "It: 1540, Loss: 6.433e-03, Lambda_1: 0.566, Lambda_2: 0.003356, Time: 0.26\n",
            "It: 1550, Loss: 6.379e-03, Lambda_1: 0.568, Lambda_2: 0.003359, Time: 0.28\n",
            "It: 1560, Loss: 6.327e-03, Lambda_1: 0.570, Lambda_2: 0.003362, Time: 0.27\n",
            "It: 1570, Loss: 6.275e-03, Lambda_1: 0.572, Lambda_2: 0.003365, Time: 0.28\n",
            "It: 1580, Loss: 6.225e-03, Lambda_1: 0.574, Lambda_2: 0.003368, Time: 0.27\n",
            "It: 1590, Loss: 6.176e-03, Lambda_1: 0.577, Lambda_2: 0.003372, Time: 0.26\n",
            "It: 1600, Loss: 6.127e-03, Lambda_1: 0.579, Lambda_2: 0.003375, Time: 0.27\n",
            "It: 1610, Loss: 6.080e-03, Lambda_1: 0.581, Lambda_2: 0.003378, Time: 0.26\n",
            "It: 1620, Loss: 6.033e-03, Lambda_1: 0.583, Lambda_2: 0.003382, Time: 0.27\n",
            "It: 1630, Loss: 5.986e-03, Lambda_1: 0.585, Lambda_2: 0.003385, Time: 0.29\n",
            "It: 1640, Loss: 5.941e-03, Lambda_1: 0.587, Lambda_2: 0.003389, Time: 0.28\n",
            "It: 1650, Loss: 5.895e-03, Lambda_1: 0.589, Lambda_2: 0.003392, Time: 0.27\n",
            "It: 1660, Loss: 5.850e-03, Lambda_1: 0.591, Lambda_2: 0.003396, Time: 0.27\n",
            "It: 1670, Loss: 5.806e-03, Lambda_1: 0.593, Lambda_2: 0.003400, Time: 0.28\n",
            "It: 1680, Loss: 5.762e-03, Lambda_1: 0.595, Lambda_2: 0.003403, Time: 0.26\n",
            "It: 1690, Loss: 5.718e-03, Lambda_1: 0.598, Lambda_2: 0.003407, Time: 0.27\n",
            "It: 1700, Loss: 5.675e-03, Lambda_1: 0.600, Lambda_2: 0.003411, Time: 0.26\n",
            "It: 1710, Loss: 5.632e-03, Lambda_1: 0.602, Lambda_2: 0.003415, Time: 0.27\n",
            "It: 1720, Loss: 5.589e-03, Lambda_1: 0.604, Lambda_2: 0.003419, Time: 0.26\n",
            "It: 1730, Loss: 5.546e-03, Lambda_1: 0.606, Lambda_2: 0.003422, Time: 0.27\n",
            "It: 1740, Loss: 5.504e-03, Lambda_1: 0.608, Lambda_2: 0.003426, Time: 0.27\n",
            "It: 1750, Loss: 5.462e-03, Lambda_1: 0.610, Lambda_2: 0.003430, Time: 0.27\n",
            "It: 1760, Loss: 5.420e-03, Lambda_1: 0.612, Lambda_2: 0.003434, Time: 0.28\n",
            "It: 1770, Loss: 5.379e-03, Lambda_1: 0.614, Lambda_2: 0.003438, Time: 0.27\n",
            "It: 1780, Loss: 5.337e-03, Lambda_1: 0.616, Lambda_2: 0.003442, Time: 0.28\n",
            "It: 1790, Loss: 5.296e-03, Lambda_1: 0.618, Lambda_2: 0.003446, Time: 0.27\n",
            "It: 1800, Loss: 5.255e-03, Lambda_1: 0.620, Lambda_2: 0.003450, Time: 0.28\n",
            "It: 1810, Loss: 5.214e-03, Lambda_1: 0.622, Lambda_2: 0.003454, Time: 0.26\n",
            "It: 1820, Loss: 5.173e-03, Lambda_1: 0.624, Lambda_2: 0.003458, Time: 0.27\n",
            "It: 1830, Loss: 5.133e-03, Lambda_1: 0.626, Lambda_2: 0.003463, Time: 0.27\n",
            "It: 1840, Loss: 5.092e-03, Lambda_1: 0.628, Lambda_2: 0.003467, Time: 0.27\n",
            "It: 1850, Loss: 5.052e-03, Lambda_1: 0.630, Lambda_2: 0.003471, Time: 0.27\n",
            "It: 1860, Loss: 5.012e-03, Lambda_1: 0.632, Lambda_2: 0.003475, Time: 0.26\n",
            "It: 1870, Loss: 4.972e-03, Lambda_1: 0.634, Lambda_2: 0.003479, Time: 0.27\n",
            "It: 1880, Loss: 4.932e-03, Lambda_1: 0.636, Lambda_2: 0.003484, Time: 0.26\n",
            "It: 1890, Loss: 4.892e-03, Lambda_1: 0.638, Lambda_2: 0.003488, Time: 0.28\n",
            "It: 1900, Loss: 4.852e-03, Lambda_1: 0.640, Lambda_2: 0.003492, Time: 0.27\n",
            "It: 1910, Loss: 4.812e-03, Lambda_1: 0.642, Lambda_2: 0.003496, Time: 0.28\n",
            "It: 1920, Loss: 4.773e-03, Lambda_1: 0.644, Lambda_2: 0.003501, Time: 0.26\n",
            "It: 1930, Loss: 4.733e-03, Lambda_1: 0.646, Lambda_2: 0.003505, Time: 0.28\n",
            "It: 1940, Loss: 4.694e-03, Lambda_1: 0.648, Lambda_2: 0.003509, Time: 0.30\n",
            "It: 1950, Loss: 4.655e-03, Lambda_1: 0.650, Lambda_2: 0.003514, Time: 0.27\n",
            "It: 1960, Loss: 4.616e-03, Lambda_1: 0.652, Lambda_2: 0.003518, Time: 0.28\n",
            "It: 1970, Loss: 4.576e-03, Lambda_1: 0.654, Lambda_2: 0.003523, Time: 0.28\n",
            "It: 1980, Loss: 4.537e-03, Lambda_1: 0.656, Lambda_2: 0.003527, Time: 0.27\n",
            "It: 1990, Loss: 4.498e-03, Lambda_1: 0.657, Lambda_2: 0.003532, Time: 0.28\n",
            "It: 2000, Loss: 4.460e-03, Lambda_1: 0.659, Lambda_2: 0.003536, Time: 0.27\n",
            "It: 2010, Loss: 4.421e-03, Lambda_1: 0.661, Lambda_2: 0.003540, Time: 0.26\n",
            "It: 2020, Loss: 4.382e-03, Lambda_1: 0.663, Lambda_2: 0.003545, Time: 0.26\n",
            "It: 2030, Loss: 4.343e-03, Lambda_1: 0.665, Lambda_2: 0.003550, Time: 0.28\n",
            "It: 2040, Loss: 4.305e-03, Lambda_1: 0.667, Lambda_2: 0.003554, Time: 0.28\n",
            "It: 2050, Loss: 4.266e-03, Lambda_1: 0.669, Lambda_2: 0.003559, Time: 0.27\n",
            "It: 2060, Loss: 4.228e-03, Lambda_1: 0.671, Lambda_2: 0.003563, Time: 0.27\n",
            "It: 2070, Loss: 4.189e-03, Lambda_1: 0.673, Lambda_2: 0.003568, Time: 0.28\n",
            "It: 2080, Loss: 4.151e-03, Lambda_1: 0.675, Lambda_2: 0.003572, Time: 0.27\n",
            "It: 2090, Loss: 4.113e-03, Lambda_1: 0.677, Lambda_2: 0.003577, Time: 0.28\n",
            "It: 2100, Loss: 4.075e-03, Lambda_1: 0.679, Lambda_2: 0.003582, Time: 0.27\n",
            "It: 2110, Loss: 4.037e-03, Lambda_1: 0.681, Lambda_2: 0.003586, Time: 0.28\n",
            "It: 2120, Loss: 3.999e-03, Lambda_1: 0.682, Lambda_2: 0.003591, Time: 0.27\n",
            "It: 2130, Loss: 3.961e-03, Lambda_1: 0.684, Lambda_2: 0.003596, Time: 0.28\n",
            "It: 2140, Loss: 3.923e-03, Lambda_1: 0.686, Lambda_2: 0.003601, Time: 0.29\n",
            "It: 2150, Loss: 3.885e-03, Lambda_1: 0.688, Lambda_2: 0.003605, Time: 0.28\n",
            "It: 2160, Loss: 3.848e-03, Lambda_1: 0.690, Lambda_2: 0.003610, Time: 0.27\n",
            "It: 2170, Loss: 3.810e-03, Lambda_1: 0.692, Lambda_2: 0.003615, Time: 0.29\n",
            "It: 2180, Loss: 3.773e-03, Lambda_1: 0.694, Lambda_2: 0.003620, Time: 0.30\n",
            "It: 2190, Loss: 3.736e-03, Lambda_1: 0.696, Lambda_2: 0.003625, Time: 0.28\n",
            "It: 2200, Loss: 3.699e-03, Lambda_1: 0.698, Lambda_2: 0.003629, Time: 0.29\n",
            "It: 2210, Loss: 3.661e-03, Lambda_1: 0.699, Lambda_2: 0.003634, Time: 0.30\n",
            "It: 2220, Loss: 3.625e-03, Lambda_1: 0.701, Lambda_2: 0.003639, Time: 0.28\n",
            "It: 2230, Loss: 3.588e-03, Lambda_1: 0.703, Lambda_2: 0.003644, Time: 0.27\n",
            "It: 2240, Loss: 3.551e-03, Lambda_1: 0.705, Lambda_2: 0.003649, Time: 0.26\n",
            "It: 2250, Loss: 3.515e-03, Lambda_1: 0.707, Lambda_2: 0.003654, Time: 0.29\n",
            "It: 2260, Loss: 3.478e-03, Lambda_1: 0.709, Lambda_2: 0.003659, Time: 0.30\n",
            "It: 2270, Loss: 3.442e-03, Lambda_1: 0.711, Lambda_2: 0.003664, Time: 0.28\n",
            "It: 2280, Loss: 3.406e-03, Lambda_1: 0.713, Lambda_2: 0.003669, Time: 0.29\n",
            "It: 2290, Loss: 3.370e-03, Lambda_1: 0.714, Lambda_2: 0.003674, Time: 0.30\n",
            "It: 2300, Loss: 3.334e-03, Lambda_1: 0.716, Lambda_2: 0.003679, Time: 0.29\n",
            "It: 2310, Loss: 3.298e-03, Lambda_1: 0.718, Lambda_2: 0.003684, Time: 0.29\n",
            "It: 2320, Loss: 3.263e-03, Lambda_1: 0.720, Lambda_2: 0.003689, Time: 0.29\n",
            "It: 2330, Loss: 3.227e-03, Lambda_1: 0.722, Lambda_2: 0.003694, Time: 0.28\n",
            "It: 2340, Loss: 3.192e-03, Lambda_1: 0.724, Lambda_2: 0.003699, Time: 0.29\n",
            "It: 2350, Loss: 3.157e-03, Lambda_1: 0.726, Lambda_2: 0.003704, Time: 0.29\n",
            "It: 2360, Loss: 3.122e-03, Lambda_1: 0.727, Lambda_2: 0.003709, Time: 0.28\n",
            "It: 2370, Loss: 3.087e-03, Lambda_1: 0.729, Lambda_2: 0.003714, Time: 0.28\n",
            "It: 2380, Loss: 3.053e-03, Lambda_1: 0.731, Lambda_2: 0.003719, Time: 0.29\n",
            "It: 2390, Loss: 3.019e-03, Lambda_1: 0.733, Lambda_2: 0.003724, Time: 0.29\n",
            "It: 2400, Loss: 2.984e-03, Lambda_1: 0.735, Lambda_2: 0.003729, Time: 0.27\n",
            "It: 2410, Loss: 2.950e-03, Lambda_1: 0.737, Lambda_2: 0.003734, Time: 0.27\n",
            "It: 2420, Loss: 2.917e-03, Lambda_1: 0.739, Lambda_2: 0.003739, Time: 0.28\n",
            "It: 2430, Loss: 2.883e-03, Lambda_1: 0.740, Lambda_2: 0.003744, Time: 0.27\n",
            "It: 2440, Loss: 2.850e-03, Lambda_1: 0.742, Lambda_2: 0.003749, Time: 0.28\n",
            "It: 2450, Loss: 2.816e-03, Lambda_1: 0.744, Lambda_2: 0.003755, Time: 0.28\n",
            "It: 2460, Loss: 2.783e-03, Lambda_1: 0.746, Lambda_2: 0.003760, Time: 0.27\n",
            "It: 2470, Loss: 2.751e-03, Lambda_1: 0.748, Lambda_2: 0.003765, Time: 0.28\n",
            "It: 2480, Loss: 2.718e-03, Lambda_1: 0.750, Lambda_2: 0.003770, Time: 0.30\n",
            "It: 2490, Loss: 2.686e-03, Lambda_1: 0.751, Lambda_2: 0.003775, Time: 0.30\n",
            "It: 2500, Loss: 2.654e-03, Lambda_1: 0.753, Lambda_2: 0.003780, Time: 0.28\n",
            "It: 2510, Loss: 2.622e-03, Lambda_1: 0.755, Lambda_2: 0.003785, Time: 0.27\n",
            "It: 2520, Loss: 2.590e-03, Lambda_1: 0.757, Lambda_2: 0.003790, Time: 0.28\n",
            "It: 2530, Loss: 2.558e-03, Lambda_1: 0.759, Lambda_2: 0.003795, Time: 0.28\n",
            "It: 2540, Loss: 2.527e-03, Lambda_1: 0.760, Lambda_2: 0.003800, Time: 0.28\n",
            "It: 2550, Loss: 2.496e-03, Lambda_1: 0.762, Lambda_2: 0.003805, Time: 0.29\n",
            "It: 2560, Loss: 2.466e-03, Lambda_1: 0.764, Lambda_2: 0.003810, Time: 0.29\n",
            "It: 2570, Loss: 2.435e-03, Lambda_1: 0.766, Lambda_2: 0.003814, Time: 0.30\n",
            "It: 2580, Loss: 2.405e-03, Lambda_1: 0.768, Lambda_2: 0.003819, Time: 0.28\n",
            "It: 2590, Loss: 2.375e-03, Lambda_1: 0.769, Lambda_2: 0.003824, Time: 0.30\n",
            "It: 2600, Loss: 2.345e-03, Lambda_1: 0.771, Lambda_2: 0.003829, Time: 0.30\n",
            "It: 2610, Loss: 2.316e-03, Lambda_1: 0.773, Lambda_2: 0.003834, Time: 0.30\n",
            "It: 2620, Loss: 2.340e-03, Lambda_1: 0.775, Lambda_2: 0.003839, Time: 0.29\n",
            "It: 2630, Loss: 2.326e-03, Lambda_1: 0.777, Lambda_2: 0.003842, Time: 0.28\n",
            "It: 2640, Loss: 2.386e-03, Lambda_1: 0.778, Lambda_2: 0.003847, Time: 0.28\n",
            "It: 2650, Loss: 2.375e-03, Lambda_1: 0.780, Lambda_2: 0.003850, Time: 0.29\n",
            "It: 2660, Loss: 2.187e-03, Lambda_1: 0.782, Lambda_2: 0.003855, Time: 0.28\n",
            "It: 2670, Loss: 2.387e-03, Lambda_1: 0.783, Lambda_2: 0.003860, Time: 0.27\n",
            "It: 2680, Loss: 2.323e-03, Lambda_1: 0.785, Lambda_2: 0.003862, Time: 0.27\n",
            "It: 2690, Loss: 2.150e-03, Lambda_1: 0.787, Lambda_2: 0.003865, Time: 0.28\n",
            "It: 2700, Loss: 2.094e-03, Lambda_1: 0.788, Lambda_2: 0.003870, Time: 0.28\n",
            "It: 2710, Loss: 2.086e-03, Lambda_1: 0.789, Lambda_2: 0.003874, Time: 0.28\n",
            "It: 2720, Loss: 2.032e-03, Lambda_1: 0.791, Lambda_2: 0.003878, Time: 0.28\n",
            "It: 2730, Loss: 2.013e-03, Lambda_1: 0.792, Lambda_2: 0.003882, Time: 0.29\n",
            "It: 2740, Loss: 1.980e-03, Lambda_1: 0.794, Lambda_2: 0.003885, Time: 0.28\n",
            "It: 2750, Loss: 1.983e-03, Lambda_1: 0.795, Lambda_2: 0.003888, Time: 0.28\n",
            "It: 2760, Loss: 1.941e-03, Lambda_1: 0.797, Lambda_2: 0.003892, Time: 0.30\n",
            "It: 2770, Loss: 1.949e-03, Lambda_1: 0.798, Lambda_2: 0.003894, Time: 0.26\n",
            "It: 2780, Loss: 2.060e-03, Lambda_1: 0.800, Lambda_2: 0.003898, Time: 0.28\n",
            "It: 2790, Loss: 1.991e-03, Lambda_1: 0.802, Lambda_2: 0.003900, Time: 0.29\n",
            "It: 2800, Loss: 2.678e-03, Lambda_1: 0.803, Lambda_2: 0.003902, Time: 0.29\n",
            "It: 2810, Loss: 2.160e-03, Lambda_1: 0.805, Lambda_2: 0.003903, Time: 0.29\n",
            "It: 2820, Loss: 1.900e-03, Lambda_1: 0.806, Lambda_2: 0.003907, Time: 0.28\n",
            "It: 2830, Loss: 2.025e-03, Lambda_1: 0.807, Lambda_2: 0.003910, Time: 0.29\n",
            "It: 2840, Loss: 1.888e-03, Lambda_1: 0.808, Lambda_2: 0.003913, Time: 0.29\n",
            "It: 2850, Loss: 1.775e-03, Lambda_1: 0.809, Lambda_2: 0.003916, Time: 0.29\n",
            "It: 2860, Loss: 1.775e-03, Lambda_1: 0.810, Lambda_2: 0.003919, Time: 0.28\n",
            "It: 2870, Loss: 1.709e-03, Lambda_1: 0.812, Lambda_2: 0.003922, Time: 0.28\n",
            "It: 2880, Loss: 1.698e-03, Lambda_1: 0.813, Lambda_2: 0.003925, Time: 0.28\n",
            "It: 2890, Loss: 1.666e-03, Lambda_1: 0.814, Lambda_2: 0.003927, Time: 0.27\n",
            "It: 2900, Loss: 1.695e-03, Lambda_1: 0.816, Lambda_2: 0.003930, Time: 0.28\n",
            "It: 2910, Loss: 1.637e-03, Lambda_1: 0.817, Lambda_2: 0.003932, Time: 0.27\n",
            "It: 2920, Loss: 1.660e-03, Lambda_1: 0.819, Lambda_2: 0.003933, Time: 0.27\n",
            "It: 2930, Loss: 1.836e-03, Lambda_1: 0.820, Lambda_2: 0.003936, Time: 0.26\n",
            "It: 2940, Loss: 1.743e-03, Lambda_1: 0.822, Lambda_2: 0.003937, Time: 0.28\n",
            "It: 2950, Loss: 1.605e-03, Lambda_1: 0.823, Lambda_2: 0.003939, Time: 0.28\n",
            "It: 2960, Loss: 1.567e-03, Lambda_1: 0.824, Lambda_2: 0.003941, Time: 0.28\n",
            "It: 2970, Loss: 1.527e-03, Lambda_1: 0.825, Lambda_2: 0.003944, Time: 0.28\n",
            "It: 2980, Loss: 1.553e-03, Lambda_1: 0.826, Lambda_2: 0.003946, Time: 0.28\n",
            "It: 2990, Loss: 1.516e-03, Lambda_1: 0.828, Lambda_2: 0.003948, Time: 0.28\n",
            "It: 3000, Loss: 1.484e-03, Lambda_1: 0.829, Lambda_2: 0.003950, Time: 0.27\n",
            "It: 3010, Loss: 1.680e-03, Lambda_1: 0.830, Lambda_2: 0.003952, Time: 0.29\n",
            "It: 3020, Loss: 2.201e-03, Lambda_1: 0.832, Lambda_2: 0.003954, Time: 0.28\n",
            "It: 3030, Loss: 1.569e-03, Lambda_1: 0.833, Lambda_2: 0.003953, Time: 0.27\n",
            "It: 3040, Loss: 1.669e-03, Lambda_1: 0.834, Lambda_2: 0.003955, Time: 0.27\n",
            "It: 3050, Loss: 1.612e-03, Lambda_1: 0.835, Lambda_2: 0.003957, Time: 0.28\n",
            "It: 3060, Loss: 1.826e-03, Lambda_1: 0.836, Lambda_2: 0.003959, Time: 0.27\n",
            "It: 3070, Loss: 1.546e-03, Lambda_1: 0.837, Lambda_2: 0.003960, Time: 0.27\n",
            "It: 3080, Loss: 1.397e-03, Lambda_1: 0.838, Lambda_2: 0.003962, Time: 0.29\n",
            "It: 3090, Loss: 1.355e-03, Lambda_1: 0.839, Lambda_2: 0.003965, Time: 0.27\n",
            "It: 3100, Loss: 1.329e-03, Lambda_1: 0.840, Lambda_2: 0.003967, Time: 0.26\n",
            "It: 3110, Loss: 1.318e-03, Lambda_1: 0.841, Lambda_2: 0.003969, Time: 0.27\n",
            "It: 3120, Loss: 1.314e-03, Lambda_1: 0.843, Lambda_2: 0.003970, Time: 0.29\n",
            "It: 3130, Loss: 1.328e-03, Lambda_1: 0.844, Lambda_2: 0.003972, Time: 0.28\n",
            "It: 3140, Loss: 1.302e-03, Lambda_1: 0.845, Lambda_2: 0.003973, Time: 0.26\n",
            "It: 3150, Loss: 1.253e-03, Lambda_1: 0.846, Lambda_2: 0.003974, Time: 0.28\n",
            "It: 3160, Loss: 1.295e-03, Lambda_1: 0.848, Lambda_2: 0.003975, Time: 0.26\n",
            "It: 3170, Loss: 2.013e-03, Lambda_1: 0.849, Lambda_2: 0.003976, Time: 0.28\n",
            "It: 3180, Loss: 1.406e-03, Lambda_1: 0.850, Lambda_2: 0.003977, Time: 0.26\n",
            "It: 3190, Loss: 1.262e-03, Lambda_1: 0.851, Lambda_2: 0.003977, Time: 0.28\n",
            "It: 3200, Loss: 1.232e-03, Lambda_1: 0.852, Lambda_2: 0.003980, Time: 0.27\n",
            "It: 3210, Loss: 1.287e-03, Lambda_1: 0.853, Lambda_2: 0.003982, Time: 0.27\n",
            "It: 3220, Loss: 1.207e-03, Lambda_1: 0.854, Lambda_2: 0.003983, Time: 0.27\n",
            "It: 3230, Loss: 1.733e-03, Lambda_1: 0.855, Lambda_2: 0.003984, Time: 0.28\n",
            "It: 3240, Loss: 2.283e-02, Lambda_1: 0.856, Lambda_2: 0.003985, Time: 0.27\n",
            "It: 3250, Loss: 2.291e-01, Lambda_1: 0.847, Lambda_2: 0.003962, Time: 0.27\n",
            "It: 3260, Loss: 5.401e-02, Lambda_1: 0.851, Lambda_2: 0.003916, Time: 0.28\n",
            "It: 3270, Loss: 3.889e-02, Lambda_1: 0.845, Lambda_2: 0.003915, Time: 0.26\n",
            "It: 3280, Loss: 1.997e-02, Lambda_1: 0.839, Lambda_2: 0.003921, Time: 0.27\n",
            "It: 3290, Loss: 1.219e-02, Lambda_1: 0.832, Lambda_2: 0.003940, Time: 0.28\n",
            "It: 3300, Loss: 8.511e-03, Lambda_1: 0.825, Lambda_2: 0.003959, Time: 0.28\n",
            "It: 3310, Loss: 6.515e-03, Lambda_1: 0.819, Lambda_2: 0.003973, Time: 0.27\n",
            "It: 3320, Loss: 5.050e-03, Lambda_1: 0.815, Lambda_2: 0.003984, Time: 0.27\n",
            "It: 3330, Loss: 3.998e-03, Lambda_1: 0.811, Lambda_2: 0.003995, Time: 0.26\n",
            "It: 3340, Loss: 3.275e-03, Lambda_1: 0.808, Lambda_2: 0.004003, Time: 0.28\n",
            "It: 3350, Loss: 2.793e-03, Lambda_1: 0.806, Lambda_2: 0.004011, Time: 0.29\n",
            "It: 3360, Loss: 2.455e-03, Lambda_1: 0.804, Lambda_2: 0.004017, Time: 0.27\n",
            "It: 3370, Loss: 2.215e-03, Lambda_1: 0.803, Lambda_2: 0.004022, Time: 0.28\n",
            "It: 3380, Loss: 2.042e-03, Lambda_1: 0.802, Lambda_2: 0.004027, Time: 0.26\n",
            "It: 3390, Loss: 1.916e-03, Lambda_1: 0.802, Lambda_2: 0.004030, Time: 0.26\n",
            "It: 3400, Loss: 1.822e-03, Lambda_1: 0.802, Lambda_2: 0.004034, Time: 0.28\n",
            "It: 3410, Loss: 1.752e-03, Lambda_1: 0.802, Lambda_2: 0.004036, Time: 0.28\n",
            "It: 3420, Loss: 1.698e-03, Lambda_1: 0.802, Lambda_2: 0.004039, Time: 0.27\n",
            "It: 3430, Loss: 1.656e-03, Lambda_1: 0.803, Lambda_2: 0.004041, Time: 0.27\n",
            "It: 3440, Loss: 1.622e-03, Lambda_1: 0.803, Lambda_2: 0.004043, Time: 0.27\n",
            "It: 3450, Loss: 1.595e-03, Lambda_1: 0.804, Lambda_2: 0.004045, Time: 0.25\n",
            "It: 3460, Loss: 1.571e-03, Lambda_1: 0.804, Lambda_2: 0.004046, Time: 0.27\n",
            "It: 3470, Loss: 1.551e-03, Lambda_1: 0.805, Lambda_2: 0.004048, Time: 0.28\n",
            "It: 3480, Loss: 1.532e-03, Lambda_1: 0.805, Lambda_2: 0.004049, Time: 0.27\n",
            "It: 3490, Loss: 1.516e-03, Lambda_1: 0.806, Lambda_2: 0.004050, Time: 0.28\n",
            "It: 3500, Loss: 1.501e-03, Lambda_1: 0.807, Lambda_2: 0.004051, Time: 0.28\n",
            "It: 3510, Loss: 1.487e-03, Lambda_1: 0.808, Lambda_2: 0.004052, Time: 0.26\n",
            "It: 3520, Loss: 1.473e-03, Lambda_1: 0.809, Lambda_2: 0.004053, Time: 0.28\n",
            "It: 3530, Loss: 1.461e-03, Lambda_1: 0.809, Lambda_2: 0.004053, Time: 0.27\n",
            "It: 3540, Loss: 1.449e-03, Lambda_1: 0.810, Lambda_2: 0.004054, Time: 0.27\n",
            "It: 3550, Loss: 1.437e-03, Lambda_1: 0.811, Lambda_2: 0.004055, Time: 0.27\n",
            "It: 3560, Loss: 1.426e-03, Lambda_1: 0.812, Lambda_2: 0.004056, Time: 0.26\n",
            "It: 3570, Loss: 1.415e-03, Lambda_1: 0.813, Lambda_2: 0.004056, Time: 0.27\n",
            "It: 3580, Loss: 1.404e-03, Lambda_1: 0.814, Lambda_2: 0.004057, Time: 0.29\n",
            "It: 3590, Loss: 1.394e-03, Lambda_1: 0.814, Lambda_2: 0.004058, Time: 0.28\n",
            "It: 3600, Loss: 1.384e-03, Lambda_1: 0.815, Lambda_2: 0.004059, Time: 0.27\n",
            "It: 3610, Loss: 1.374e-03, Lambda_1: 0.816, Lambda_2: 0.004059, Time: 0.27\n",
            "It: 3620, Loss: 1.365e-03, Lambda_1: 0.817, Lambda_2: 0.004060, Time: 0.27\n",
            "It: 3630, Loss: 1.355e-03, Lambda_1: 0.818, Lambda_2: 0.004060, Time: 0.29\n",
            "It: 3640, Loss: 1.346e-03, Lambda_1: 0.819, Lambda_2: 0.004061, Time: 0.27\n",
            "It: 3650, Loss: 1.337e-03, Lambda_1: 0.820, Lambda_2: 0.004062, Time: 0.28\n",
            "It: 3660, Loss: 1.328e-03, Lambda_1: 0.821, Lambda_2: 0.004062, Time: 0.28\n",
            "It: 3670, Loss: 1.320e-03, Lambda_1: 0.821, Lambda_2: 0.004063, Time: 0.27\n",
            "It: 3680, Loss: 1.311e-03, Lambda_1: 0.822, Lambda_2: 0.004064, Time: 0.28\n",
            "It: 3690, Loss: 1.302e-03, Lambda_1: 0.823, Lambda_2: 0.004064, Time: 0.28\n",
            "It: 3700, Loss: 1.294e-03, Lambda_1: 0.824, Lambda_2: 0.004065, Time: 0.28\n",
            "It: 3710, Loss: 1.286e-03, Lambda_1: 0.825, Lambda_2: 0.004066, Time: 0.28\n",
            "It: 3720, Loss: 1.277e-03, Lambda_1: 0.826, Lambda_2: 0.004066, Time: 0.26\n",
            "It: 3730, Loss: 1.269e-03, Lambda_1: 0.827, Lambda_2: 0.004067, Time: 0.29\n",
            "It: 3740, Loss: 1.261e-03, Lambda_1: 0.827, Lambda_2: 0.004068, Time: 0.27\n",
            "It: 3750, Loss: 1.253e-03, Lambda_1: 0.828, Lambda_2: 0.004068, Time: 0.27\n",
            "It: 3760, Loss: 1.246e-03, Lambda_1: 0.829, Lambda_2: 0.004069, Time: 0.26\n",
            "It: 3770, Loss: 1.238e-03, Lambda_1: 0.830, Lambda_2: 0.004070, Time: 0.28\n",
            "It: 3780, Loss: 1.230e-03, Lambda_1: 0.831, Lambda_2: 0.004070, Time: 0.27\n",
            "It: 3790, Loss: 1.223e-03, Lambda_1: 0.832, Lambda_2: 0.004071, Time: 0.26\n",
            "It: 3800, Loss: 1.215e-03, Lambda_1: 0.833, Lambda_2: 0.004072, Time: 0.26\n",
            "It: 3810, Loss: 1.208e-03, Lambda_1: 0.833, Lambda_2: 0.004072, Time: 0.28\n",
            "It: 3820, Loss: 1.200e-03, Lambda_1: 0.834, Lambda_2: 0.004073, Time: 0.27\n",
            "It: 3830, Loss: 1.193e-03, Lambda_1: 0.835, Lambda_2: 0.004074, Time: 0.27\n",
            "It: 3840, Loss: 1.186e-03, Lambda_1: 0.836, Lambda_2: 0.004074, Time: 0.29\n",
            "It: 3850, Loss: 1.179e-03, Lambda_1: 0.837, Lambda_2: 0.004075, Time: 0.27\n",
            "It: 3860, Loss: 1.171e-03, Lambda_1: 0.838, Lambda_2: 0.004076, Time: 0.27\n",
            "It: 3870, Loss: 1.164e-03, Lambda_1: 0.838, Lambda_2: 0.004076, Time: 0.27\n",
            "It: 3880, Loss: 1.157e-03, Lambda_1: 0.839, Lambda_2: 0.004077, Time: 0.28\n",
            "It: 3890, Loss: 1.151e-03, Lambda_1: 0.840, Lambda_2: 0.004078, Time: 0.27\n",
            "It: 3900, Loss: 1.144e-03, Lambda_1: 0.841, Lambda_2: 0.004078, Time: 0.27\n",
            "It: 3910, Loss: 1.137e-03, Lambda_1: 0.842, Lambda_2: 0.004079, Time: 0.29\n",
            "It: 3920, Loss: 1.130e-03, Lambda_1: 0.843, Lambda_2: 0.004080, Time: 0.27\n",
            "It: 3930, Loss: 1.123e-03, Lambda_1: 0.843, Lambda_2: 0.004080, Time: 0.26\n",
            "It: 3940, Loss: 1.117e-03, Lambda_1: 0.844, Lambda_2: 0.004081, Time: 0.27\n",
            "It: 3950, Loss: 1.110e-03, Lambda_1: 0.845, Lambda_2: 0.004082, Time: 0.28\n",
            "It: 3960, Loss: 1.104e-03, Lambda_1: 0.846, Lambda_2: 0.004082, Time: 0.27\n",
            "It: 3970, Loss: 1.097e-03, Lambda_1: 0.847, Lambda_2: 0.004083, Time: 0.26\n",
            "It: 3980, Loss: 1.091e-03, Lambda_1: 0.847, Lambda_2: 0.004084, Time: 0.26\n",
            "It: 3990, Loss: 1.084e-03, Lambda_1: 0.848, Lambda_2: 0.004084, Time: 0.27\n",
            "It: 4000, Loss: 1.078e-03, Lambda_1: 0.849, Lambda_2: 0.004085, Time: 0.27\n",
            "It: 4010, Loss: 1.072e-03, Lambda_1: 0.850, Lambda_2: 0.004086, Time: 0.28\n",
            "It: 4020, Loss: 1.066e-03, Lambda_1: 0.851, Lambda_2: 0.004086, Time: 0.27\n",
            "It: 4030, Loss: 1.059e-03, Lambda_1: 0.851, Lambda_2: 0.004087, Time: 0.27\n",
            "It: 4040, Loss: 1.053e-03, Lambda_1: 0.852, Lambda_2: 0.004088, Time: 0.27\n",
            "It: 4050, Loss: 1.047e-03, Lambda_1: 0.853, Lambda_2: 0.004088, Time: 0.28\n",
            "It: 4060, Loss: 1.041e-03, Lambda_1: 0.854, Lambda_2: 0.004089, Time: 0.28\n",
            "It: 4070, Loss: 1.035e-03, Lambda_1: 0.855, Lambda_2: 0.004090, Time: 0.29\n",
            "It: 4080, Loss: 1.029e-03, Lambda_1: 0.855, Lambda_2: 0.004090, Time: 0.27\n",
            "It: 4090, Loss: 1.023e-03, Lambda_1: 0.856, Lambda_2: 0.004091, Time: 0.27\n",
            "It: 4100, Loss: 1.017e-03, Lambda_1: 0.857, Lambda_2: 0.004092, Time: 0.26\n",
            "It: 4110, Loss: 1.011e-03, Lambda_1: 0.858, Lambda_2: 0.004092, Time: 0.27\n",
            "It: 4120, Loss: 1.006e-03, Lambda_1: 0.858, Lambda_2: 0.004093, Time: 0.28\n",
            "It: 4130, Loss: 9.999e-04, Lambda_1: 0.859, Lambda_2: 0.004094, Time: 0.32\n",
            "It: 4140, Loss: 9.941e-04, Lambda_1: 0.860, Lambda_2: 0.004094, Time: 0.29\n",
            "It: 4150, Loss: 9.885e-04, Lambda_1: 0.861, Lambda_2: 0.004095, Time: 0.27\n",
            "It: 4160, Loss: 9.828e-04, Lambda_1: 0.861, Lambda_2: 0.004096, Time: 0.29\n",
            "It: 4170, Loss: 9.772e-04, Lambda_1: 0.862, Lambda_2: 0.004096, Time: 0.27\n",
            "It: 4180, Loss: 9.716e-04, Lambda_1: 0.863, Lambda_2: 0.004097, Time: 0.28\n",
            "It: 4190, Loss: 9.661e-04, Lambda_1: 0.864, Lambda_2: 0.004098, Time: 0.28\n",
            "It: 4200, Loss: 9.606e-04, Lambda_1: 0.864, Lambda_2: 0.004098, Time: 0.28\n",
            "It: 4210, Loss: 9.551e-04, Lambda_1: 0.865, Lambda_2: 0.004099, Time: 0.28\n",
            "It: 4220, Loss: 9.497e-04, Lambda_1: 0.866, Lambda_2: 0.004100, Time: 0.27\n",
            "It: 4230, Loss: 9.443e-04, Lambda_1: 0.867, Lambda_2: 0.004100, Time: 0.27\n",
            "It: 4240, Loss: 9.389e-04, Lambda_1: 0.867, Lambda_2: 0.004101, Time: 0.30\n",
            "It: 4250, Loss: 9.336e-04, Lambda_1: 0.868, Lambda_2: 0.004102, Time: 0.33\n",
            "It: 4260, Loss: 9.283e-04, Lambda_1: 0.869, Lambda_2: 0.004102, Time: 0.30\n",
            "It: 4270, Loss: 9.230e-04, Lambda_1: 0.870, Lambda_2: 0.004103, Time: 0.32\n",
            "It: 4280, Loss: 9.178e-04, Lambda_1: 0.870, Lambda_2: 0.004104, Time: 0.31\n",
            "It: 4290, Loss: 9.126e-04, Lambda_1: 0.871, Lambda_2: 0.004104, Time: 0.30\n",
            "It: 4300, Loss: 9.074e-04, Lambda_1: 0.872, Lambda_2: 0.004105, Time: 0.32\n",
            "It: 4310, Loss: 9.023e-04, Lambda_1: 0.872, Lambda_2: 0.004105, Time: 0.30\n",
            "It: 4320, Loss: 8.971e-04, Lambda_1: 0.873, Lambda_2: 0.004106, Time: 0.29\n",
            "It: 4330, Loss: 8.920e-04, Lambda_1: 0.874, Lambda_2: 0.004107, Time: 0.31\n",
            "It: 4340, Loss: 8.870e-04, Lambda_1: 0.874, Lambda_2: 0.004107, Time: 0.30\n",
            "It: 4350, Loss: 8.820e-04, Lambda_1: 0.875, Lambda_2: 0.004108, Time: 0.30\n",
            "It: 4360, Loss: 8.769e-04, Lambda_1: 0.876, Lambda_2: 0.004109, Time: 0.34\n",
            "It: 4370, Loss: 8.720e-04, Lambda_1: 0.877, Lambda_2: 0.004109, Time: 0.32\n",
            "It: 4380, Loss: 8.670e-04, Lambda_1: 0.877, Lambda_2: 0.004110, Time: 0.31\n",
            "It: 4390, Loss: 8.621e-04, Lambda_1: 0.878, Lambda_2: 0.004111, Time: 0.30\n",
            "It: 4400, Loss: 8.572e-04, Lambda_1: 0.879, Lambda_2: 0.004111, Time: 0.32\n",
            "It: 4410, Loss: 8.523e-04, Lambda_1: 0.879, Lambda_2: 0.004112, Time: 0.30\n",
            "It: 4420, Loss: 8.474e-04, Lambda_1: 0.880, Lambda_2: 0.004113, Time: 0.30\n",
            "It: 4430, Loss: 8.426e-04, Lambda_1: 0.881, Lambda_2: 0.004113, Time: 0.30\n",
            "It: 4440, Loss: 8.378e-04, Lambda_1: 0.881, Lambda_2: 0.004114, Time: 0.31\n",
            "It: 4450, Loss: 8.330e-04, Lambda_1: 0.882, Lambda_2: 0.004114, Time: 0.30\n",
            "It: 4460, Loss: 8.283e-04, Lambda_1: 0.883, Lambda_2: 0.004115, Time: 0.33\n",
            "It: 4470, Loss: 8.235e-04, Lambda_1: 0.883, Lambda_2: 0.004116, Time: 0.31\n",
            "It: 4480, Loss: 8.188e-04, Lambda_1: 0.884, Lambda_2: 0.004116, Time: 0.32\n",
            "It: 4490, Loss: 8.142e-04, Lambda_1: 0.885, Lambda_2: 0.004117, Time: 0.32\n",
            "It: 4500, Loss: 8.095e-04, Lambda_1: 0.885, Lambda_2: 0.004118, Time: 0.31\n",
            "It: 4510, Loss: 8.048e-04, Lambda_1: 0.886, Lambda_2: 0.004118, Time: 0.32\n",
            "It: 4520, Loss: 8.002e-04, Lambda_1: 0.887, Lambda_2: 0.004119, Time: 0.33\n",
            "It: 4530, Loss: 7.956e-04, Lambda_1: 0.887, Lambda_2: 0.004119, Time: 0.31\n",
            "It: 4540, Loss: 7.910e-04, Lambda_1: 0.888, Lambda_2: 0.004120, Time: 0.32\n",
            "It: 4550, Loss: 7.865e-04, Lambda_1: 0.888, Lambda_2: 0.004121, Time: 0.37\n",
            "It: 4560, Loss: 7.819e-04, Lambda_1: 0.889, Lambda_2: 0.004121, Time: 0.40\n",
            "It: 4570, Loss: 7.774e-04, Lambda_1: 0.890, Lambda_2: 0.004122, Time: 0.37\n",
            "It: 4580, Loss: 7.729e-04, Lambda_1: 0.890, Lambda_2: 0.004122, Time: 0.45\n",
            "It: 4590, Loss: 7.685e-04, Lambda_1: 0.891, Lambda_2: 0.004123, Time: 0.29\n",
            "It: 4600, Loss: 7.640e-04, Lambda_1: 0.892, Lambda_2: 0.004124, Time: 0.30\n",
            "It: 4610, Loss: 7.596e-04, Lambda_1: 0.892, Lambda_2: 0.004124, Time: 0.31\n",
            "It: 4620, Loss: 7.552e-04, Lambda_1: 0.893, Lambda_2: 0.004125, Time: 0.30\n",
            "It: 4630, Loss: 7.508e-04, Lambda_1: 0.894, Lambda_2: 0.004125, Time: 0.50\n",
            "It: 4640, Loss: 7.464e-04, Lambda_1: 0.894, Lambda_2: 0.004126, Time: 0.48\n",
            "It: 4650, Loss: 7.420e-04, Lambda_1: 0.895, Lambda_2: 0.004127, Time: 0.44\n",
            "It: 4660, Loss: 7.377e-04, Lambda_1: 0.895, Lambda_2: 0.004127, Time: 0.45\n",
            "It: 4670, Loss: 7.333e-04, Lambda_1: 0.896, Lambda_2: 0.004128, Time: 0.27\n",
            "It: 4680, Loss: 7.290e-04, Lambda_1: 0.897, Lambda_2: 0.004128, Time: 0.28\n",
            "It: 4690, Loss: 7.248e-04, Lambda_1: 0.897, Lambda_2: 0.004129, Time: 0.29\n",
            "It: 4700, Loss: 7.205e-04, Lambda_1: 0.898, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 4710, Loss: 7.162e-04, Lambda_1: 0.898, Lambda_2: 0.004130, Time: 0.29\n",
            "It: 4720, Loss: 7.120e-04, Lambda_1: 0.899, Lambda_2: 0.004131, Time: 0.30\n",
            "It: 4730, Loss: 7.078e-04, Lambda_1: 0.900, Lambda_2: 0.004131, Time: 0.27\n",
            "It: 4740, Loss: 7.036e-04, Lambda_1: 0.900, Lambda_2: 0.004132, Time: 0.27\n",
            "It: 4750, Loss: 6.994e-04, Lambda_1: 0.901, Lambda_2: 0.004132, Time: 0.28\n",
            "It: 4760, Loss: 6.953e-04, Lambda_1: 0.901, Lambda_2: 0.004133, Time: 0.27\n",
            "It: 4770, Loss: 6.911e-04, Lambda_1: 0.902, Lambda_2: 0.004133, Time: 0.28\n",
            "It: 4780, Loss: 6.870e-04, Lambda_1: 0.903, Lambda_2: 0.004134, Time: 0.28\n",
            "It: 4790, Loss: 6.829e-04, Lambda_1: 0.903, Lambda_2: 0.004134, Time: 0.27\n",
            "It: 4800, Loss: 6.788e-04, Lambda_1: 0.904, Lambda_2: 0.004135, Time: 0.28\n",
            "It: 4810, Loss: 6.747e-04, Lambda_1: 0.904, Lambda_2: 0.004136, Time: 0.28\n",
            "It: 4820, Loss: 6.707e-04, Lambda_1: 0.905, Lambda_2: 0.004136, Time: 0.29\n",
            "It: 4830, Loss: 6.666e-04, Lambda_1: 0.905, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 4840, Loss: 6.626e-04, Lambda_1: 0.906, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 4850, Loss: 6.586e-04, Lambda_1: 0.907, Lambda_2: 0.004138, Time: 0.28\n",
            "It: 4860, Loss: 6.546e-04, Lambda_1: 0.907, Lambda_2: 0.004138, Time: 0.27\n",
            "It: 4870, Loss: 6.506e-04, Lambda_1: 0.908, Lambda_2: 0.004139, Time: 0.29\n",
            "It: 4880, Loss: 6.467e-04, Lambda_1: 0.908, Lambda_2: 0.004139, Time: 0.28\n",
            "It: 4890, Loss: 6.427e-04, Lambda_1: 0.909, Lambda_2: 0.004140, Time: 0.26\n",
            "It: 4900, Loss: 6.388e-04, Lambda_1: 0.909, Lambda_2: 0.004140, Time: 0.28\n",
            "It: 4910, Loss: 6.349e-04, Lambda_1: 0.910, Lambda_2: 0.004141, Time: 0.28\n",
            "It: 4920, Loss: 6.310e-04, Lambda_1: 0.911, Lambda_2: 0.004141, Time: 0.28\n",
            "It: 4930, Loss: 6.271e-04, Lambda_1: 0.911, Lambda_2: 0.004142, Time: 0.28\n",
            "It: 4940, Loss: 6.233e-04, Lambda_1: 0.912, Lambda_2: 0.004142, Time: 0.27\n",
            "It: 4950, Loss: 6.194e-04, Lambda_1: 0.912, Lambda_2: 0.004142, Time: 0.27\n",
            "It: 4960, Loss: 6.156e-04, Lambda_1: 0.913, Lambda_2: 0.004143, Time: 0.28\n",
            "It: 4970, Loss: 6.118e-04, Lambda_1: 0.913, Lambda_2: 0.004143, Time: 0.28\n",
            "It: 4980, Loss: 6.080e-04, Lambda_1: 0.914, Lambda_2: 0.004144, Time: 0.27\n",
            "It: 4990, Loss: 6.042e-04, Lambda_1: 0.914, Lambda_2: 0.004144, Time: 0.28\n",
            "It: 5000, Loss: 6.005e-04, Lambda_1: 0.915, Lambda_2: 0.004145, Time: 0.28\n",
            "It: 5010, Loss: 5.967e-04, Lambda_1: 0.915, Lambda_2: 0.004145, Time: 0.28\n",
            "It: 5020, Loss: 5.930e-04, Lambda_1: 0.916, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 5030, Loss: 5.893e-04, Lambda_1: 0.917, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 5040, Loss: 5.856e-04, Lambda_1: 0.917, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 5050, Loss: 5.819e-04, Lambda_1: 0.918, Lambda_2: 0.004147, Time: 0.28\n",
            "It: 5060, Loss: 5.783e-04, Lambda_1: 0.918, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 5070, Loss: 5.746e-04, Lambda_1: 0.919, Lambda_2: 0.004148, Time: 0.26\n",
            "It: 5080, Loss: 5.710e-04, Lambda_1: 0.919, Lambda_2: 0.004148, Time: 0.29\n",
            "It: 5090, Loss: 5.674e-04, Lambda_1: 0.920, Lambda_2: 0.004148, Time: 0.26\n",
            "It: 5100, Loss: 5.638e-04, Lambda_1: 0.920, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 5110, Loss: 5.603e-04, Lambda_1: 0.921, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 5120, Loss: 5.567e-04, Lambda_1: 0.921, Lambda_2: 0.004149, Time: 0.30\n",
            "It: 5130, Loss: 5.532e-04, Lambda_1: 0.922, Lambda_2: 0.004150, Time: 0.30\n",
            "It: 5140, Loss: 5.497e-04, Lambda_1: 0.922, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 5150, Loss: 5.461e-04, Lambda_1: 0.923, Lambda_2: 0.004151, Time: 0.29\n",
            "It: 5160, Loss: 5.427e-04, Lambda_1: 0.923, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 5170, Loss: 5.392e-04, Lambda_1: 0.924, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 5180, Loss: 5.357e-04, Lambda_1: 0.924, Lambda_2: 0.004152, Time: 0.28\n",
            "It: 5190, Loss: 5.323e-04, Lambda_1: 0.925, Lambda_2: 0.004152, Time: 0.27\n",
            "It: 5200, Loss: 5.289e-04, Lambda_1: 0.925, Lambda_2: 0.004152, Time: 0.29\n",
            "It: 5210, Loss: 5.255e-04, Lambda_1: 0.926, Lambda_2: 0.004152, Time: 0.27\n",
            "It: 5220, Loss: 5.221e-04, Lambda_1: 0.926, Lambda_2: 0.004153, Time: 0.28\n",
            "It: 5230, Loss: 5.188e-04, Lambda_1: 0.927, Lambda_2: 0.004153, Time: 0.27\n",
            "It: 5240, Loss: 5.154e-04, Lambda_1: 0.927, Lambda_2: 0.004153, Time: 0.27\n",
            "It: 5250, Loss: 5.121e-04, Lambda_1: 0.928, Lambda_2: 0.004154, Time: 0.27\n",
            "It: 5260, Loss: 5.088e-04, Lambda_1: 0.928, Lambda_2: 0.004154, Time: 0.28\n",
            "It: 5270, Loss: 5.055e-04, Lambda_1: 0.929, Lambda_2: 0.004154, Time: 0.28\n",
            "It: 5280, Loss: 5.022e-04, Lambda_1: 0.929, Lambda_2: 0.004154, Time: 0.27\n",
            "It: 5290, Loss: 4.990e-04, Lambda_1: 0.930, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5300, Loss: 4.958e-04, Lambda_1: 0.930, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5310, Loss: 4.925e-04, Lambda_1: 0.931, Lambda_2: 0.004155, Time: 0.28\n",
            "It: 5320, Loss: 4.894e-04, Lambda_1: 0.931, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5330, Loss: 4.862e-04, Lambda_1: 0.932, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5340, Loss: 4.830e-04, Lambda_1: 0.932, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5350, Loss: 4.799e-04, Lambda_1: 0.933, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5360, Loss: 4.768e-04, Lambda_1: 0.933, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5370, Loss: 4.737e-04, Lambda_1: 0.934, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5380, Loss: 4.706e-04, Lambda_1: 0.934, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5390, Loss: 4.675e-04, Lambda_1: 0.935, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5400, Loss: 4.645e-04, Lambda_1: 0.935, Lambda_2: 0.004157, Time: 0.28\n",
            "It: 5410, Loss: 4.614e-04, Lambda_1: 0.936, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5420, Loss: 4.584e-04, Lambda_1: 0.936, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5430, Loss: 4.555e-04, Lambda_1: 0.937, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5440, Loss: 4.525e-04, Lambda_1: 0.937, Lambda_2: 0.004158, Time: 0.28\n",
            "It: 5450, Loss: 4.496e-04, Lambda_1: 0.938, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5460, Loss: 4.466e-04, Lambda_1: 0.938, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5470, Loss: 4.437e-04, Lambda_1: 0.938, Lambda_2: 0.004158, Time: 0.29\n",
            "It: 5480, Loss: 4.408e-04, Lambda_1: 0.939, Lambda_2: 0.004158, Time: 0.28\n",
            "It: 5490, Loss: 4.380e-04, Lambda_1: 0.939, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5500, Loss: 4.351e-04, Lambda_1: 0.940, Lambda_2: 0.004158, Time: 0.26\n",
            "It: 5510, Loss: 4.323e-04, Lambda_1: 0.940, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5520, Loss: 4.295e-04, Lambda_1: 0.941, Lambda_2: 0.004158, Time: 0.28\n",
            "It: 5530, Loss: 4.267e-04, Lambda_1: 0.941, Lambda_2: 0.004158, Time: 0.26\n",
            "It: 5540, Loss: 4.240e-04, Lambda_1: 0.942, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5550, Loss: 4.212e-04, Lambda_1: 0.942, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5560, Loss: 4.185e-04, Lambda_1: 0.943, Lambda_2: 0.004159, Time: 0.27\n",
            "It: 5570, Loss: 4.158e-04, Lambda_1: 0.943, Lambda_2: 0.004159, Time: 0.27\n",
            "It: 5580, Loss: 4.131e-04, Lambda_1: 0.943, Lambda_2: 0.004159, Time: 0.28\n",
            "It: 5590, Loss: 4.104e-04, Lambda_1: 0.944, Lambda_2: 0.004159, Time: 0.27\n",
            "It: 5600, Loss: 4.078e-04, Lambda_1: 0.944, Lambda_2: 0.004159, Time: 0.27\n",
            "It: 5610, Loss: 4.052e-04, Lambda_1: 0.945, Lambda_2: 0.004159, Time: 0.26\n",
            "It: 5620, Loss: 4.029e-04, Lambda_1: 0.945, Lambda_2: 0.004159, Time: 0.28\n",
            "It: 5630, Loss: 4.184e-04, Lambda_1: 0.946, Lambda_2: 0.004158, Time: 0.28\n",
            "It: 5640, Loss: 5.642e-04, Lambda_1: 0.946, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5650, Loss: 5.647e-04, Lambda_1: 0.946, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5660, Loss: 4.206e-04, Lambda_1: 0.947, Lambda_2: 0.004157, Time: 0.28\n",
            "It: 5670, Loss: 5.357e-04, Lambda_1: 0.947, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5680, Loss: 4.376e-04, Lambda_1: 0.947, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5690, Loss: 4.022e-04, Lambda_1: 0.948, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5700, Loss: 3.936e-04, Lambda_1: 0.948, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5710, Loss: 3.933e-04, Lambda_1: 0.948, Lambda_2: 0.004158, Time: 0.30\n",
            "It: 5720, Loss: 3.801e-04, Lambda_1: 0.949, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5730, Loss: 3.832e-04, Lambda_1: 0.949, Lambda_2: 0.004158, Time: 0.29\n",
            "It: 5740, Loss: 3.933e-04, Lambda_1: 0.949, Lambda_2: 0.004158, Time: 0.26\n",
            "It: 5750, Loss: 4.459e-04, Lambda_1: 0.950, Lambda_2: 0.004158, Time: 0.28\n",
            "It: 5760, Loss: 3.830e-04, Lambda_1: 0.950, Lambda_2: 0.004158, Time: 0.29\n",
            "It: 5770, Loss: 3.781e-04, Lambda_1: 0.950, Lambda_2: 0.004157, Time: 0.28\n",
            "It: 5780, Loss: 5.631e-04, Lambda_1: 0.951, Lambda_2: 0.004158, Time: 0.27\n",
            "It: 5790, Loss: 4.483e-04, Lambda_1: 0.951, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5800, Loss: 4.044e-04, Lambda_1: 0.951, Lambda_2: 0.004157, Time: 0.29\n",
            "It: 5810, Loss: 4.743e-04, Lambda_1: 0.952, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5820, Loss: 7.233e-04, Lambda_1: 0.952, Lambda_2: 0.004157, Time: 0.27\n",
            "It: 5830, Loss: 4.136e-04, Lambda_1: 0.952, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5840, Loss: 3.580e-04, Lambda_1: 0.953, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5850, Loss: 3.619e-04, Lambda_1: 0.953, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5860, Loss: 3.512e-04, Lambda_1: 0.953, Lambda_2: 0.004156, Time: 0.29\n",
            "It: 5870, Loss: 3.596e-04, Lambda_1: 0.953, Lambda_2: 0.004156, Time: 0.28\n",
            "It: 5880, Loss: 3.547e-04, Lambda_1: 0.954, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5890, Loss: 3.692e-04, Lambda_1: 0.954, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5900, Loss: 3.854e-04, Lambda_1: 0.954, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5910, Loss: 3.876e-04, Lambda_1: 0.955, Lambda_2: 0.004156, Time: 0.27\n",
            "It: 5920, Loss: 3.947e-04, Lambda_1: 0.955, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5930, Loss: 4.026e-04, Lambda_1: 0.955, Lambda_2: 0.004155, Time: 0.28\n",
            "It: 5940, Loss: 3.857e-04, Lambda_1: 0.956, Lambda_2: 0.004155, Time: 0.28\n",
            "It: 5950, Loss: 4.381e-04, Lambda_1: 0.956, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5960, Loss: 3.519e-04, Lambda_1: 0.956, Lambda_2: 0.004155, Time: 0.27\n",
            "It: 5970, Loss: 4.567e-04, Lambda_1: 0.956, Lambda_2: 0.004155, Time: 0.26\n",
            "It: 5980, Loss: 3.314e-04, Lambda_1: 0.957, Lambda_2: 0.004154, Time: 0.29\n",
            "It: 5990, Loss: 3.850e-04, Lambda_1: 0.957, Lambda_2: 0.004154, Time: 0.26\n",
            "It: 6000, Loss: 5.149e-04, Lambda_1: 0.957, Lambda_2: 0.004154, Time: 0.27\n",
            "It: 6010, Loss: 4.570e-04, Lambda_1: 0.957, Lambda_2: 0.004153, Time: 0.29\n",
            "It: 6020, Loss: 6.611e-04, Lambda_1: 0.958, Lambda_2: 0.004153, Time: 0.26\n",
            "It: 6030, Loss: 4.399e-04, Lambda_1: 0.958, Lambda_2: 0.004153, Time: 0.27\n",
            "It: 6040, Loss: 3.597e-04, Lambda_1: 0.958, Lambda_2: 0.004153, Time: 0.27\n",
            "It: 6050, Loss: 4.179e-04, Lambda_1: 0.958, Lambda_2: 0.004153, Time: 0.28\n",
            "It: 6060, Loss: 4.555e-04, Lambda_1: 0.958, Lambda_2: 0.004153, Time: 0.26\n",
            "It: 6070, Loss: 4.482e-04, Lambda_1: 0.958, Lambda_2: 0.004152, Time: 0.28\n",
            "It: 6080, Loss: 4.395e-04, Lambda_1: 0.959, Lambda_2: 0.004153, Time: 0.27\n",
            "It: 6090, Loss: 4.558e-04, Lambda_1: 0.959, Lambda_2: 0.004152, Time: 0.28\n",
            "It: 6100, Loss: 4.590e-04, Lambda_1: 0.959, Lambda_2: 0.004152, Time: 0.27\n",
            "It: 6110, Loss: 4.529e-04, Lambda_1: 0.959, Lambda_2: 0.004152, Time: 0.27\n",
            "It: 6120, Loss: 3.962e-04, Lambda_1: 0.960, Lambda_2: 0.004152, Time: 0.29\n",
            "It: 6130, Loss: 3.142e-04, Lambda_1: 0.960, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 6140, Loss: 4.615e-04, Lambda_1: 0.960, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 6150, Loss: 5.463e-04, Lambda_1: 0.960, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 6160, Loss: 4.116e-04, Lambda_1: 0.960, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 6170, Loss: 3.713e-04, Lambda_1: 0.960, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 6180, Loss: 3.898e-04, Lambda_1: 0.961, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 6190, Loss: 3.725e-04, Lambda_1: 0.961, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 6200, Loss: 3.018e-04, Lambda_1: 0.961, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 6210, Loss: 3.511e-04, Lambda_1: 0.961, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 6220, Loss: 3.170e-04, Lambda_1: 0.962, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 6230, Loss: 3.323e-04, Lambda_1: 0.962, Lambda_2: 0.004150, Time: 0.29\n",
            "It: 6240, Loss: 4.097e-04, Lambda_1: 0.962, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 6250, Loss: 3.078e-04, Lambda_1: 0.962, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 6260, Loss: 5.303e-04, Lambda_1: 0.962, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 6270, Loss: 6.100e-04, Lambda_1: 0.963, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 6280, Loss: 7.455e-04, Lambda_1: 0.963, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 6290, Loss: 7.016e-04, Lambda_1: 0.963, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 6300, Loss: 7.974e-04, Lambda_1: 0.963, Lambda_2: 0.004147, Time: 0.29\n",
            "It: 6310, Loss: 3.024e-04, Lambda_1: 0.963, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 6320, Loss: 4.459e-04, Lambda_1: 0.963, Lambda_2: 0.004148, Time: 0.28\n",
            "It: 6330, Loss: 4.254e-04, Lambda_1: 0.963, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 6340, Loss: 4.327e-04, Lambda_1: 0.963, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 6350, Loss: 3.907e-04, Lambda_1: 0.964, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 6360, Loss: 3.178e-04, Lambda_1: 0.964, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 6370, Loss: 3.009e-04, Lambda_1: 0.964, Lambda_2: 0.004147, Time: 0.28\n",
            "It: 6380, Loss: 5.574e-04, Lambda_1: 0.964, Lambda_2: 0.004146, Time: 0.27\n",
            "It: 6390, Loss: 5.030e-04, Lambda_1: 0.964, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 6400, Loss: 3.046e-04, Lambda_1: 0.964, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 6410, Loss: 2.798e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.29\n",
            "It: 6420, Loss: 2.819e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.27\n",
            "It: 6430, Loss: 2.923e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 6440, Loss: 3.314e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 6450, Loss: 2.987e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 6460, Loss: 3.352e-04, Lambda_1: 0.965, Lambda_2: 0.004146, Time: 0.27\n",
            "It: 6470, Loss: 2.771e-04, Lambda_1: 0.966, Lambda_2: 0.004145, Time: 0.27\n",
            "It: 6480, Loss: 3.540e-04, Lambda_1: 0.966, Lambda_2: 0.004145, Time: 0.28\n",
            "It: 6490, Loss: 2.863e-04, Lambda_1: 0.966, Lambda_2: 0.004145, Time: 0.27\n",
            "It: 6500, Loss: 4.742e-04, Lambda_1: 0.966, Lambda_2: 0.004144, Time: 0.27\n",
            "It: 6510, Loss: 2.641e-04, Lambda_1: 0.966, Lambda_2: 0.004144, Time: 0.28\n",
            "It: 6520, Loss: 5.058e-04, Lambda_1: 0.967, Lambda_2: 0.004144, Time: 0.26\n",
            "It: 6530, Loss: 2.713e-04, Lambda_1: 0.967, Lambda_2: 0.004144, Time: 0.28\n",
            "It: 6540, Loss: 5.386e-04, Lambda_1: 0.967, Lambda_2: 0.004143, Time: 0.27\n",
            "It: 6550, Loss: 3.302e-04, Lambda_1: 0.967, Lambda_2: 0.004143, Time: 0.28\n",
            "It: 6560, Loss: 3.230e-04, Lambda_1: 0.967, Lambda_2: 0.004143, Time: 0.26\n",
            "It: 6570, Loss: 2.943e-04, Lambda_1: 0.967, Lambda_2: 0.004143, Time: 0.27\n",
            "It: 6580, Loss: 3.679e-04, Lambda_1: 0.967, Lambda_2: 0.004143, Time: 0.28\n",
            "It: 6590, Loss: 4.583e-04, Lambda_1: 0.968, Lambda_2: 0.004142, Time: 0.29\n",
            "It: 6600, Loss: 6.440e-04, Lambda_1: 0.968, Lambda_2: 0.004142, Time: 0.27\n",
            "It: 6610, Loss: 5.528e-04, Lambda_1: 0.968, Lambda_2: 0.004142, Time: 0.27\n",
            "It: 6620, Loss: 6.916e-04, Lambda_1: 0.968, Lambda_2: 0.004142, Time: 0.28\n",
            "It: 6630, Loss: 3.860e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.26\n",
            "It: 6640, Loss: 3.711e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.29\n",
            "It: 6650, Loss: 4.653e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.26\n",
            "It: 6660, Loss: 3.487e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.28\n",
            "It: 6670, Loss: 4.439e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.27\n",
            "It: 6680, Loss: 5.299e-04, Lambda_1: 0.968, Lambda_2: 0.004141, Time: 0.28\n",
            "It: 6690, Loss: 2.719e-04, Lambda_1: 0.968, Lambda_2: 0.004140, Time: 0.28\n",
            "It: 6700, Loss: 2.533e-04, Lambda_1: 0.968, Lambda_2: 0.004140, Time: 0.29\n",
            "It: 6710, Loss: 2.712e-04, Lambda_1: 0.968, Lambda_2: 0.004140, Time: 0.27\n",
            "It: 6720, Loss: 3.235e-04, Lambda_1: 0.969, Lambda_2: 0.004140, Time: 0.27\n",
            "It: 6730, Loss: 4.903e-04, Lambda_1: 0.969, Lambda_2: 0.004140, Time: 0.28\n",
            "It: 6740, Loss: 6.409e-04, Lambda_1: 0.969, Lambda_2: 0.004139, Time: 0.28\n",
            "It: 6750, Loss: 2.899e-04, Lambda_1: 0.969, Lambda_2: 0.004139, Time: 0.27\n",
            "It: 6760, Loss: 5.888e-04, Lambda_1: 0.969, Lambda_2: 0.004139, Time: 0.27\n",
            "It: 6770, Loss: 3.818e-04, Lambda_1: 0.969, Lambda_2: 0.004138, Time: 0.28\n",
            "It: 6780, Loss: 2.950e-04, Lambda_1: 0.969, Lambda_2: 0.004138, Time: 0.27\n",
            "It: 6790, Loss: 2.452e-04, Lambda_1: 0.969, Lambda_2: 0.004138, Time: 0.28\n",
            "It: 6800, Loss: 2.894e-04, Lambda_1: 0.969, Lambda_2: 0.004138, Time: 0.29\n",
            "It: 6810, Loss: 5.639e-04, Lambda_1: 0.969, Lambda_2: 0.004138, Time: 0.27\n",
            "It: 6820, Loss: 5.467e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 6830, Loss: 3.323e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 6840, Loss: 4.099e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 6850, Loss: 4.817e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.27\n",
            "It: 6860, Loss: 3.444e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.28\n",
            "It: 6870, Loss: 2.693e-04, Lambda_1: 0.970, Lambda_2: 0.004137, Time: 0.29\n",
            "It: 6880, Loss: 4.589e-04, Lambda_1: 0.970, Lambda_2: 0.004136, Time: 0.28\n",
            "It: 6890, Loss: 5.124e-04, Lambda_1: 0.970, Lambda_2: 0.004136, Time: 0.28\n",
            "It: 6900, Loss: 2.839e-04, Lambda_1: 0.970, Lambda_2: 0.004136, Time: 0.28\n",
            "It: 6910, Loss: 3.820e-04, Lambda_1: 0.971, Lambda_2: 0.004136, Time: 0.29\n",
            "It: 6920, Loss: 5.586e-04, Lambda_1: 0.971, Lambda_2: 0.004136, Time: 0.27\n",
            "It: 6930, Loss: 4.312e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.27\n",
            "It: 6940, Loss: 2.566e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.28\n",
            "It: 6950, Loss: 3.990e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.27\n",
            "It: 6960, Loss: 4.810e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.28\n",
            "It: 6970, Loss: 3.200e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.29\n",
            "It: 6980, Loss: 2.940e-04, Lambda_1: 0.971, Lambda_2: 0.004135, Time: 0.29\n",
            "It: 6990, Loss: 4.850e-04, Lambda_1: 0.971, Lambda_2: 0.004134, Time: 0.28\n",
            "It: 7000, Loss: 4.551e-04, Lambda_1: 0.971, Lambda_2: 0.004134, Time: 0.26\n",
            "It: 7010, Loss: 2.659e-04, Lambda_1: 0.971, Lambda_2: 0.004134, Time: 0.27\n",
            "It: 7020, Loss: 3.986e-04, Lambda_1: 0.971, Lambda_2: 0.004134, Time: 0.28\n",
            "It: 7030, Loss: 5.317e-04, Lambda_1: 0.971, Lambda_2: 0.004134, Time: 0.27\n",
            "It: 7040, Loss: 3.441e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.26\n",
            "It: 7050, Loss: 2.931e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.28\n",
            "It: 7060, Loss: 4.550e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.28\n",
            "It: 7070, Loss: 4.631e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.27\n",
            "It: 7080, Loss: 2.534e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.28\n",
            "It: 7090, Loss: 3.394e-04, Lambda_1: 0.972, Lambda_2: 0.004133, Time: 0.28\n",
            "It: 7100, Loss: 5.148e-04, Lambda_1: 0.972, Lambda_2: 0.004132, Time: 0.27\n",
            "It: 7110, Loss: 3.859e-04, Lambda_1: 0.972, Lambda_2: 0.004132, Time: 0.27\n",
            "It: 7120, Loss: 3.013e-04, Lambda_1: 0.972, Lambda_2: 0.004132, Time: 0.28\n",
            "It: 7130, Loss: 4.555e-04, Lambda_1: 0.972, Lambda_2: 0.004132, Time: 0.27\n",
            "It: 7140, Loss: 4.614e-04, Lambda_1: 0.972, Lambda_2: 0.004132, Time: 0.29\n",
            "It: 7150, Loss: 2.414e-04, Lambda_1: 0.972, Lambda_2: 0.004131, Time: 0.28\n",
            "It: 7160, Loss: 3.533e-04, Lambda_1: 0.972, Lambda_2: 0.004131, Time: 0.28\n",
            "It: 7170, Loss: 4.628e-04, Lambda_1: 0.972, Lambda_2: 0.004131, Time: 0.26\n",
            "It: 7180, Loss: 3.736e-04, Lambda_1: 0.973, Lambda_2: 0.004131, Time: 0.26\n",
            "It: 7190, Loss: 2.549e-04, Lambda_1: 0.973, Lambda_2: 0.004130, Time: 0.28\n",
            "It: 7200, Loss: 4.609e-04, Lambda_1: 0.973, Lambda_2: 0.004131, Time: 0.27\n",
            "It: 7210, Loss: 4.884e-04, Lambda_1: 0.973, Lambda_2: 0.004130, Time: 0.28\n",
            "It: 7220, Loss: 2.752e-04, Lambda_1: 0.973, Lambda_2: 0.004130, Time: 0.27\n",
            "It: 7230, Loss: 3.530e-04, Lambda_1: 0.973, Lambda_2: 0.004130, Time: 0.29\n",
            "It: 7240, Loss: 5.423e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 7250, Loss: 3.154e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 7260, Loss: 2.841e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.29\n",
            "It: 7270, Loss: 4.139e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 7280, Loss: 3.975e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.28\n",
            "It: 7290, Loss: 2.242e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 7300, Loss: 3.498e-04, Lambda_1: 0.973, Lambda_2: 0.004128, Time: 0.28\n",
            "It: 7310, Loss: 5.136e-04, Lambda_1: 0.973, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 7320, Loss: 3.863e-04, Lambda_1: 0.974, Lambda_2: 0.004128, Time: 0.28\n",
            "It: 7330, Loss: 3.620e-04, Lambda_1: 0.974, Lambda_2: 0.004128, Time: 0.27\n",
            "It: 7340, Loss: 5.959e-04, Lambda_1: 0.974, Lambda_2: 0.004128, Time: 0.28\n",
            "It: 7350, Loss: 4.520e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.26\n",
            "It: 7360, Loss: 2.421e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.26\n",
            "It: 7370, Loss: 4.207e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.27\n",
            "It: 7380, Loss: 4.004e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.27\n",
            "It: 7390, Loss: 2.501e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.28\n",
            "It: 7400, Loss: 2.600e-04, Lambda_1: 0.974, Lambda_2: 0.004127, Time: 0.26\n",
            "It: 7410, Loss: 4.508e-04, Lambda_1: 0.974, Lambda_2: 0.004126, Time: 0.28\n",
            "It: 7420, Loss: 4.106e-04, Lambda_1: 0.974, Lambda_2: 0.004126, Time: 0.27\n",
            "It: 7430, Loss: 2.992e-04, Lambda_1: 0.974, Lambda_2: 0.004126, Time: 0.29\n",
            "It: 7440, Loss: 3.939e-04, Lambda_1: 0.974, Lambda_2: 0.004126, Time: 0.28\n",
            "It: 7450, Loss: 5.628e-04, Lambda_1: 0.974, Lambda_2: 0.004126, Time: 0.28\n",
            "It: 7460, Loss: 2.343e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.28\n",
            "It: 7470, Loss: 4.592e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.27\n",
            "It: 7480, Loss: 2.862e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.27\n",
            "It: 7490, Loss: 2.741e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.27\n",
            "It: 7500, Loss: 2.110e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.28\n",
            "It: 7510, Loss: 3.636e-04, Lambda_1: 0.975, Lambda_2: 0.004125, Time: 0.27\n",
            "It: 7520, Loss: 4.129e-04, Lambda_1: 0.975, Lambda_2: 0.004124, Time: 0.28\n",
            "It: 7530, Loss: 2.962e-04, Lambda_1: 0.975, Lambda_2: 0.004124, Time: 0.27\n",
            "It: 7540, Loss: 2.678e-04, Lambda_1: 0.975, Lambda_2: 0.004124, Time: 0.26\n",
            "It: 7550, Loss: 5.976e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.29\n",
            "It: 7560, Loss: 4.399e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.28\n",
            "It: 7570, Loss: 5.998e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.27\n",
            "It: 7580, Loss: 3.478e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.27\n",
            "It: 7590, Loss: 5.311e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.28\n",
            "It: 7600, Loss: 2.582e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.27\n",
            "It: 7610, Loss: 2.345e-04, Lambda_1: 0.975, Lambda_2: 0.004122, Time: 0.27\n",
            "It: 7620, Loss: 2.830e-04, Lambda_1: 0.975, Lambda_2: 0.004123, Time: 0.28\n",
            "It: 7630, Loss: 3.137e-04, Lambda_1: 0.975, Lambda_2: 0.004122, Time: 0.26\n",
            "It: 7640, Loss: 3.529e-04, Lambda_1: 0.975, Lambda_2: 0.004122, Time: 0.26\n",
            "It: 7650, Loss: 2.580e-04, Lambda_1: 0.976, Lambda_2: 0.004122, Time: 0.26\n",
            "It: 7660, Loss: 2.156e-04, Lambda_1: 0.976, Lambda_2: 0.004122, Time: 0.28\n",
            "It: 7670, Loss: 3.571e-04, Lambda_1: 0.976, Lambda_2: 0.004122, Time: 0.27\n",
            "It: 7680, Loss: 7.404e-04, Lambda_1: 0.976, Lambda_2: 0.004121, Time: 0.27\n",
            "It: 7690, Loss: 2.891e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.26\n",
            "It: 7700, Loss: 3.242e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.27\n",
            "It: 7710, Loss: 6.088e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.27\n",
            "It: 7720, Loss: 2.384e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.26\n",
            "It: 7730, Loss: 2.464e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.27\n",
            "It: 7740, Loss: 2.666e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.27\n",
            "It: 7750, Loss: 3.961e-04, Lambda_1: 0.976, Lambda_2: 0.004120, Time: 0.28\n",
            "It: 7760, Loss: 3.297e-04, Lambda_1: 0.976, Lambda_2: 0.004119, Time: 0.28\n",
            "It: 7770, Loss: 3.117e-04, Lambda_1: 0.976, Lambda_2: 0.004119, Time: 0.27\n",
            "It: 7780, Loss: 4.324e-04, Lambda_1: 0.976, Lambda_2: 0.004119, Time: 0.28\n",
            "It: 7790, Loss: 5.129e-04, Lambda_1: 0.976, Lambda_2: 0.004119, Time: 0.27\n",
            "It: 7800, Loss: 2.745e-04, Lambda_1: 0.976, Lambda_2: 0.004118, Time: 0.27\n",
            "It: 7810, Loss: 4.778e-04, Lambda_1: 0.976, Lambda_2: 0.004118, Time: 0.29\n",
            "It: 7820, Loss: 7.209e-04, Lambda_1: 0.977, Lambda_2: 0.004118, Time: 0.26\n",
            "It: 7830, Loss: 2.306e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.28\n",
            "It: 7840, Loss: 3.151e-04, Lambda_1: 0.976, Lambda_2: 0.004118, Time: 0.28\n",
            "It: 7850, Loss: 2.731e-04, Lambda_1: 0.976, Lambda_2: 0.004117, Time: 0.28\n",
            "It: 7860, Loss: 3.003e-04, Lambda_1: 0.976, Lambda_2: 0.004118, Time: 0.26\n",
            "It: 7870, Loss: 2.565e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.27\n",
            "It: 7880, Loss: 1.905e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.28\n",
            "It: 7890, Loss: 2.288e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.27\n",
            "It: 7900, Loss: 4.205e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.26\n",
            "It: 7910, Loss: 4.700e-04, Lambda_1: 0.977, Lambda_2: 0.004117, Time: 0.29\n",
            "It: 7920, Loss: 3.464e-04, Lambda_1: 0.977, Lambda_2: 0.004116, Time: 0.27\n",
            "It: 7930, Loss: 3.265e-04, Lambda_1: 0.977, Lambda_2: 0.004116, Time: 0.28\n",
            "It: 7940, Loss: 7.821e-04, Lambda_1: 0.977, Lambda_2: 0.004116, Time: 0.26\n",
            "It: 7950, Loss: 3.133e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.28\n",
            "It: 7960, Loss: 3.659e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.29\n",
            "It: 7970, Loss: 2.936e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.28\n",
            "It: 7980, Loss: 2.167e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.28\n",
            "It: 7990, Loss: 1.908e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.29\n",
            "It: 8000, Loss: 1.999e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.26\n",
            "It: 8010, Loss: 2.651e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.27\n",
            "It: 8020, Loss: 4.023e-04, Lambda_1: 0.977, Lambda_2: 0.004115, Time: 0.30\n",
            "It: 8030, Loss: 4.145e-04, Lambda_1: 0.978, Lambda_2: 0.004114, Time: 0.28\n",
            "It: 8040, Loss: 2.753e-04, Lambda_1: 0.978, Lambda_2: 0.004114, Time: 0.27\n",
            "It: 8050, Loss: 6.653e-04, Lambda_1: 0.978, Lambda_2: 0.004114, Time: 0.27\n",
            "It: 8060, Loss: 2.828e-04, Lambda_1: 0.978, Lambda_2: 0.004113, Time: 0.29\n",
            "It: 8070, Loss: 3.982e-04, Lambda_1: 0.978, Lambda_2: 0.004113, Time: 0.28\n",
            "It: 8080, Loss: 4.984e-04, Lambda_1: 0.978, Lambda_2: 0.004113, Time: 0.27\n",
            "It: 8090, Loss: 3.245e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.27\n",
            "It: 8100, Loss: 2.312e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.26\n",
            "It: 8110, Loss: 2.069e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.27\n",
            "It: 8120, Loss: 2.272e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.27\n",
            "It: 8130, Loss: 4.683e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.28\n",
            "It: 8140, Loss: 4.057e-04, Lambda_1: 0.978, Lambda_2: 0.004112, Time: 0.27\n",
            "It: 8150, Loss: 2.287e-04, Lambda_1: 0.978, Lambda_2: 0.004111, Time: 0.27\n",
            "It: 8160, Loss: 6.217e-04, Lambda_1: 0.978, Lambda_2: 0.004111, Time: 0.26\n",
            "It: 8170, Loss: 3.094e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.28\n",
            "It: 8180, Loss: 4.517e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.28\n",
            "It: 8190, Loss: 4.013e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.27\n",
            "It: 8200, Loss: 4.112e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.28\n",
            "It: 8210, Loss: 2.435e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.28\n",
            "It: 8220, Loss: 2.266e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.28\n",
            "It: 8230, Loss: 2.879e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.27\n",
            "It: 8240, Loss: 3.337e-04, Lambda_1: 0.978, Lambda_2: 0.004110, Time: 0.29\n",
            "It: 8250, Loss: 2.564e-04, Lambda_1: 0.978, Lambda_2: 0.004109, Time: 0.27\n",
            "It: 8260, Loss: 2.879e-04, Lambda_1: 0.979, Lambda_2: 0.004109, Time: 0.27\n",
            "It: 8270, Loss: 5.117e-04, Lambda_1: 0.979, Lambda_2: 0.004109, Time: 0.28\n",
            "It: 8280, Loss: 5.162e-04, Lambda_1: 0.979, Lambda_2: 0.004108, Time: 0.28\n",
            "It: 8290, Loss: 2.219e-04, Lambda_1: 0.979, Lambda_2: 0.004108, Time: 0.27\n",
            "It: 8300, Loss: 6.434e-04, Lambda_1: 0.979, Lambda_2: 0.004108, Time: 0.28\n",
            "It: 8310, Loss: 4.627e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.29\n",
            "It: 8320, Loss: 1.913e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.27\n",
            "It: 8330, Loss: 2.621e-04, Lambda_1: 0.979, Lambda_2: 0.004108, Time: 0.27\n",
            "It: 8340, Loss: 3.474e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.28\n",
            "It: 8350, Loss: 2.690e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.27\n",
            "It: 8360, Loss: 1.953e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.27\n",
            "It: 8370, Loss: 3.185e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.27\n",
            "It: 8380, Loss: 5.495e-04, Lambda_1: 0.979, Lambda_2: 0.004107, Time: 0.27\n",
            "It: 8390, Loss: 2.502e-04, Lambda_1: 0.979, Lambda_2: 0.004106, Time: 0.27\n",
            "It: 8400, Loss: 5.848e-04, Lambda_1: 0.979, Lambda_2: 0.004106, Time: 0.27\n",
            "It: 8410, Loss: 4.222e-04, Lambda_1: 0.979, Lambda_2: 0.004106, Time: 0.29\n",
            "It: 8420, Loss: 3.921e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.29\n",
            "It: 8430, Loss: 3.305e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.27\n",
            "It: 8440, Loss: 2.957e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.27\n",
            "It: 8450, Loss: 3.658e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.29\n",
            "It: 8460, Loss: 1.984e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.28\n",
            "It: 8470, Loss: 3.195e-04, Lambda_1: 0.979, Lambda_2: 0.004105, Time: 0.28\n",
            "It: 8480, Loss: 3.966e-04, Lambda_1: 0.979, Lambda_2: 0.004104, Time: 0.27\n",
            "It: 8490, Loss: 3.557e-04, Lambda_1: 0.980, Lambda_2: 0.004104, Time: 0.29\n",
            "It: 8500, Loss: 3.185e-04, Lambda_1: 0.980, Lambda_2: 0.004104, Time: 0.27\n",
            "It: 8510, Loss: 4.644e-04, Lambda_1: 0.980, Lambda_2: 0.004104, Time: 0.27\n",
            "It: 8520, Loss: 5.625e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.30\n",
            "It: 8530, Loss: 2.968e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.29\n",
            "It: 8540, Loss: 3.217e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.29\n",
            "It: 8550, Loss: 3.269e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.28\n",
            "It: 8560, Loss: 2.030e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.27\n",
            "It: 8570, Loss: 2.818e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.27\n",
            "It: 8580, Loss: 3.496e-04, Lambda_1: 0.980, Lambda_2: 0.004103, Time: 0.28\n",
            "It: 8590, Loss: 3.156e-04, Lambda_1: 0.980, Lambda_2: 0.004102, Time: 0.29\n",
            "It: 8600, Loss: 2.130e-04, Lambda_1: 0.980, Lambda_2: 0.004102, Time: 0.26\n",
            "It: 8610, Loss: 5.912e-04, Lambda_1: 0.980, Lambda_2: 0.004102, Time: 0.27\n",
            "It: 8620, Loss: 2.999e-04, Lambda_1: 0.980, Lambda_2: 0.004101, Time: 0.27\n",
            "It: 8630, Loss: 3.388e-04, Lambda_1: 0.980, Lambda_2: 0.004101, Time: 0.28\n",
            "It: 8640, Loss: 5.323e-04, Lambda_1: 0.980, Lambda_2: 0.004101, Time: 0.26\n",
            "It: 8650, Loss: 5.267e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.28\n",
            "It: 8660, Loss: 2.211e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.28\n",
            "It: 8670, Loss: 2.460e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.26\n",
            "It: 8680, Loss: 2.319e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.28\n",
            "It: 8690, Loss: 2.262e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.27\n",
            "It: 8700, Loss: 1.946e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.29\n",
            "It: 8710, Loss: 1.673e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.27\n",
            "It: 8720, Loss: 1.644e-04, Lambda_1: 0.980, Lambda_2: 0.004100, Time: 0.27\n",
            "It: 8730, Loss: 1.625e-04, Lambda_1: 0.981, Lambda_2: 0.004099, Time: 0.27\n",
            "It: 8740, Loss: 1.687e-04, Lambda_1: 0.981, Lambda_2: 0.004099, Time: 0.29\n",
            "It: 8750, Loss: 2.152e-04, Lambda_1: 0.981, Lambda_2: 0.004099, Time: 0.26\n",
            "It: 8760, Loss: 1.655e-04, Lambda_1: 0.981, Lambda_2: 0.004099, Time: 0.27\n",
            "It: 8770, Loss: 1.691e-04, Lambda_1: 0.981, Lambda_2: 0.004098, Time: 0.29\n",
            "It: 8780, Loss: 3.379e-04, Lambda_1: 0.981, Lambda_2: 0.004098, Time: 0.27\n",
            "It: 8790, Loss: 1.632e-04, Lambda_1: 0.981, Lambda_2: 0.004098, Time: 0.27\n",
            "It: 8800, Loss: 4.697e-04, Lambda_1: 0.981, Lambda_2: 0.004097, Time: 0.27\n",
            "It: 8810, Loss: 2.542e-04, Lambda_1: 0.981, Lambda_2: 0.004097, Time: 0.29\n",
            "It: 8820, Loss: 1.811e-04, Lambda_1: 0.981, Lambda_2: 0.004097, Time: 0.27\n",
            "It: 8830, Loss: 1.585e-04, Lambda_1: 0.981, Lambda_2: 0.004097, Time: 0.28\n",
            "It: 8840, Loss: 1.718e-04, Lambda_1: 0.982, Lambda_2: 0.004097, Time: 0.29\n",
            "It: 8850, Loss: 4.548e-04, Lambda_1: 0.982, Lambda_2: 0.004096, Time: 0.26\n",
            "It: 8860, Loss: 5.519e-04, Lambda_1: 0.982, Lambda_2: 0.004096, Time: 0.27\n",
            "It: 8870, Loss: 7.676e-04, Lambda_1: 0.982, Lambda_2: 0.004095, Time: 0.27\n",
            "It: 8880, Loss: 1.810e-03, Lambda_1: 0.982, Lambda_2: 0.004094, Time: 0.29\n",
            "It: 8890, Loss: 6.137e-01, Lambda_1: 0.976, Lambda_2: 0.004082, Time: 0.27\n",
            "It: 8900, Loss: 2.320e-01, Lambda_1: 0.966, Lambda_2: 0.004047, Time: 0.27\n",
            "It: 8910, Loss: 1.570e-01, Lambda_1: 0.965, Lambda_2: 0.004032, Time: 0.28\n",
            "It: 8920, Loss: 1.123e-01, Lambda_1: 0.967, Lambda_2: 0.004008, Time: 0.26\n",
            "It: 8930, Loss: 8.202e-02, Lambda_1: 0.967, Lambda_2: 0.004000, Time: 0.28\n",
            "It: 8940, Loss: 4.980e-02, Lambda_1: 0.963, Lambda_2: 0.004009, Time: 0.29\n",
            "It: 8950, Loss: 3.338e-02, Lambda_1: 0.959, Lambda_2: 0.004015, Time: 0.28\n",
            "It: 8960, Loss: 2.706e-02, Lambda_1: 0.954, Lambda_2: 0.004022, Time: 0.27\n",
            "It: 8970, Loss: 2.161e-02, Lambda_1: 0.949, Lambda_2: 0.004033, Time: 0.26\n",
            "It: 8980, Loss: 1.713e-02, Lambda_1: 0.944, Lambda_2: 0.004045, Time: 0.27\n",
            "It: 8990, Loss: 1.402e-02, Lambda_1: 0.939, Lambda_2: 0.004055, Time: 0.29\n",
            "It: 9000, Loss: 1.158e-02, Lambda_1: 0.935, Lambda_2: 0.004065, Time: 0.27\n",
            "It: 9010, Loss: 9.469e-03, Lambda_1: 0.931, Lambda_2: 0.004073, Time: 0.28\n",
            "It: 9020, Loss: 7.679e-03, Lambda_1: 0.927, Lambda_2: 0.004080, Time: 0.28\n",
            "It: 9030, Loss: 6.221e-03, Lambda_1: 0.924, Lambda_2: 0.004088, Time: 0.28\n",
            "It: 9040, Loss: 5.064e-03, Lambda_1: 0.921, Lambda_2: 0.004095, Time: 0.28\n",
            "It: 9050, Loss: 4.158e-03, Lambda_1: 0.918, Lambda_2: 0.004101, Time: 0.28\n",
            "It: 9060, Loss: 3.458e-03, Lambda_1: 0.916, Lambda_2: 0.004107, Time: 0.28\n",
            "It: 9070, Loss: 2.918e-03, Lambda_1: 0.914, Lambda_2: 0.004112, Time: 0.28\n",
            "It: 9080, Loss: 2.499e-03, Lambda_1: 0.912, Lambda_2: 0.004116, Time: 0.28\n",
            "It: 9090, Loss: 2.169e-03, Lambda_1: 0.911, Lambda_2: 0.004120, Time: 0.27\n",
            "It: 9100, Loss: 1.906e-03, Lambda_1: 0.909, Lambda_2: 0.004123, Time: 0.28\n",
            "It: 9110, Loss: 1.694e-03, Lambda_1: 0.908, Lambda_2: 0.004126, Time: 0.27\n",
            "It: 9120, Loss: 1.521e-03, Lambda_1: 0.907, Lambda_2: 0.004129, Time: 0.27\n",
            "It: 9130, Loss: 1.379e-03, Lambda_1: 0.906, Lambda_2: 0.004131, Time: 0.29\n",
            "It: 9140, Loss: 1.261e-03, Lambda_1: 0.906, Lambda_2: 0.004133, Time: 0.27\n",
            "It: 9150, Loss: 1.162e-03, Lambda_1: 0.905, Lambda_2: 0.004135, Time: 0.26\n",
            "It: 9160, Loss: 1.080e-03, Lambda_1: 0.905, Lambda_2: 0.004136, Time: 0.27\n",
            "It: 9170, Loss: 1.009e-03, Lambda_1: 0.904, Lambda_2: 0.004138, Time: 0.28\n",
            "It: 9180, Loss: 9.497e-04, Lambda_1: 0.904, Lambda_2: 0.004139, Time: 0.28\n",
            "It: 9190, Loss: 8.985e-04, Lambda_1: 0.904, Lambda_2: 0.004140, Time: 0.27\n",
            "It: 9200, Loss: 8.544e-04, Lambda_1: 0.903, Lambda_2: 0.004141, Time: 0.28\n",
            "It: 9210, Loss: 8.161e-04, Lambda_1: 0.903, Lambda_2: 0.004142, Time: 0.28\n",
            "It: 9220, Loss: 7.827e-04, Lambda_1: 0.903, Lambda_2: 0.004142, Time: 0.28\n",
            "It: 9230, Loss: 7.533e-04, Lambda_1: 0.903, Lambda_2: 0.004143, Time: 0.26\n",
            "It: 9240, Loss: 7.273e-04, Lambda_1: 0.903, Lambda_2: 0.004144, Time: 0.28\n",
            "It: 9250, Loss: 7.043e-04, Lambda_1: 0.903, Lambda_2: 0.004144, Time: 0.27\n",
            "It: 9260, Loss: 6.837e-04, Lambda_1: 0.903, Lambda_2: 0.004145, Time: 0.27\n",
            "It: 9270, Loss: 6.652e-04, Lambda_1: 0.903, Lambda_2: 0.004145, Time: 0.28\n",
            "It: 9280, Loss: 6.485e-04, Lambda_1: 0.903, Lambda_2: 0.004145, Time: 0.27\n",
            "It: 9290, Loss: 6.333e-04, Lambda_1: 0.903, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 9300, Loss: 6.195e-04, Lambda_1: 0.903, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 9310, Loss: 6.069e-04, Lambda_1: 0.903, Lambda_2: 0.004146, Time: 0.28\n",
            "It: 9320, Loss: 5.953e-04, Lambda_1: 0.903, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 9330, Loss: 5.846e-04, Lambda_1: 0.903, Lambda_2: 0.004147, Time: 0.28\n",
            "It: 9340, Loss: 5.747e-04, Lambda_1: 0.903, Lambda_2: 0.004147, Time: 0.29\n",
            "It: 9350, Loss: 5.655e-04, Lambda_1: 0.904, Lambda_2: 0.004147, Time: 0.27\n",
            "It: 9360, Loss: 5.569e-04, Lambda_1: 0.904, Lambda_2: 0.004147, Time: 0.28\n",
            "It: 9370, Loss: 5.489e-04, Lambda_1: 0.904, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 9380, Loss: 5.413e-04, Lambda_1: 0.904, Lambda_2: 0.004148, Time: 0.28\n",
            "It: 9390, Loss: 5.343e-04, Lambda_1: 0.904, Lambda_2: 0.004148, Time: 0.28\n",
            "It: 9400, Loss: 5.276e-04, Lambda_1: 0.904, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 9410, Loss: 5.213e-04, Lambda_1: 0.905, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 9420, Loss: 5.153e-04, Lambda_1: 0.905, Lambda_2: 0.004148, Time: 0.29\n",
            "It: 9430, Loss: 5.096e-04, Lambda_1: 0.905, Lambda_2: 0.004148, Time: 0.27\n",
            "It: 9440, Loss: 5.041e-04, Lambda_1: 0.905, Lambda_2: 0.004148, Time: 0.26\n",
            "It: 9450, Loss: 4.989e-04, Lambda_1: 0.906, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9460, Loss: 4.940e-04, Lambda_1: 0.906, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9470, Loss: 4.892e-04, Lambda_1: 0.906, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 9480, Loss: 4.846e-04, Lambda_1: 0.906, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 9490, Loss: 4.802e-04, Lambda_1: 0.906, Lambda_2: 0.004149, Time: 0.29\n",
            "It: 9500, Loss: 4.760e-04, Lambda_1: 0.907, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9510, Loss: 4.719e-04, Lambda_1: 0.907, Lambda_2: 0.004149, Time: 0.26\n",
            "It: 9520, Loss: 4.679e-04, Lambda_1: 0.907, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 9530, Loss: 4.640e-04, Lambda_1: 0.908, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9540, Loss: 4.603e-04, Lambda_1: 0.908, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9550, Loss: 4.566e-04, Lambda_1: 0.908, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 9560, Loss: 4.531e-04, Lambda_1: 0.908, Lambda_2: 0.004149, Time: 0.30\n",
            "It: 9570, Loss: 4.496e-04, Lambda_1: 0.909, Lambda_2: 0.004149, Time: 0.27\n",
            "It: 9580, Loss: 4.463e-04, Lambda_1: 0.909, Lambda_2: 0.004149, Time: 0.28\n",
            "It: 9590, Loss: 4.430e-04, Lambda_1: 0.909, Lambda_2: 0.004150, Time: 0.29\n",
            "It: 9600, Loss: 4.398e-04, Lambda_1: 0.909, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9610, Loss: 4.366e-04, Lambda_1: 0.910, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9620, Loss: 4.335e-04, Lambda_1: 0.910, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9630, Loss: 4.305e-04, Lambda_1: 0.910, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9640, Loss: 4.276e-04, Lambda_1: 0.911, Lambda_2: 0.004150, Time: 0.29\n",
            "It: 9650, Loss: 4.246e-04, Lambda_1: 0.911, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9660, Loss: 4.218e-04, Lambda_1: 0.911, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9670, Loss: 4.190e-04, Lambda_1: 0.911, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9680, Loss: 4.162e-04, Lambda_1: 0.912, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9690, Loss: 4.135e-04, Lambda_1: 0.912, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9700, Loss: 4.108e-04, Lambda_1: 0.912, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9710, Loss: 4.081e-04, Lambda_1: 0.913, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9720, Loss: 4.055e-04, Lambda_1: 0.913, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9730, Loss: 4.030e-04, Lambda_1: 0.913, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9740, Loss: 4.004e-04, Lambda_1: 0.914, Lambda_2: 0.004150, Time: 0.30\n",
            "It: 9750, Loss: 3.979e-04, Lambda_1: 0.914, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9760, Loss: 3.954e-04, Lambda_1: 0.914, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9770, Loss: 3.930e-04, Lambda_1: 0.915, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9780, Loss: 3.906e-04, Lambda_1: 0.915, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9790, Loss: 3.882e-04, Lambda_1: 0.915, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9800, Loss: 3.858e-04, Lambda_1: 0.915, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9810, Loss: 3.835e-04, Lambda_1: 0.916, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9820, Loss: 3.811e-04, Lambda_1: 0.916, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9830, Loss: 3.789e-04, Lambda_1: 0.916, Lambda_2: 0.004150, Time: 0.27\n",
            "It: 9840, Loss: 3.766e-04, Lambda_1: 0.917, Lambda_2: 0.004150, Time: 0.26\n",
            "It: 9850, Loss: 3.743e-04, Lambda_1: 0.917, Lambda_2: 0.004150, Time: 0.28\n",
            "It: 9860, Loss: 3.721e-04, Lambda_1: 0.917, Lambda_2: 0.004150, Time: 0.29\n",
            "It: 9870, Loss: 3.699e-04, Lambda_1: 0.918, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 9880, Loss: 3.677e-04, Lambda_1: 0.918, Lambda_2: 0.004151, Time: 0.29\n",
            "It: 9890, Loss: 3.656e-04, Lambda_1: 0.918, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9900, Loss: 3.634e-04, Lambda_1: 0.919, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9910, Loss: 3.613e-04, Lambda_1: 0.919, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 9920, Loss: 3.592e-04, Lambda_1: 0.919, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 9930, Loss: 3.571e-04, Lambda_1: 0.920, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9940, Loss: 3.550e-04, Lambda_1: 0.920, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9950, Loss: 3.530e-04, Lambda_1: 0.920, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9960, Loss: 3.510e-04, Lambda_1: 0.921, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 9970, Loss: 3.489e-04, Lambda_1: 0.921, Lambda_2: 0.004151, Time: 0.28\n",
            "It: 9980, Loss: 3.469e-04, Lambda_1: 0.921, Lambda_2: 0.004151, Time: 0.27\n",
            "It: 9990, Loss: 3.449e-04, Lambda_1: 0.921, Lambda_2: 0.004151, Time: 0.29\n",
            "Loss: 3.431732e-04, l1: 0.92178, l2: 0.00415\n",
            "Loss: 2.106997e+01, l1: 1.82021, l2: 0.00420\n",
            "Loss: 3.429178e-04, l1: 0.92192, l2: 0.00415\n",
            "Loss: 3.424470e-04, l1: 0.92211, l2: 0.00415\n",
            "Loss: 3.395175e-04, l1: 0.92399, l2: 0.00415\n",
            "Loss: 3.368015e-04, l1: 0.92616, l2: 0.00415\n",
            "Loss: 3.339184e-04, l1: 0.92723, l2: 0.00415\n",
            "Loss: 3.294326e-04, l1: 0.93015, l2: 0.00415\n",
            "Loss: 3.281829e-04, l1: 0.93098, l2: 0.00415\n",
            "Loss: 3.252666e-04, l1: 0.93291, l2: 0.00416\n",
            "Loss: 3.224731e-04, l1: 0.93471, l2: 0.00416\n",
            "Loss: 3.227985e-04, l1: 0.93787, l2: 0.00416\n",
            "Loss: 3.208654e-04, l1: 0.93622, l2: 0.00416\n",
            "Loss: 3.181394e-04, l1: 0.93757, l2: 0.00416\n",
            "Loss: 3.155987e-04, l1: 0.93861, l2: 0.00416\n",
            "Loss: 3.121555e-04, l1: 0.94020, l2: 0.00416\n",
            "Loss: 3.077387e-04, l1: 0.94253, l2: 0.00417\n",
            "Loss: 3.002318e-04, l1: 0.94633, l2: 0.00418\n",
            "Loss: 2.927106e-04, l1: 0.95097, l2: 0.00418\n",
            "Loss: 2.862321e-04, l1: 0.95390, l2: 0.00419\n",
            "Loss: 2.774115e-04, l1: 0.95755, l2: 0.00420\n",
            "Loss: 2.689366e-04, l1: 0.96112, l2: 0.00421\n",
            "Loss: 2.654026e-04, l1: 0.96230, l2: 0.00421\n",
            "Loss: 2.620314e-04, l1: 0.96342, l2: 0.00421\n",
            "Loss: 2.562397e-04, l1: 0.96589, l2: 0.00422\n",
            "Loss: 2.518933e-04, l1: 0.96896, l2: 0.00422\n",
            "Loss: 2.491845e-04, l1: 0.96983, l2: 0.00423\n",
            "Loss: 2.472882e-04, l1: 0.96985, l2: 0.00423\n",
            "Loss: 2.447742e-04, l1: 0.97075, l2: 0.00423\n",
            "Loss: 2.392409e-04, l1: 0.97303, l2: 0.00424\n",
            "Loss: 2.362386e-04, l1: 0.97449, l2: 0.00424\n",
            "Loss: 2.332615e-04, l1: 0.97498, l2: 0.00424\n",
            "Loss: 2.319222e-04, l1: 0.97585, l2: 0.00424\n",
            "Loss: 2.303598e-04, l1: 0.97661, l2: 0.00425\n",
            "Loss: 2.293100e-04, l1: 0.97748, l2: 0.00425\n",
            "Loss: 2.267328e-04, l1: 0.97858, l2: 0.00425\n",
            "Loss: 2.224663e-04, l1: 0.98202, l2: 0.00426\n",
            "Loss: 2.182127e-04, l1: 0.98237, l2: 0.00426\n",
            "Loss: 2.163624e-04, l1: 0.98203, l2: 0.00426\n",
            "Loss: 2.148448e-04, l1: 0.98206, l2: 0.00426\n",
            "Loss: 2.135954e-04, l1: 0.98235, l2: 0.00426\n",
            "Loss: 2.116806e-04, l1: 0.98298, l2: 0.00426\n",
            "Loss: 2.103010e-04, l1: 0.98376, l2: 0.00426\n",
            "Loss: 2.089374e-04, l1: 0.98414, l2: 0.00426\n",
            "Loss: 2.071129e-04, l1: 0.98464, l2: 0.00426\n",
            "Loss: 2.044027e-04, l1: 0.98541, l2: 0.00426\n",
            "Loss: 2.012805e-04, l1: 0.98585, l2: 0.00426\n",
            "Loss: 1.989474e-04, l1: 0.98606, l2: 0.00426\n",
            "Loss: 1.971607e-04, l1: 0.98629, l2: 0.00426\n",
            "Loss: 1.953540e-04, l1: 0.98652, l2: 0.00426\n",
            "Loss: 1.925855e-04, l1: 0.98703, l2: 0.00426\n",
            "Loss: 1.907546e-04, l1: 0.98674, l2: 0.00426\n",
            "Loss: 1.894452e-04, l1: 0.98793, l2: 0.00426\n",
            "Loss: 1.884443e-04, l1: 0.98783, l2: 0.00426\n",
            "Loss: 1.879351e-04, l1: 0.98791, l2: 0.00426\n",
            "Loss: 1.875110e-04, l1: 0.98770, l2: 0.00426\n",
            "Loss: 1.871650e-04, l1: 0.98771, l2: 0.00426\n",
            "Loss: 1.865999e-04, l1: 0.98815, l2: 0.00426\n",
            "Loss: 1.858838e-04, l1: 0.98838, l2: 0.00426\n",
            "Loss: 1.851350e-04, l1: 0.98838, l2: 0.00426\n",
            "Loss: 1.843785e-04, l1: 0.99017, l2: 0.00426\n",
            "Loss: 1.833008e-04, l1: 0.98904, l2: 0.00426\n",
            "Loss: 1.828990e-04, l1: 0.98846, l2: 0.00425\n",
            "Loss: 1.824062e-04, l1: 0.98788, l2: 0.00425\n",
            "Loss: 1.818288e-04, l1: 0.98761, l2: 0.00425\n",
            "Loss: 1.842339e-04, l1: 0.98760, l2: 0.00425\n",
            "Loss: 1.812967e-04, l1: 0.98761, l2: 0.00425\n",
            "Loss: 1.800469e-04, l1: 0.98743, l2: 0.00425\n",
            "Loss: 1.792880e-04, l1: 0.98749, l2: 0.00425\n",
            "Loss: 1.782761e-04, l1: 0.98787, l2: 0.00425\n",
            "Loss: 1.775419e-04, l1: 0.98857, l2: 0.00425\n",
            "Loss: 1.767689e-04, l1: 0.98927, l2: 0.00425\n",
            "Loss: 1.762046e-04, l1: 0.98968, l2: 0.00425\n",
            "Loss: 1.756258e-04, l1: 0.98913, l2: 0.00424\n",
            "Loss: 1.753034e-04, l1: 0.98883, l2: 0.00424\n",
            "Loss: 1.751476e-04, l1: 0.98823, l2: 0.00424\n",
            "Loss: 1.749960e-04, l1: 0.98774, l2: 0.00424\n",
            "Loss: 1.745562e-04, l1: 0.98748, l2: 0.00424\n",
            "Loss: 1.771582e-04, l1: 0.98525, l2: 0.00423\n",
            "Loss: 1.742539e-04, l1: 0.98694, l2: 0.00424\n",
            "Loss: 1.736352e-04, l1: 0.98735, l2: 0.00424\n",
            "Loss: 1.730487e-04, l1: 0.98831, l2: 0.00424\n",
            "Loss: 1.725876e-04, l1: 0.98893, l2: 0.00424\n",
            "Loss: 1.721278e-04, l1: 0.98917, l2: 0.00424\n",
            "Loss: 1.713943e-04, l1: 0.98910, l2: 0.00423\n",
            "Loss: 1.704910e-04, l1: 0.98889, l2: 0.00423\n",
            "Loss: 1.694873e-04, l1: 0.98838, l2: 0.00422\n",
            "Loss: 1.723533e-04, l1: 0.98839, l2: 0.00421\n",
            "Loss: 1.690468e-04, l1: 0.98838, l2: 0.00422\n",
            "Loss: 1.684784e-04, l1: 0.98832, l2: 0.00422\n",
            "Loss: 1.680256e-04, l1: 0.98854, l2: 0.00422\n",
            "Loss: 1.673965e-04, l1: 0.98917, l2: 0.00422\n",
            "Loss: 1.666405e-04, l1: 0.98998, l2: 0.00421\n",
            "Loss: 1.656787e-04, l1: 0.99058, l2: 0.00421\n",
            "Loss: 1.648150e-04, l1: 0.99077, l2: 0.00420\n",
            "Loss: 1.640868e-04, l1: 0.99033, l2: 0.00420\n",
            "Loss: 1.635810e-04, l1: 0.98982, l2: 0.00420\n",
            "Loss: 1.631909e-04, l1: 0.98934, l2: 0.00420\n",
            "Loss: 1.622391e-04, l1: 0.98894, l2: 0.00419\n",
            "Loss: 1.608906e-04, l1: 0.98868, l2: 0.00419\n",
            "Loss: 1.590702e-04, l1: 0.98945, l2: 0.00418\n",
            "Loss: 1.577274e-04, l1: 0.99011, l2: 0.00417\n",
            "Loss: 1.567424e-04, l1: 0.99049, l2: 0.00417\n",
            "Loss: 1.562037e-04, l1: 0.99064, l2: 0.00417\n",
            "Loss: 1.552823e-04, l1: 0.99051, l2: 0.00416\n",
            "Loss: 1.543943e-04, l1: 0.99007, l2: 0.00416\n",
            "Loss: 1.530304e-04, l1: 0.98979, l2: 0.00415\n",
            "Loss: 1.519785e-04, l1: 0.98775, l2: 0.00413\n",
            "Loss: 1.510475e-04, l1: 0.98881, l2: 0.00413\n",
            "Loss: 1.502904e-04, l1: 0.98911, l2: 0.00413\n",
            "Loss: 1.494485e-04, l1: 0.99035, l2: 0.00412\n",
            "Loss: 1.495726e-04, l1: 0.99007, l2: 0.00412\n",
            "Loss: 1.489490e-04, l1: 0.99022, l2: 0.00412\n",
            "Loss: 1.482944e-04, l1: 0.99091, l2: 0.00412\n",
            "Loss: 1.477690e-04, l1: 0.99132, l2: 0.00412\n",
            "Loss: 1.472131e-04, l1: 0.99073, l2: 0.00412\n",
            "Loss: 1.470036e-04, l1: 0.98960, l2: 0.00411\n",
            "Loss: 1.465803e-04, l1: 0.98948, l2: 0.00411\n",
            "Loss: 1.464144e-04, l1: 0.98941, l2: 0.00411\n",
            "Loss: 1.461566e-04, l1: 0.98935, l2: 0.00411\n",
            "Loss: 1.458027e-04, l1: 0.98947, l2: 0.00411\n",
            "Loss: 1.454972e-04, l1: 0.98959, l2: 0.00410\n",
            "Loss: 1.446795e-04, l1: 0.99008, l2: 0.00410\n",
            "Loss: 1.440754e-04, l1: 0.99042, l2: 0.00410\n",
            "Loss: 1.432482e-04, l1: 0.99082, l2: 0.00409\n",
            "Loss: 1.425965e-04, l1: 0.99075, l2: 0.00408\n",
            "Loss: 1.419187e-04, l1: 0.99057, l2: 0.00408\n",
            "Loss: 1.415204e-04, l1: 0.98953, l2: 0.00407\n",
            "Loss: 1.413146e-04, l1: 0.99003, l2: 0.00407\n",
            "Loss: 1.408076e-04, l1: 0.98991, l2: 0.00407\n",
            "Loss: 1.403460e-04, l1: 0.98975, l2: 0.00407\n",
            "Loss: 1.399321e-04, l1: 0.98962, l2: 0.00406\n",
            "Loss: 1.395728e-04, l1: 0.98972, l2: 0.00406\n",
            "Loss: 1.391678e-04, l1: 0.98971, l2: 0.00406\n",
            "Loss: 1.394885e-04, l1: 0.99088, l2: 0.00405\n",
            "Loss: 1.389684e-04, l1: 0.99016, l2: 0.00406\n",
            "Loss: 1.387248e-04, l1: 0.99014, l2: 0.00405\n",
            "Loss: 1.384231e-04, l1: 0.99013, l2: 0.00405\n",
            "Loss: 1.381414e-04, l1: 0.99006, l2: 0.00405\n",
            "Loss: 1.375791e-04, l1: 0.98983, l2: 0.00405\n",
            "Loss: 1.370025e-04, l1: 0.98954, l2: 0.00404\n",
            "Loss: 1.361471e-04, l1: 0.98945, l2: 0.00403\n",
            "Loss: 1.396667e-04, l1: 0.98883, l2: 0.00402\n",
            "Loss: 1.357344e-04, l1: 0.98930, l2: 0.00403\n",
            "Loss: 1.350829e-04, l1: 0.98932, l2: 0.00403\n",
            "Loss: 1.344041e-04, l1: 0.98936, l2: 0.00402\n",
            "Loss: 1.337727e-04, l1: 0.99089, l2: 0.00401\n",
            "Loss: 1.331812e-04, l1: 0.98994, l2: 0.00401\n",
            "Loss: 1.329245e-04, l1: 0.98989, l2: 0.00401\n",
            "Loss: 1.326255e-04, l1: 0.99004, l2: 0.00401\n",
            "Loss: 1.324468e-04, l1: 0.98948, l2: 0.00399\n",
            "Loss: 1.317731e-04, l1: 0.99021, l2: 0.00399\n",
            "Loss: 1.313593e-04, l1: 0.99021, l2: 0.00400\n",
            "Loss: 1.309373e-04, l1: 0.99013, l2: 0.00400\n",
            "Loss: 1.305505e-04, l1: 0.98990, l2: 0.00400\n",
            "Loss: 1.298517e-04, l1: 0.99000, l2: 0.00400\n",
            "Loss: 1.292700e-04, l1: 0.98964, l2: 0.00399\n",
            "Loss: 1.288551e-04, l1: 0.98940, l2: 0.00399\n",
            "Loss: 1.285868e-04, l1: 0.98947, l2: 0.00398\n",
            "Loss: 1.282427e-04, l1: 0.98952, l2: 0.00397\n",
            "Loss: 1.278477e-04, l1: 0.98955, l2: 0.00397\n",
            "Loss: 1.276128e-04, l1: 0.98962, l2: 0.00397\n",
            "Loss: 1.282806e-04, l1: 0.98742, l2: 0.00396\n",
            "Loss: 1.275322e-04, l1: 0.98908, l2: 0.00396\n",
            "Loss: 1.273367e-04, l1: 0.98914, l2: 0.00396\n",
            "Loss: 1.270351e-04, l1: 0.98917, l2: 0.00396\n",
            "Loss: 1.268187e-04, l1: 0.98870, l2: 0.00396\n",
            "Loss: 1.265104e-04, l1: 0.98950, l2: 0.00396\n",
            "Loss: 1.263877e-04, l1: 0.98935, l2: 0.00396\n",
            "Loss: 1.262217e-04, l1: 0.98938, l2: 0.00396\n",
            "Loss: 1.258302e-04, l1: 0.98966, l2: 0.00396\n",
            "Loss: 1.255115e-04, l1: 0.99073, l2: 0.00395\n",
            "Loss: 1.249686e-04, l1: 0.99101, l2: 0.00394\n",
            "Loss: 1.246244e-04, l1: 0.99137, l2: 0.00394\n",
            "Loss: 1.241554e-04, l1: 0.99171, l2: 0.00394\n",
            "Loss: 1.235510e-04, l1: 0.99185, l2: 0.00393\n",
            "Loss: 1.234216e-04, l1: 0.99197, l2: 0.00392\n",
            "Loss: 1.229448e-04, l1: 0.99149, l2: 0.00392\n",
            "Loss: 1.227840e-04, l1: 0.99134, l2: 0.00392\n",
            "Loss: 1.224324e-04, l1: 0.99125, l2: 0.00393\n",
            "Loss: 1.221168e-04, l1: 0.99177, l2: 0.00392\n",
            "Loss: 1.217193e-04, l1: 0.99196, l2: 0.00393\n",
            "Loss: 1.213462e-04, l1: 0.99311, l2: 0.00392\n",
            "Loss: 1.210051e-04, l1: 0.99337, l2: 0.00392\n",
            "Loss: 1.207555e-04, l1: 0.99330, l2: 0.00392\n",
            "Loss: 1.205593e-04, l1: 0.99300, l2: 0.00391\n",
            "Loss: 1.203422e-04, l1: 0.99268, l2: 0.00391\n",
            "Loss: 1.197497e-04, l1: 0.99203, l2: 0.00392\n",
            "Loss: 1.190663e-04, l1: 0.99188, l2: 0.00391\n",
            "Loss: 1.208717e-04, l1: 0.99262, l2: 0.00391\n",
            "Loss: 1.186410e-04, l1: 0.99210, l2: 0.00391\n",
            "Loss: 1.179232e-04, l1: 0.99218, l2: 0.00390\n",
            "Loss: 1.176598e-04, l1: 0.99255, l2: 0.00390\n",
            "Loss: 1.173747e-04, l1: 0.99280, l2: 0.00389\n",
            "Loss: 1.169561e-04, l1: 0.99278, l2: 0.00389\n",
            "Loss: 1.164938e-04, l1: 0.99248, l2: 0.00387\n",
            "Loss: 1.159336e-04, l1: 0.99204, l2: 0.00387\n",
            "Loss: 1.153651e-04, l1: 0.99142, l2: 0.00386\n",
            "Loss: 1.149950e-04, l1: 0.99127, l2: 0.00385\n",
            "Loss: 1.146672e-04, l1: 0.99163, l2: 0.00384\n",
            "Loss: 1.143435e-04, l1: 0.99168, l2: 0.00384\n",
            "Loss: 1.140803e-04, l1: 0.99227, l2: 0.00384\n",
            "Loss: 1.137219e-04, l1: 0.99321, l2: 0.00384\n",
            "Loss: 1.131758e-04, l1: 0.99404, l2: 0.00384\n",
            "Loss: 1.128618e-04, l1: 0.99562, l2: 0.00383\n",
            "Loss: 1.121955e-04, l1: 0.99498, l2: 0.00383\n",
            "Loss: 1.117001e-04, l1: 0.99423, l2: 0.00383\n",
            "Loss: 1.108604e-04, l1: 0.99345, l2: 0.00382\n",
            "Loss: 1.099632e-04, l1: 0.99238, l2: 0.00380\n",
            "Loss: 1.091961e-04, l1: 0.99332, l2: 0.00380\n",
            "Loss: 1.087189e-04, l1: 0.99328, l2: 0.00379\n",
            "Loss: 1.084248e-04, l1: 0.99361, l2: 0.00379\n",
            "Loss: 1.080459e-04, l1: 0.99374, l2: 0.00379\n",
            "Loss: 1.077184e-04, l1: 0.99354, l2: 0.00379\n",
            "Loss: 1.073191e-04, l1: 0.99334, l2: 0.00379\n",
            "Loss: 1.068631e-04, l1: 0.99292, l2: 0.00378\n",
            "Loss: 1.064176e-04, l1: 0.99328, l2: 0.00377\n",
            "Loss: 1.067158e-04, l1: 0.99253, l2: 0.00376\n",
            "Loss: 1.061884e-04, l1: 0.99298, l2: 0.00377\n",
            "Loss: 1.059006e-04, l1: 0.99398, l2: 0.00377\n",
            "Loss: 1.057670e-04, l1: 0.99422, l2: 0.00377\n",
            "Loss: 1.055322e-04, l1: 0.99513, l2: 0.00377\n",
            "Loss: 1.053555e-04, l1: 0.99493, l2: 0.00377\n",
            "Loss: 1.049668e-04, l1: 0.99458, l2: 0.00377\n",
            "Loss: 1.047362e-04, l1: 0.99441, l2: 0.00377\n",
            "Loss: 1.043826e-04, l1: 0.99479, l2: 0.00377\n",
            "Loss: 1.041412e-04, l1: 0.99514, l2: 0.00376\n",
            "Loss: 1.038335e-04, l1: 0.99510, l2: 0.00377\n",
            "Loss: 1.035752e-04, l1: 0.99537, l2: 0.00376\n",
            "Loss: 1.033475e-04, l1: 0.99551, l2: 0.00376\n",
            "Loss: 1.030205e-04, l1: 0.99570, l2: 0.00376\n",
            "Loss: 1.028184e-04, l1: 0.99583, l2: 0.00376\n",
            "Loss: 1.025847e-04, l1: 0.99557, l2: 0.00376\n",
            "Loss: 1.023774e-04, l1: 0.99528, l2: 0.00375\n",
            "Loss: 1.021455e-04, l1: 0.99494, l2: 0.00375\n",
            "Loss: 1.017704e-04, l1: 0.99457, l2: 0.00374\n",
            "Loss: 1.012477e-04, l1: 0.99442, l2: 0.00374\n",
            "Loss: 1.007233e-04, l1: 0.99456, l2: 0.00373\n",
            "Loss: 1.003986e-04, l1: 0.99509, l2: 0.00372\n",
            "Loss: 1.001062e-04, l1: 0.99511, l2: 0.00372\n",
            "Loss: 9.990440e-05, l1: 0.99524, l2: 0.00373\n",
            "Loss: 9.974599e-05, l1: 0.99531, l2: 0.00372\n",
            "Loss: 9.940495e-05, l1: 0.99520, l2: 0.00372\n",
            "Loss: 1.023923e-04, l1: 0.99469, l2: 0.00372\n",
            "Loss: 9.932149e-05, l1: 0.99512, l2: 0.00372\n",
            "Loss: 9.917533e-05, l1: 0.99487, l2: 0.00372\n",
            "Loss: 9.896854e-05, l1: 0.99479, l2: 0.00372\n",
            "Loss: 9.867588e-05, l1: 0.99452, l2: 0.00371\n",
            "Loss: 9.851654e-05, l1: 0.99375, l2: 0.00370\n",
            "Loss: 9.835494e-05, l1: 0.99393, l2: 0.00370\n",
            "Loss: 9.814897e-05, l1: 0.99391, l2: 0.00370\n",
            "Loss: 9.800513e-05, l1: 0.99390, l2: 0.00370\n",
            "Loss: 9.780033e-05, l1: 0.99382, l2: 0.00370\n",
            "Loss: 9.818168e-05, l1: 0.99309, l2: 0.00368\n",
            "Loss: 9.772588e-05, l1: 0.99361, l2: 0.00369\n",
            "Loss: 9.748168e-05, l1: 0.99361, l2: 0.00369\n",
            "Loss: 9.730265e-05, l1: 0.99358, l2: 0.00368\n",
            "Loss: 9.715309e-05, l1: 0.99359, l2: 0.00368\n",
            "Loss: 9.698016e-05, l1: 0.99364, l2: 0.00368\n",
            "Loss: 9.666156e-05, l1: 0.99393, l2: 0.00368\n",
            "Loss: 9.647585e-05, l1: 0.99436, l2: 0.00368\n",
            "Loss: 9.603953e-05, l1: 0.99456, l2: 0.00368\n",
            "Loss: 9.578790e-05, l1: 0.99479, l2: 0.00368\n",
            "Loss: 9.545209e-05, l1: 0.99493, l2: 0.00367\n",
            "Loss: 9.536146e-05, l1: 0.99589, l2: 0.00367\n",
            "Loss: 9.506152e-05, l1: 0.99542, l2: 0.00367\n",
            "Loss: 9.488402e-05, l1: 0.99510, l2: 0.00367\n",
            "Loss: 9.460644e-05, l1: 0.99482, l2: 0.00367\n",
            "Loss: 9.426428e-05, l1: 0.99401, l2: 0.00366\n",
            "Loss: 9.448193e-05, l1: 0.99410, l2: 0.00366\n",
            "Loss: 9.402714e-05, l1: 0.99405, l2: 0.00366\n",
            "Loss: 9.377601e-05, l1: 0.99396, l2: 0.00366\n",
            "Loss: 9.351499e-05, l1: 0.99410, l2: 0.00366\n",
            "Loss: 9.333054e-05, l1: 0.99423, l2: 0.00366\n",
            "Loss: 9.289682e-05, l1: 0.99455, l2: 0.00365\n",
            "Loss: 9.257161e-05, l1: 0.99466, l2: 0.00365\n",
            "Loss: 9.530423e-05, l1: 0.99783, l2: 0.00364\n",
            "Loss: 9.246865e-05, l1: 0.99517, l2: 0.00365\n",
            "Loss: 9.230016e-05, l1: 0.99500, l2: 0.00364\n",
            "Loss: 9.211271e-05, l1: 0.99479, l2: 0.00364\n",
            "Loss: 9.190832e-05, l1: 0.99459, l2: 0.00364\n",
            "Loss: 9.172758e-05, l1: 0.99461, l2: 0.00364\n",
            "Loss: 9.159361e-05, l1: 0.99471, l2: 0.00364\n",
            "Loss: 9.150056e-05, l1: 0.99476, l2: 0.00364\n",
            "Loss: 9.140241e-05, l1: 0.99478, l2: 0.00364\n",
            "Loss: 9.121675e-05, l1: 0.99453, l2: 0.00363\n",
            "Loss: 9.105477e-05, l1: 0.99463, l2: 0.00362\n",
            "Loss: 9.090749e-05, l1: 0.99441, l2: 0.00362\n",
            "Loss: 9.079951e-05, l1: 0.99428, l2: 0.00362\n",
            "Loss: 9.060658e-05, l1: 0.99419, l2: 0.00362\n",
            "Loss: 9.078463e-05, l1: 0.99349, l2: 0.00362\n",
            "Loss: 9.049056e-05, l1: 0.99392, l2: 0.00362\n",
            "Loss: 9.032960e-05, l1: 0.99394, l2: 0.00362\n",
            "Loss: 9.019654e-05, l1: 0.99388, l2: 0.00362\n",
            "Loss: 9.014837e-05, l1: 0.99369, l2: 0.00361\n",
            "Loss: 8.998819e-05, l1: 0.99382, l2: 0.00361\n",
            "Loss: 8.992130e-05, l1: 0.99383, l2: 0.00361\n",
            "Loss: 8.977291e-05, l1: 0.99393, l2: 0.00361\n",
            "Loss: 8.967603e-05, l1: 0.99425, l2: 0.00360\n",
            "Loss: 8.954578e-05, l1: 0.99407, l2: 0.00360\n",
            "Loss: 8.944622e-05, l1: 0.99403, l2: 0.00360\n",
            "Loss: 8.924954e-05, l1: 0.99399, l2: 0.00359\n",
            "Loss: 8.904717e-05, l1: 0.99401, l2: 0.00359\n",
            "Loss: 8.870838e-05, l1: 0.99406, l2: 0.00359\n",
            "Loss: 8.957124e-05, l1: 0.99624, l2: 0.00358\n",
            "Loss: 8.855872e-05, l1: 0.99467, l2: 0.00358\n",
            "Loss: 8.822469e-05, l1: 0.99470, l2: 0.00358\n",
            "Loss: 8.800873e-05, l1: 0.99485, l2: 0.00357\n",
            "Loss: 8.780340e-05, l1: 0.99505, l2: 0.00357\n",
            "Loss: 8.764267e-05, l1: 0.99521, l2: 0.00356\n",
            "Loss: 8.754815e-05, l1: 0.99560, l2: 0.00356\n",
            "Loss: 8.741386e-05, l1: 0.99551, l2: 0.00356\n",
            "Loss: 8.731532e-05, l1: 0.99554, l2: 0.00356\n",
            "Loss: 8.722457e-05, l1: 0.99566, l2: 0.00356\n",
            "Loss: 8.706296e-05, l1: 0.99602, l2: 0.00356\n",
            "Loss: 8.680696e-05, l1: 0.99618, l2: 0.00355\n",
            "Loss: 8.638737e-05, l1: 0.99651, l2: 0.00355\n",
            "Loss: 8.615470e-05, l1: 0.99626, l2: 0.00355\n",
            "Loss: 8.596059e-05, l1: 0.99606, l2: 0.00354\n",
            "Loss: 8.576391e-05, l1: 0.99568, l2: 0.00354\n",
            "Loss: 8.563859e-05, l1: 0.99563, l2: 0.00354\n",
            "Loss: 8.552826e-05, l1: 0.99583, l2: 0.00354\n",
            "Loss: 8.533202e-05, l1: 0.99584, l2: 0.00354\n",
            "Loss: 8.511193e-05, l1: 0.99592, l2: 0.00353\n",
            "Loss: 8.495143e-05, l1: 0.99602, l2: 0.00353\n",
            "Loss: 8.480700e-05, l1: 0.99568, l2: 0.00353\n",
            "Loss: 8.464297e-05, l1: 0.99534, l2: 0.00353\n",
            "Loss: 8.447687e-05, l1: 0.99514, l2: 0.00353\n",
            "Loss: 8.417766e-05, l1: 0.99532, l2: 0.00353\n",
            "Loss: 8.397232e-05, l1: 0.99492, l2: 0.00353\n",
            "Loss: 8.377415e-05, l1: 0.99534, l2: 0.00353\n",
            "Loss: 8.367705e-05, l1: 0.99561, l2: 0.00353\n",
            "Loss: 8.349384e-05, l1: 0.99545, l2: 0.00353\n",
            "Loss: 8.324132e-05, l1: 0.99577, l2: 0.00353\n",
            "Loss: 8.304189e-05, l1: 0.99540, l2: 0.00352\n",
            "Loss: 8.284851e-05, l1: 0.99514, l2: 0.00353\n",
            "Loss: 8.290503e-05, l1: 0.99465, l2: 0.00353\n",
            "Loss: 8.278217e-05, l1: 0.99493, l2: 0.00353\n",
            "Loss: 8.273340e-05, l1: 0.99507, l2: 0.00352\n",
            "Loss: 8.255609e-05, l1: 0.99554, l2: 0.00352\n",
            "Loss: 8.241897e-05, l1: 0.99565, l2: 0.00352\n",
            "Loss: 8.217583e-05, l1: 0.99590, l2: 0.00351\n",
            "Loss: 8.191123e-05, l1: 0.99529, l2: 0.00351\n",
            "Loss: 8.167698e-05, l1: 0.99469, l2: 0.00351\n",
            "Loss: 8.140913e-05, l1: 0.99410, l2: 0.00351\n",
            "Loss: 8.164160e-05, l1: 0.99358, l2: 0.00351\n",
            "Loss: 8.122597e-05, l1: 0.99389, l2: 0.00351\n",
            "Loss: 8.110720e-05, l1: 0.99433, l2: 0.00351\n",
            "Loss: 8.097258e-05, l1: 0.99513, l2: 0.00351\n",
            "Loss: 8.082282e-05, l1: 0.99564, l2: 0.00351\n",
            "Loss: 8.075939e-05, l1: 0.99605, l2: 0.00349\n",
            "Loss: 8.052097e-05, l1: 0.99583, l2: 0.00350\n",
            "Loss: 8.031275e-05, l1: 0.99580, l2: 0.00350\n",
            "Loss: 7.989933e-05, l1: 0.99573, l2: 0.00349\n",
            "Loss: 7.961065e-05, l1: 0.99560, l2: 0.00349\n",
            "Loss: 7.936025e-05, l1: 0.99615, l2: 0.00347\n",
            "Loss: 7.895283e-05, l1: 0.99620, l2: 0.00347\n",
            "Loss: 7.877922e-05, l1: 0.99624, l2: 0.00347\n",
            "Loss: 7.862792e-05, l1: 0.99610, l2: 0.00347\n",
            "Loss: 7.854842e-05, l1: 0.99614, l2: 0.00346\n",
            "Loss: 7.848015e-05, l1: 0.99617, l2: 0.00346\n",
            "Loss: 7.838812e-05, l1: 0.99616, l2: 0.00346\n",
            "Loss: 7.821730e-05, l1: 0.99612, l2: 0.00346\n",
            "Loss: 7.805569e-05, l1: 0.99605, l2: 0.00345\n",
            "Loss: 7.801602e-05, l1: 0.99624, l2: 0.00344\n",
            "Loss: 7.772545e-05, l1: 0.99616, l2: 0.00344\n",
            "Loss: 7.763668e-05, l1: 0.99614, l2: 0.00345\n",
            "Loss: 7.750032e-05, l1: 0.99621, l2: 0.00345\n",
            "Loss: 7.733428e-05, l1: 0.99625, l2: 0.00344\n",
            "Loss: 7.714455e-05, l1: 0.99656, l2: 0.00344\n",
            "Loss: 7.680891e-05, l1: 0.99623, l2: 0.00343\n",
            "Loss: 7.664252e-05, l1: 0.99597, l2: 0.00343\n",
            "Loss: 7.655571e-05, l1: 0.99578, l2: 0.00342\n",
            "Loss: 7.647963e-05, l1: 0.99577, l2: 0.00342\n",
            "Loss: 7.643670e-05, l1: 0.99585, l2: 0.00342\n",
            "Loss: 7.636443e-05, l1: 0.99593, l2: 0.00342\n",
            "Loss: 7.631326e-05, l1: 0.99596, l2: 0.00342\n",
            "Loss: 7.626342e-05, l1: 0.99586, l2: 0.00342\n",
            "Loss: 7.619731e-05, l1: 0.99571, l2: 0.00342\n",
            "Loss: 7.612954e-05, l1: 0.99551, l2: 0.00342\n",
            "Loss: 7.606443e-05, l1: 0.99537, l2: 0.00342\n",
            "Loss: 7.589870e-05, l1: 0.99524, l2: 0.00342\n",
            "Loss: 7.570678e-05, l1: 0.99511, l2: 0.00342\n",
            "Loss: 7.557138e-05, l1: 0.99511, l2: 0.00341\n",
            "Loss: 7.547523e-05, l1: 0.99520, l2: 0.00341\n",
            "Loss: 7.544043e-05, l1: 0.99519, l2: 0.00340\n",
            "Loss: 7.533361e-05, l1: 0.99521, l2: 0.00340\n",
            "Loss: 7.528575e-05, l1: 0.99516, l2: 0.00340\n",
            "Loss: 7.519155e-05, l1: 0.99502, l2: 0.00340\n",
            "Loss: 7.511201e-05, l1: 0.99499, l2: 0.00340\n",
            "Loss: 7.505556e-05, l1: 0.99498, l2: 0.00340\n",
            "Loss: 7.497596e-05, l1: 0.99503, l2: 0.00339\n",
            "Loss: 7.492567e-05, l1: 0.99492, l2: 0.00340\n",
            "Loss: 7.487509e-05, l1: 0.99500, l2: 0.00340\n",
            "Loss: 7.478106e-05, l1: 0.99519, l2: 0.00340\n",
            "Loss: 7.471536e-05, l1: 0.99526, l2: 0.00340\n",
            "Loss: 7.452701e-05, l1: 0.99530, l2: 0.00340\n",
            "Loss: 7.445546e-05, l1: 0.99536, l2: 0.00339\n",
            "Loss: 7.436627e-05, l1: 0.99522, l2: 0.00339\n",
            "Loss: 7.430617e-05, l1: 0.99512, l2: 0.00339\n",
            "Loss: 7.420084e-05, l1: 0.99511, l2: 0.00339\n",
            "Loss: 7.668737e-05, l1: 0.99641, l2: 0.00342\n",
            "Loss: 7.417078e-05, l1: 0.99524, l2: 0.00339\n",
            "Loss: 7.402225e-05, l1: 0.99537, l2: 0.00339\n",
            "Loss: 7.385244e-05, l1: 0.99558, l2: 0.00339\n",
            "Loss: 7.356907e-05, l1: 0.99596, l2: 0.00339\n",
            "Loss: 7.337904e-05, l1: 0.99605, l2: 0.00339\n",
            "Loss: 7.323711e-05, l1: 0.99608, l2: 0.00340\n",
            "Loss: 7.311827e-05, l1: 0.99603, l2: 0.00340\n",
            "Loss: 7.296891e-05, l1: 0.99592, l2: 0.00340\n",
            "Loss: 7.277477e-05, l1: 0.99569, l2: 0.00339\n",
            "Loss: 7.270656e-05, l1: 0.99562, l2: 0.00339\n",
            "Loss: 7.252173e-05, l1: 0.99565, l2: 0.00339\n",
            "Loss: 7.237220e-05, l1: 0.99565, l2: 0.00339\n",
            "Loss: 7.223065e-05, l1: 0.99559, l2: 0.00339\n",
            "Loss: 7.208818e-05, l1: 0.99548, l2: 0.00338\n",
            "Loss: 7.300681e-05, l1: 0.99488, l2: 0.00336\n",
            "Loss: 7.201553e-05, l1: 0.99535, l2: 0.00338\n",
            "Loss: 7.180885e-05, l1: 0.99523, l2: 0.00337\n",
            "Loss: 7.162295e-05, l1: 0.99531, l2: 0.00337\n",
            "Loss: 7.141283e-05, l1: 0.99515, l2: 0.00337\n",
            "Loss: 7.118301e-05, l1: 0.99535, l2: 0.00335\n",
            "Loss: 7.096842e-05, l1: 0.99495, l2: 0.00335\n",
            "Loss: 7.075115e-05, l1: 0.99482, l2: 0.00334\n",
            "Loss: 7.064971e-05, l1: 0.99469, l2: 0.00333\n",
            "Loss: 7.057001e-05, l1: 0.99462, l2: 0.00333\n",
            "Loss: 7.065174e-05, l1: 0.99469, l2: 0.00333\n",
            "Loss: 7.050738e-05, l1: 0.99465, l2: 0.00333\n",
            "Loss: 7.031289e-05, l1: 0.99470, l2: 0.00333\n",
            "Loss: 7.008318e-05, l1: 0.99484, l2: 0.00332\n",
            "Loss: 6.985175e-05, l1: 0.99486, l2: 0.00332\n",
            "Loss: 6.960919e-05, l1: 0.99480, l2: 0.00332\n",
            "Loss: 6.941236e-05, l1: 0.99471, l2: 0.00332\n",
            "Loss: 6.926952e-05, l1: 0.99478, l2: 0.00332\n",
            "Loss: 7.004077e-05, l1: 0.99414, l2: 0.00330\n",
            "Loss: 6.920547e-05, l1: 0.99464, l2: 0.00331\n",
            "Loss: 6.906199e-05, l1: 0.99488, l2: 0.00331\n",
            "Loss: 6.894007e-05, l1: 0.99517, l2: 0.00330\n",
            "Loss: 6.888342e-05, l1: 0.99531, l2: 0.00330\n",
            "Loss: 6.881461e-05, l1: 0.99543, l2: 0.00329\n",
            "Loss: 6.871707e-05, l1: 0.99557, l2: 0.00329\n",
            "Loss: 6.855971e-05, l1: 0.99587, l2: 0.00329\n",
            "Loss: 6.853760e-05, l1: 0.99613, l2: 0.00328\n",
            "Loss: 6.844317e-05, l1: 0.99601, l2: 0.00328\n",
            "Loss: 6.836378e-05, l1: 0.99598, l2: 0.00328\n",
            "Loss: 6.832300e-05, l1: 0.99580, l2: 0.00329\n",
            "Loss: 6.829158e-05, l1: 0.99577, l2: 0.00329\n",
            "Loss: 6.817738e-05, l1: 0.99573, l2: 0.00328\n",
            "Loss: 6.810098e-05, l1: 0.99568, l2: 0.00328\n",
            "Loss: 6.868556e-05, l1: 0.99530, l2: 0.00328\n",
            "Loss: 6.807505e-05, l1: 0.99561, l2: 0.00328\n",
            "Loss: 6.799339e-05, l1: 0.99555, l2: 0.00328\n",
            "Loss: 6.794449e-05, l1: 0.99549, l2: 0.00328\n",
            "Loss: 6.791338e-05, l1: 0.99543, l2: 0.00328\n",
            "Loss: 6.788297e-05, l1: 0.99535, l2: 0.00327\n",
            "Loss: 6.782215e-05, l1: 0.99512, l2: 0.00327\n",
            "Loss: 6.779507e-05, l1: 0.99472, l2: 0.00327\n",
            "Loss: 6.769567e-05, l1: 0.99476, l2: 0.00327\n",
            "Loss: 6.764066e-05, l1: 0.99474, l2: 0.00327\n",
            "Loss: 6.759606e-05, l1: 0.99467, l2: 0.00328\n",
            "Loss: 6.748399e-05, l1: 0.99451, l2: 0.00328\n",
            "Loss: 6.729616e-05, l1: 0.99431, l2: 0.00328\n",
            "Loss: 6.791790e-05, l1: 0.99423, l2: 0.00329\n",
            "Loss: 6.723539e-05, l1: 0.99429, l2: 0.00328\n",
            "Loss: 6.704961e-05, l1: 0.99428, l2: 0.00328\n",
            "Loss: 6.688906e-05, l1: 0.99443, l2: 0.00328\n",
            "Loss: 6.677743e-05, l1: 0.99448, l2: 0.00328\n",
            "Loss: 6.664600e-05, l1: 0.99465, l2: 0.00328\n",
            "Loss: 6.653348e-05, l1: 0.99473, l2: 0.00327\n",
            "Loss: 6.642292e-05, l1: 0.99479, l2: 0.00327\n",
            "Loss: 6.632894e-05, l1: 0.99487, l2: 0.00326\n",
            "Loss: 6.635180e-05, l1: 0.99491, l2: 0.00326\n",
            "Loss: 6.628897e-05, l1: 0.99489, l2: 0.00326\n",
            "Loss: 6.621108e-05, l1: 0.99519, l2: 0.00325\n",
            "Loss: 6.610696e-05, l1: 0.99514, l2: 0.00325\n",
            "Loss: 6.604164e-05, l1: 0.99507, l2: 0.00326\n",
            "Loss: 6.595410e-05, l1: 0.99513, l2: 0.00326\n",
            "Loss: 6.587409e-05, l1: 0.99517, l2: 0.00326\n",
            "Loss: 6.584598e-05, l1: 0.99517, l2: 0.00325\n",
            "Loss: 6.568975e-05, l1: 0.99497, l2: 0.00325\n",
            "Loss: 6.559309e-05, l1: 0.99471, l2: 0.00325\n",
            "Loss: 6.543170e-05, l1: 0.99432, l2: 0.00325\n",
            "Loss: 6.520422e-05, l1: 0.99403, l2: 0.00325\n",
            "Loss: 6.581669e-05, l1: 0.99399, l2: 0.00326\n",
            "Loss: 6.514064e-05, l1: 0.99402, l2: 0.00325\n",
            "Loss: 6.503149e-05, l1: 0.99382, l2: 0.00325\n",
            "Loss: 6.493723e-05, l1: 0.99426, l2: 0.00325\n",
            "Loss: 6.481737e-05, l1: 0.99438, l2: 0.00325\n",
            "Loss: 6.463901e-05, l1: 0.99460, l2: 0.00325\n",
            "Loss: 6.449182e-05, l1: 0.99472, l2: 0.00325\n",
            "Loss: 6.437350e-05, l1: 0.99472, l2: 0.00325\n",
            "Loss: 6.429612e-05, l1: 0.99470, l2: 0.00325\n",
            "Loss: 6.405989e-05, l1: 0.99513, l2: 0.00325\n",
            "Loss: 6.388832e-05, l1: 0.99517, l2: 0.00325\n",
            "Loss: 6.377849e-05, l1: 0.99532, l2: 0.00326\n",
            "Loss: 6.371943e-05, l1: 0.99547, l2: 0.00326\n",
            "Loss: 6.360692e-05, l1: 0.99576, l2: 0.00326\n",
            "Loss: 6.438923e-05, l1: 0.99627, l2: 0.00325\n",
            "Loss: 6.354923e-05, l1: 0.99587, l2: 0.00326\n",
            "Loss: 6.347591e-05, l1: 0.99561, l2: 0.00326\n",
            "Loss: 6.352051e-05, l1: 0.99559, l2: 0.00325\n",
            "Loss: 6.342841e-05, l1: 0.99560, l2: 0.00326\n",
            "Loss: 6.338820e-05, l1: 0.99561, l2: 0.00326\n",
            "Loss: 6.334609e-05, l1: 0.99561, l2: 0.00326\n",
            "Loss: 6.322577e-05, l1: 0.99568, l2: 0.00325\n",
            "Loss: 6.314469e-05, l1: 0.99579, l2: 0.00325\n",
            "Loss: 6.303847e-05, l1: 0.99589, l2: 0.00325\n",
            "Loss: 6.295180e-05, l1: 0.99607, l2: 0.00325\n",
            "Loss: 6.290260e-05, l1: 0.99615, l2: 0.00325\n",
            "Loss: 6.289622e-05, l1: 0.99628, l2: 0.00325\n",
            "Loss: 6.286285e-05, l1: 0.99622, l2: 0.00325\n",
            "Loss: 6.282220e-05, l1: 0.99621, l2: 0.00325\n",
            "Loss: 6.276479e-05, l1: 0.99619, l2: 0.00325\n",
            "Loss: 6.270972e-05, l1: 0.99617, l2: 0.00325\n",
            "Loss: 6.262124e-05, l1: 0.99629, l2: 0.00325\n",
            "Loss: 6.253166e-05, l1: 0.99644, l2: 0.00325\n",
            "Loss: 6.259690e-05, l1: 0.99649, l2: 0.00325\n",
            "Loss: 6.247257e-05, l1: 0.99646, l2: 0.00325\n",
            "Loss: 6.241265e-05, l1: 0.99651, l2: 0.00325\n",
            "Loss: 6.233993e-05, l1: 0.99660, l2: 0.00325\n",
            "Loss: 6.227513e-05, l1: 0.99669, l2: 0.00325\n",
            "Loss: 6.218896e-05, l1: 0.99680, l2: 0.00325\n",
            "Loss: 6.212611e-05, l1: 0.99690, l2: 0.00325\n",
            "Loss: 6.211740e-05, l1: 0.99709, l2: 0.00325\n",
            "Loss: 6.204969e-05, l1: 0.99692, l2: 0.00325\n",
            "Loss: 6.202667e-05, l1: 0.99685, l2: 0.00325\n",
            "Loss: 6.195678e-05, l1: 0.99674, l2: 0.00325\n",
            "Loss: 6.187089e-05, l1: 0.99670, l2: 0.00325\n",
            "Loss: 6.177789e-05, l1: 0.99678, l2: 0.00325\n",
            "Loss: 6.169208e-05, l1: 0.99694, l2: 0.00325\n",
            "Loss: 6.201141e-05, l1: 0.99699, l2: 0.00324\n",
            "Loss: 6.165532e-05, l1: 0.99695, l2: 0.00325\n",
            "Loss: 6.157145e-05, l1: 0.99722, l2: 0.00324\n",
            "Loss: 6.151445e-05, l1: 0.99733, l2: 0.00324\n",
            "Loss: 6.144720e-05, l1: 0.99732, l2: 0.00324\n",
            "Loss: 6.137861e-05, l1: 0.99724, l2: 0.00324\n",
            "Loss: 6.129777e-05, l1: 0.99706, l2: 0.00324\n",
            "Loss: 6.121016e-05, l1: 0.99688, l2: 0.00323\n",
            "Loss: 6.111385e-05, l1: 0.99668, l2: 0.00323\n",
            "Loss: 6.097741e-05, l1: 0.99650, l2: 0.00323\n",
            "Loss: 6.085246e-05, l1: 0.99639, l2: 0.00323\n",
            "Loss: 6.076303e-05, l1: 0.99643, l2: 0.00323\n",
            "Loss: 6.072016e-05, l1: 0.99654, l2: 0.00323\n",
            "Loss: 6.068450e-05, l1: 0.99654, l2: 0.00323\n",
            "Loss: 6.064776e-05, l1: 0.99655, l2: 0.00323\n",
            "Loss: 6.059912e-05, l1: 0.99657, l2: 0.00323\n",
            "Loss: 6.054588e-05, l1: 0.99669, l2: 0.00322\n",
            "Loss: 6.045924e-05, l1: 0.99674, l2: 0.00322\n",
            "Loss: 6.039712e-05, l1: 0.99679, l2: 0.00322\n",
            "Loss: 6.032432e-05, l1: 0.99687, l2: 0.00322\n",
            "Loss: 6.026543e-05, l1: 0.99696, l2: 0.00322\n",
            "Loss: 6.019879e-05, l1: 0.99699, l2: 0.00322\n",
            "Loss: 6.014380e-05, l1: 0.99697, l2: 0.00322\n",
            "Loss: 6.004648e-05, l1: 0.99681, l2: 0.00322\n",
            "Loss: 5.997023e-05, l1: 0.99666, l2: 0.00322\n",
            "Loss: 5.990948e-05, l1: 0.99644, l2: 0.00322\n",
            "Loss: 5.985285e-05, l1: 0.99632, l2: 0.00322\n",
            "Loss: 5.981110e-05, l1: 0.99634, l2: 0.00322\n",
            "Loss: 5.977164e-05, l1: 0.99628, l2: 0.00322\n",
            "Loss: 5.973291e-05, l1: 0.99615, l2: 0.00322\n",
            "Loss: 5.968047e-05, l1: 0.99598, l2: 0.00321\n",
            "Loss: 6.147888e-05, l1: 0.99490, l2: 0.00321\n",
            "Loss: 5.966845e-05, l1: 0.99589, l2: 0.00321\n",
            "Loss: 5.961637e-05, l1: 0.99575, l2: 0.00321\n",
            "Loss: 5.955905e-05, l1: 0.99569, l2: 0.00321\n",
            "Loss: 5.948443e-05, l1: 0.99567, l2: 0.00321\n",
            "Loss: 5.941764e-05, l1: 0.99577, l2: 0.00321\n",
            "Loss: 5.926894e-05, l1: 0.99595, l2: 0.00321\n",
            "Loss: 5.916218e-05, l1: 0.99591, l2: 0.00320\n",
            "Loss: 5.905683e-05, l1: 0.99587, l2: 0.00320\n",
            "Loss: 5.898959e-05, l1: 0.99566, l2: 0.00320\n",
            "Loss: 5.887734e-05, l1: 0.99561, l2: 0.00320\n",
            "Loss: 5.872642e-05, l1: 0.99542, l2: 0.00319\n",
            "Loss: 5.858943e-05, l1: 0.99510, l2: 0.00319\n",
            "Loss: 5.846683e-05, l1: 0.99475, l2: 0.00319\n",
            "Loss: 5.840724e-05, l1: 0.99466, l2: 0.00319\n",
            "Loss: 5.837196e-05, l1: 0.99466, l2: 0.00319\n",
            "Loss: 5.832781e-05, l1: 0.99470, l2: 0.00319\n",
            "Loss: 5.828599e-05, l1: 0.99460, l2: 0.00318\n",
            "Loss: 5.822775e-05, l1: 0.99462, l2: 0.00318\n",
            "Loss: 5.817902e-05, l1: 0.99458, l2: 0.00318\n",
            "Loss: 5.814925e-05, l1: 0.99449, l2: 0.00318\n",
            "Loss: 5.809498e-05, l1: 0.99440, l2: 0.00318\n",
            "Loss: 5.799473e-05, l1: 0.99429, l2: 0.00318\n",
            "Loss: 5.821624e-05, l1: 0.99382, l2: 0.00317\n",
            "Loss: 5.794904e-05, l1: 0.99415, l2: 0.00318\n",
            "Loss: 5.785305e-05, l1: 0.99424, l2: 0.00318\n",
            "Loss: 5.771121e-05, l1: 0.99446, l2: 0.00318\n",
            "Loss: 5.761679e-05, l1: 0.99463, l2: 0.00318\n",
            "Loss: 5.750684e-05, l1: 0.99486, l2: 0.00318\n",
            "Loss: 5.742789e-05, l1: 0.99489, l2: 0.00317\n",
            "Loss: 5.736488e-05, l1: 0.99481, l2: 0.00317\n",
            "Loss: 5.733359e-05, l1: 0.99466, l2: 0.00317\n",
            "Loss: 5.730250e-05, l1: 0.99463, l2: 0.00318\n",
            "Loss: 5.719844e-05, l1: 0.99450, l2: 0.00317\n",
            "Loss: 5.712094e-05, l1: 0.99450, l2: 0.00318\n",
            "Loss: 5.705646e-05, l1: 0.99451, l2: 0.00317\n",
            "Loss: 5.703972e-05, l1: 0.99468, l2: 0.00317\n",
            "Loss: 5.697981e-05, l1: 0.99458, l2: 0.00317\n",
            "Loss: 5.694389e-05, l1: 0.99455, l2: 0.00317\n",
            "Loss: 5.689317e-05, l1: 0.99454, l2: 0.00317\n",
            "Loss: 5.684685e-05, l1: 0.99457, l2: 0.00317\n",
            "Loss: 5.675704e-05, l1: 0.99467, l2: 0.00317\n",
            "Loss: 5.671208e-05, l1: 0.99489, l2: 0.00317\n",
            "Loss: 5.724548e-05, l1: 0.99534, l2: 0.00316\n",
            "Loss: 5.669163e-05, l1: 0.99496, l2: 0.00317\n",
            "Loss: 5.666004e-05, l1: 0.99496, l2: 0.00317\n",
            "Loss: 5.663120e-05, l1: 0.99502, l2: 0.00317\n",
            "Loss: 5.658279e-05, l1: 0.99513, l2: 0.00317\n",
            "Loss: 5.652049e-05, l1: 0.99529, l2: 0.00317\n",
            "Loss: 5.645714e-05, l1: 0.99542, l2: 0.00317\n",
            "Loss: 5.639834e-05, l1: 0.99572, l2: 0.00317\n",
            "Loss: 5.636867e-05, l1: 0.99568, l2: 0.00317\n",
            "Loss: 5.633484e-05, l1: 0.99570, l2: 0.00317\n",
            "Loss: 5.630792e-05, l1: 0.99578, l2: 0.00317\n",
            "Loss: 5.626496e-05, l1: 0.99599, l2: 0.00317\n",
            "Loss: 5.622650e-05, l1: 0.99620, l2: 0.00317\n",
            "Loss: 5.617494e-05, l1: 0.99643, l2: 0.00317\n",
            "Loss: 5.611860e-05, l1: 0.99660, l2: 0.00317\n",
            "Loss: 5.605583e-05, l1: 0.99672, l2: 0.00317\n",
            "Loss: 5.627485e-05, l1: 0.99678, l2: 0.00318\n",
            "Loss: 5.604194e-05, l1: 0.99673, l2: 0.00317\n",
            "Loss: 5.598981e-05, l1: 0.99679, l2: 0.00317\n",
            "Loss: 5.596621e-05, l1: 0.99674, l2: 0.00317\n",
            "Loss: 5.593378e-05, l1: 0.99667, l2: 0.00317\n",
            "Loss: 5.590954e-05, l1: 0.99661, l2: 0.00317\n",
            "Loss: 5.587636e-05, l1: 0.99667, l2: 0.00317\n",
            "Loss: 5.583648e-05, l1: 0.99686, l2: 0.00317\n",
            "Loss: 5.582309e-05, l1: 0.99689, l2: 0.00317\n",
            "Loss: 5.579157e-05, l1: 0.99696, l2: 0.00317\n",
            "Loss: 5.573044e-05, l1: 0.99709, l2: 0.00317\n",
            "Loss: 5.567609e-05, l1: 0.99715, l2: 0.00317\n",
            "Loss: 5.559660e-05, l1: 0.99722, l2: 0.00317\n",
            "Loss: 5.552419e-05, l1: 0.99724, l2: 0.00317\n",
            "Loss: 5.545364e-05, l1: 0.99723, l2: 0.00317\n",
            "Loss: 5.539564e-05, l1: 0.99713, l2: 0.00317\n",
            "Loss: 5.535530e-05, l1: 0.99696, l2: 0.00317\n",
            "Loss: 5.533177e-05, l1: 0.99692, l2: 0.00317\n",
            "Loss: 5.530017e-05, l1: 0.99688, l2: 0.00317\n",
            "Loss: 5.526508e-05, l1: 0.99685, l2: 0.00317\n",
            "Loss: 5.522886e-05, l1: 0.99687, l2: 0.00317\n",
            "Loss: 5.520459e-05, l1: 0.99684, l2: 0.00316\n",
            "Loss: 5.517047e-05, l1: 0.99690, l2: 0.00316\n",
            "Loss: 5.514980e-05, l1: 0.99693, l2: 0.00316\n",
            "Loss: 5.513563e-05, l1: 0.99693, l2: 0.00316\n",
            "Loss: 5.511568e-05, l1: 0.99693, l2: 0.00317\n",
            "Loss: 5.510348e-05, l1: 0.99689, l2: 0.00316\n",
            "Loss: 5.506987e-05, l1: 0.99690, l2: 0.00316\n",
            "Loss: 5.504477e-05, l1: 0.99691, l2: 0.00317\n",
            "Loss: 5.502697e-05, l1: 0.99690, l2: 0.00317\n",
            "Loss: 5.501492e-05, l1: 0.99690, l2: 0.00316\n",
            "Loss: 5.498288e-05, l1: 0.99688, l2: 0.00316\n",
            "Loss: 5.533356e-05, l1: 0.99673, l2: 0.00316\n",
            "Loss: 5.497023e-05, l1: 0.99686, l2: 0.00316\n",
            "Loss: 5.493503e-05, l1: 0.99680, l2: 0.00316\n",
            "Loss: 5.490760e-05, l1: 0.99679, l2: 0.00316\n",
            "Loss: 5.488228e-05, l1: 0.99669, l2: 0.00316\n",
            "Loss: 5.485440e-05, l1: 0.99672, l2: 0.00316\n",
            "Loss: 5.482311e-05, l1: 0.99676, l2: 0.00316\n",
            "Loss: 5.481417e-05, l1: 0.99675, l2: 0.00316\n",
            "Loss: 5.479250e-05, l1: 0.99678, l2: 0.00316\n",
            "Loss: 5.477819e-05, l1: 0.99679, l2: 0.00316\n",
            "Loss: 5.475261e-05, l1: 0.99681, l2: 0.00316\n",
            "Loss: 5.472056e-05, l1: 0.99686, l2: 0.00316\n",
            "Loss: 5.467814e-05, l1: 0.99698, l2: 0.00316\n",
            "Loss: 5.478727e-05, l1: 0.99695, l2: 0.00316\n",
            "Loss: 5.466664e-05, l1: 0.99697, l2: 0.00316\n",
            "Loss: 5.464124e-05, l1: 0.99704, l2: 0.00316\n",
            "Loss: 5.462384e-05, l1: 0.99705, l2: 0.00316\n",
            "Loss: 5.460125e-05, l1: 0.99703, l2: 0.00316\n",
            "Loss: 5.457948e-05, l1: 0.99701, l2: 0.00316\n",
            "Loss: 5.455468e-05, l1: 0.99698, l2: 0.00316\n",
            "Loss: 5.453316e-05, l1: 0.99692, l2: 0.00316\n",
            "Loss: 5.450920e-05, l1: 0.99693, l2: 0.00316\n",
            "Loss: 5.445884e-05, l1: 0.99699, l2: 0.00316\n",
            "Loss: 5.443534e-05, l1: 0.99703, l2: 0.00316\n",
            "Loss: 5.439387e-05, l1: 0.99711, l2: 0.00316\n",
            "Loss: 5.436629e-05, l1: 0.99712, l2: 0.00316\n",
            "Loss: 5.435002e-05, l1: 0.99719, l2: 0.00316\n",
            "Loss: 5.435809e-05, l1: 0.99716, l2: 0.00316\n",
            "Loss: 5.433746e-05, l1: 0.99717, l2: 0.00316\n",
            "Loss: 5.431798e-05, l1: 0.99712, l2: 0.00316\n",
            "Loss: 5.430186e-05, l1: 0.99706, l2: 0.00316\n",
            "Loss: 5.429273e-05, l1: 0.99704, l2: 0.00316\n",
            "Loss: 5.428295e-05, l1: 0.99703, l2: 0.00315\n",
            "Loss: 5.426603e-05, l1: 0.99704, l2: 0.00315\n",
            "Loss: 5.422242e-05, l1: 0.99708, l2: 0.00315\n",
            "Loss: 5.425241e-05, l1: 0.99721, l2: 0.00316\n",
            "Loss: 5.419776e-05, l1: 0.99713, l2: 0.00315\n",
            "Loss: 5.415231e-05, l1: 0.99719, l2: 0.00315\n",
            "Loss: 5.411751e-05, l1: 0.99728, l2: 0.00316\n",
            "Loss: 5.410069e-05, l1: 0.99732, l2: 0.00316\n",
            "Loss: 5.408587e-05, l1: 0.99733, l2: 0.00316\n",
            "Loss: 5.405977e-05, l1: 0.99732, l2: 0.00316\n",
            "Loss: 5.403046e-05, l1: 0.99727, l2: 0.00316\n",
            "Loss: 5.399080e-05, l1: 0.99721, l2: 0.00316\n",
            "Loss: 5.393798e-05, l1: 0.99717, l2: 0.00316\n",
            "Loss: 5.389161e-05, l1: 0.99714, l2: 0.00316\n",
            "Loss: 5.389463e-05, l1: 0.99712, l2: 0.00315\n",
            "Loss: 5.385840e-05, l1: 0.99713, l2: 0.00316\n",
            "Loss: 5.386730e-05, l1: 0.99716, l2: 0.00316\n",
            "Loss: 5.381302e-05, l1: 0.99714, l2: 0.00316\n",
            "Loss: 5.377748e-05, l1: 0.99712, l2: 0.00316\n",
            "Loss: 5.372854e-05, l1: 0.99703, l2: 0.00316\n",
            "Loss: 5.370797e-05, l1: 0.99696, l2: 0.00316\n",
            "Loss: 5.371375e-05, l1: 0.99665, l2: 0.00316\n",
            "Loss: 5.368504e-05, l1: 0.99682, l2: 0.00316\n",
            "Loss: 5.365467e-05, l1: 0.99672, l2: 0.00316\n",
            "Loss: 5.363558e-05, l1: 0.99669, l2: 0.00316\n",
            "Loss: 5.361329e-05, l1: 0.99668, l2: 0.00316\n",
            "Loss: 5.358960e-05, l1: 0.99668, l2: 0.00316\n",
            "Loss: 5.354649e-05, l1: 0.99668, l2: 0.00316\n",
            "Loss: 5.351857e-05, l1: 0.99664, l2: 0.00316\n",
            "Loss: 5.348755e-05, l1: 0.99656, l2: 0.00316\n",
            "Loss: 5.344538e-05, l1: 0.99648, l2: 0.00316\n",
            "Loss: 5.344908e-05, l1: 0.99632, l2: 0.00316\n",
            "Loss: 5.340273e-05, l1: 0.99640, l2: 0.00316\n",
            "Loss: 5.334294e-05, l1: 0.99634, l2: 0.00316\n",
            "Loss: 5.328180e-05, l1: 0.99641, l2: 0.00316\n",
            "Loss: 5.324196e-05, l1: 0.99641, l2: 0.00316\n",
            "Loss: 5.324199e-05, l1: 0.99683, l2: 0.00317\n",
            "Loss: 5.320721e-05, l1: 0.99662, l2: 0.00316\n",
            "Loss: 5.315585e-05, l1: 0.99664, l2: 0.00316\n",
            "Loss: 5.307689e-05, l1: 0.99670, l2: 0.00316\n",
            "Loss: 5.304367e-05, l1: 0.99671, l2: 0.00316\n",
            "Loss: 5.301782e-05, l1: 0.99673, l2: 0.00317\n",
            "Loss: 5.298077e-05, l1: 0.99672, l2: 0.00317\n",
            "Loss: 5.296362e-05, l1: 0.99672, l2: 0.00317\n",
            "Loss: 5.293600e-05, l1: 0.99673, l2: 0.00317\n",
            "Loss: 5.291450e-05, l1: 0.99677, l2: 0.00317\n",
            "Loss: 5.287780e-05, l1: 0.99685, l2: 0.00317\n",
            "Loss: 5.284265e-05, l1: 0.99691, l2: 0.00317\n",
            "Loss: 5.313287e-05, l1: 0.99708, l2: 0.00316\n",
            "Loss: 5.283176e-05, l1: 0.99694, l2: 0.00317\n",
            "Loss: 5.279452e-05, l1: 0.99695, l2: 0.00317\n",
            "Loss: 5.277510e-05, l1: 0.99692, l2: 0.00317\n",
            "Loss: 5.275406e-05, l1: 0.99687, l2: 0.00317\n",
            "Loss: 5.273507e-05, l1: 0.99682, l2: 0.00317\n",
            "Loss: 5.270462e-05, l1: 0.99675, l2: 0.00317\n",
            "Loss: 5.269325e-05, l1: 0.99660, l2: 0.00317\n",
            "Loss: 5.271620e-05, l1: 0.99655, l2: 0.00317\n",
            "Loss: 5.267505e-05, l1: 0.99658, l2: 0.00317\n",
            "Loss: 5.265724e-05, l1: 0.99666, l2: 0.00317\n",
            "Loss: 5.264665e-05, l1: 0.99668, l2: 0.00317\n",
            "Loss: 5.262208e-05, l1: 0.99669, l2: 0.00317\n",
            "Loss: 5.259154e-05, l1: 0.99667, l2: 0.00317\n",
            "Loss: 5.353161e-05, l1: 0.99687, l2: 0.00316\n",
            "Loss: 5.258418e-05, l1: 0.99668, l2: 0.00317\n",
            "Loss: 5.255092e-05, l1: 0.99665, l2: 0.00317\n",
            "Loss: 5.252763e-05, l1: 0.99660, l2: 0.00317\n",
            "Loss: 5.249336e-05, l1: 0.99654, l2: 0.00317\n",
            "Loss: 5.245708e-05, l1: 0.99650, l2: 0.00316\n",
            "Loss: 5.242974e-05, l1: 0.99660, l2: 0.00316\n",
            "Loss: 5.242212e-05, l1: 0.99661, l2: 0.00316\n",
            "Loss: 5.236524e-05, l1: 0.99662, l2: 0.00316\n",
            "Loss: 5.234660e-05, l1: 0.99668, l2: 0.00316\n",
            "Loss: 5.230997e-05, l1: 0.99682, l2: 0.00317\n",
            "Loss: 5.227093e-05, l1: 0.99691, l2: 0.00317\n",
            "Loss: 5.221356e-05, l1: 0.99702, l2: 0.00317\n",
            "Loss: 5.217880e-05, l1: 0.99708, l2: 0.00317\n",
            "Loss: 5.213810e-05, l1: 0.99705, l2: 0.00317\n",
            "Loss: 5.210311e-05, l1: 0.99699, l2: 0.00317\n",
            "Loss: 5.208307e-05, l1: 0.99695, l2: 0.00317\n",
            "Loss: 5.207869e-05, l1: 0.99693, l2: 0.00317\n",
            "Loss: 5.204208e-05, l1: 0.99701, l2: 0.00317\n",
            "Loss: 5.202993e-05, l1: 0.99707, l2: 0.00317\n",
            "Loss: 5.201576e-05, l1: 0.99715, l2: 0.00317\n",
            "Loss: 5.200405e-05, l1: 0.99718, l2: 0.00317\n",
            "Loss: 5.196041e-05, l1: 0.99723, l2: 0.00317\n",
            "Loss: 5.192836e-05, l1: 0.99717, l2: 0.00317\n",
            "Loss: 5.207338e-05, l1: 0.99737, l2: 0.00317\n",
            "Loss: 5.189374e-05, l1: 0.99723, l2: 0.00317\n",
            "Loss: 5.185983e-05, l1: 0.99726, l2: 0.00317\n",
            "Loss: 5.182532e-05, l1: 0.99726, l2: 0.00317\n",
            "Loss: 5.182745e-05, l1: 0.99726, l2: 0.00317\n",
            "Loss: 5.180352e-05, l1: 0.99726, l2: 0.00317\n",
            "Loss: 5.177262e-05, l1: 0.99727, l2: 0.00317\n",
            "Loss: 5.174220e-05, l1: 0.99729, l2: 0.00317\n",
            "Loss: 5.171732e-05, l1: 0.99736, l2: 0.00317\n",
            "Loss: 5.169621e-05, l1: 0.99736, l2: 0.00317\n",
            "Loss: 5.166902e-05, l1: 0.99742, l2: 0.00317\n",
            "Loss: 5.162749e-05, l1: 0.99748, l2: 0.00317\n",
            "Loss: 5.160005e-05, l1: 0.99747, l2: 0.00317\n",
            "Loss: 5.161010e-05, l1: 0.99756, l2: 0.00317\n",
            "Loss: 5.159044e-05, l1: 0.99751, l2: 0.00317\n",
            "Loss: 5.157671e-05, l1: 0.99748, l2: 0.00317\n",
            "Loss: 5.156118e-05, l1: 0.99746, l2: 0.00317\n",
            "Loss: 5.154499e-05, l1: 0.99748, l2: 0.00317\n",
            "Loss: 5.151358e-05, l1: 0.99756, l2: 0.00317\n",
            "Loss: 5.149078e-05, l1: 0.99762, l2: 0.00317\n",
            "Loss: 5.157421e-05, l1: 0.99787, l2: 0.00317\n",
            "Loss: 5.148526e-05, l1: 0.99767, l2: 0.00317\n",
            "Loss: 5.147238e-05, l1: 0.99768, l2: 0.00317\n",
            "Loss: 5.145922e-05, l1: 0.99768, l2: 0.00317\n",
            "Loss: 5.144175e-05, l1: 0.99768, l2: 0.00317\n",
            "Loss: 5.141846e-05, l1: 0.99768, l2: 0.00317\n",
            "Loss: 5.140743e-05, l1: 0.99776, l2: 0.00317\n",
            "Loss: 5.135470e-05, l1: 0.99772, l2: 0.00317\n",
            "Loss: 5.133176e-05, l1: 0.99771, l2: 0.00317\n",
            "Loss: 5.130246e-05, l1: 0.99772, l2: 0.00317\n",
            "Loss: 5.127530e-05, l1: 0.99773, l2: 0.00317\n",
            "Loss: 5.125295e-05, l1: 0.99769, l2: 0.00317\n",
            "Loss: 5.128302e-05, l1: 0.99777, l2: 0.00317\n",
            "Loss: 5.124215e-05, l1: 0.99772, l2: 0.00317\n",
            "Loss: 5.122477e-05, l1: 0.99766, l2: 0.00317\n",
            "Loss: 5.119183e-05, l1: 0.99759, l2: 0.00317\n",
            "Loss: 5.114305e-05, l1: 0.99751, l2: 0.00317\n",
            "Loss: 5.110784e-05, l1: 0.99744, l2: 0.00317\n",
            "Loss: 5.106223e-05, l1: 0.99750, l2: 0.00317\n",
            "Loss: 5.102401e-05, l1: 0.99760, l2: 0.00317\n",
            "Loss: 5.100232e-05, l1: 0.99762, l2: 0.00318\n",
            "Loss: 5.095480e-05, l1: 0.99762, l2: 0.00317\n",
            "Loss: 5.103476e-05, l1: 0.99771, l2: 0.00318\n",
            "Loss: 5.094379e-05, l1: 0.99765, l2: 0.00318\n",
            "Loss: 5.092425e-05, l1: 0.99762, l2: 0.00317\n",
            "Loss: 5.090031e-05, l1: 0.99762, l2: 0.00317\n",
            "Loss: 5.091221e-05, l1: 0.99755, l2: 0.00317\n",
            "Loss: 5.088849e-05, l1: 0.99759, l2: 0.00317\n",
            "Loss: 5.086728e-05, l1: 0.99762, l2: 0.00317\n",
            "Loss: 5.084756e-05, l1: 0.99766, l2: 0.00317\n",
            "Loss: 5.081977e-05, l1: 0.99771, l2: 0.00318\n",
            "Loss: 5.079849e-05, l1: 0.99770, l2: 0.00318\n",
            "Loss: 5.078828e-05, l1: 0.99769, l2: 0.00318\n",
            "Loss: 5.076613e-05, l1: 0.99764, l2: 0.00318\n",
            "Loss: 5.074849e-05, l1: 0.99755, l2: 0.00318\n",
            "Loss: 5.073152e-05, l1: 0.99746, l2: 0.00318\n",
            "Loss: 5.070257e-05, l1: 0.99738, l2: 0.00318\n",
            "Loss: 5.066525e-05, l1: 0.99732, l2: 0.00318\n",
            "Loss: 5.065174e-05, l1: 0.99736, l2: 0.00318\n",
            "Loss: 5.062077e-05, l1: 0.99734, l2: 0.00318\n",
            "Loss: 5.058079e-05, l1: 0.99729, l2: 0.00318\n",
            "Loss: 5.050745e-05, l1: 0.99719, l2: 0.00318\n",
            "Loss: 5.046588e-05, l1: 0.99713, l2: 0.00318\n",
            "Loss: 5.043993e-05, l1: 0.99717, l2: 0.00318\n",
            "Loss: 5.042507e-05, l1: 0.99718, l2: 0.00318\n",
            "Loss: 5.039380e-05, l1: 0.99721, l2: 0.00319\n",
            "Loss: 5.036273e-05, l1: 0.99723, l2: 0.00319\n",
            "Loss: 5.032449e-05, l1: 0.99727, l2: 0.00319\n",
            "Loss: 5.028079e-05, l1: 0.99727, l2: 0.00319\n",
            "Loss: 5.024475e-05, l1: 0.99728, l2: 0.00319\n",
            "Loss: 5.022490e-05, l1: 0.99726, l2: 0.00319\n",
            "Loss: 5.022482e-05, l1: 0.99729, l2: 0.00319\n",
            "Loss: 5.021371e-05, l1: 0.99728, l2: 0.00319\n",
            "Loss: 5.020015e-05, l1: 0.99727, l2: 0.00319\n",
            "Loss: 5.017595e-05, l1: 0.99723, l2: 0.00319\n",
            "Loss: 5.016415e-05, l1: 0.99721, l2: 0.00319\n",
            "Loss: 5.013233e-05, l1: 0.99724, l2: 0.00319\n",
            "Loss: 5.010773e-05, l1: 0.99720, l2: 0.00319\n",
            "Loss: 5.008801e-05, l1: 0.99724, l2: 0.00319\n",
            "Loss: 5.005351e-05, l1: 0.99727, l2: 0.00319\n",
            "Loss: 5.001448e-05, l1: 0.99727, l2: 0.00319\n",
            "Loss: 4.996780e-05, l1: 0.99722, l2: 0.00319\n",
            "Loss: 4.992757e-05, l1: 0.99715, l2: 0.00319\n",
            "Loss: 4.999809e-05, l1: 0.99696, l2: 0.00319\n",
            "Loss: 4.991175e-05, l1: 0.99709, l2: 0.00319\n",
            "Loss: 4.989039e-05, l1: 0.99704, l2: 0.00319\n",
            "Loss: 4.986702e-05, l1: 0.99696, l2: 0.00319\n",
            "Loss: 4.985311e-05, l1: 0.99693, l2: 0.00319\n",
            "Loss: 4.982565e-05, l1: 0.99689, l2: 0.00319\n",
            "Loss: 4.983275e-05, l1: 0.99689, l2: 0.00318\n",
            "Loss: 4.980721e-05, l1: 0.99689, l2: 0.00319\n",
            "Loss: 4.978342e-05, l1: 0.99687, l2: 0.00319\n",
            "Loss: 4.975118e-05, l1: 0.99683, l2: 0.00319\n",
            "Loss: 4.973298e-05, l1: 0.99677, l2: 0.00319\n",
            "Loss: 4.972357e-05, l1: 0.99673, l2: 0.00319\n",
            "Loss: 4.971369e-05, l1: 0.99675, l2: 0.00319\n",
            "Loss: 4.970687e-05, l1: 0.99675, l2: 0.00319\n",
            "Loss: 4.969901e-05, l1: 0.99676, l2: 0.00319\n",
            "Loss: 4.968529e-05, l1: 0.99678, l2: 0.00319\n",
            "Loss: 4.966044e-05, l1: 0.99685, l2: 0.00319\n",
            "Loss: 4.962362e-05, l1: 0.99697, l2: 0.00319\n",
            "Loss: 4.959574e-05, l1: 0.99710, l2: 0.00319\n",
            "Loss: 4.955423e-05, l1: 0.99720, l2: 0.00319\n",
            "Loss: 4.952558e-05, l1: 0.99721, l2: 0.00319\n",
            "Loss: 4.950786e-05, l1: 0.99720, l2: 0.00319\n",
            "Loss: 4.948967e-05, l1: 0.99717, l2: 0.00319\n",
            "Loss: 4.945650e-05, l1: 0.99717, l2: 0.00319\n",
            "Loss: 4.943304e-05, l1: 0.99713, l2: 0.00319\n",
            "Loss: 4.940972e-05, l1: 0.99712, l2: 0.00319\n",
            "Loss: 4.939115e-05, l1: 0.99715, l2: 0.00319\n",
            "Loss: 4.937172e-05, l1: 0.99717, l2: 0.00319\n",
            "Loss: 4.935366e-05, l1: 0.99722, l2: 0.00319\n",
            "Loss: 4.934007e-05, l1: 0.99724, l2: 0.00319\n",
            "Loss: 4.931691e-05, l1: 0.99730, l2: 0.00319\n",
            "Loss: 4.928887e-05, l1: 0.99733, l2: 0.00319\n",
            "Loss: 4.925614e-05, l1: 0.99740, l2: 0.00319\n",
            "Loss: 4.922802e-05, l1: 0.99739, l2: 0.00319\n",
            "Loss: 4.919533e-05, l1: 0.99742, l2: 0.00319\n",
            "Loss: 4.917184e-05, l1: 0.99742, l2: 0.00319\n",
            "Loss: 4.915646e-05, l1: 0.99739, l2: 0.00319\n",
            "Loss: 4.914789e-05, l1: 0.99735, l2: 0.00319\n",
            "Loss: 4.912062e-05, l1: 0.99735, l2: 0.00319\n",
            "Loss: 4.909840e-05, l1: 0.99735, l2: 0.00319\n",
            "Loss: 4.907312e-05, l1: 0.99733, l2: 0.00319\n",
            "Loss: 4.906265e-05, l1: 0.99737, l2: 0.00319\n",
            "Loss: 4.903764e-05, l1: 0.99736, l2: 0.00319\n",
            "Loss: 4.902339e-05, l1: 0.99736, l2: 0.00319\n",
            "Loss: 4.901814e-05, l1: 0.99744, l2: 0.00319\n",
            "Loss: 4.900438e-05, l1: 0.99745, l2: 0.00319\n",
            "Loss: 4.899877e-05, l1: 0.99746, l2: 0.00319\n",
            "Loss: 4.899258e-05, l1: 0.99750, l2: 0.00319\n",
            "Loss: 4.898679e-05, l1: 0.99754, l2: 0.00319\n",
            "Loss: 4.897664e-05, l1: 0.99759, l2: 0.00319\n",
            "Loss: 4.896347e-05, l1: 0.99763, l2: 0.00319\n",
            "Loss: 4.895238e-05, l1: 0.99765, l2: 0.00320\n",
            "Loss: 4.893545e-05, l1: 0.99764, l2: 0.00320\n",
            "Loss: 4.902009e-05, l1: 0.99778, l2: 0.00320\n",
            "Loss: 4.893136e-05, l1: 0.99766, l2: 0.00320\n",
            "Loss: 4.891806e-05, l1: 0.99767, l2: 0.00320\n",
            "Loss: 4.890910e-05, l1: 0.99768, l2: 0.00320\n",
            "Loss: 4.889863e-05, l1: 0.99769, l2: 0.00320\n",
            "Loss: 4.888502e-05, l1: 0.99771, l2: 0.00320\n",
            "Loss: 4.886296e-05, l1: 0.99766, l2: 0.00320\n",
            "Loss: 4.886579e-05, l1: 0.99790, l2: 0.00320\n",
            "Loss: 4.883913e-05, l1: 0.99778, l2: 0.00320\n",
            "Loss: 4.881635e-05, l1: 0.99777, l2: 0.00320\n",
            "Loss: 4.879554e-05, l1: 0.99776, l2: 0.00320\n",
            "Loss: 4.878768e-05, l1: 0.99778, l2: 0.00320\n",
            "Loss: 4.877618e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.876548e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.875660e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.875055e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.874045e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.871691e-05, l1: 0.99782, l2: 0.00320\n",
            "Loss: 4.870084e-05, l1: 0.99784, l2: 0.00320\n",
            "Loss: 4.867370e-05, l1: 0.99780, l2: 0.00320\n",
            "Loss: 4.865818e-05, l1: 0.99775, l2: 0.00320\n",
            "Loss: 4.865031e-05, l1: 0.99769, l2: 0.00320\n",
            "Loss: 4.862693e-05, l1: 0.99762, l2: 0.00320\n",
            "Loss: 4.860978e-05, l1: 0.99762, l2: 0.00320\n",
            "Loss: 4.857653e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.856082e-05, l1: 0.99747, l2: 0.00321\n",
            "Loss: 4.854051e-05, l1: 0.99742, l2: 0.00321\n",
            "Loss: 4.851360e-05, l1: 0.99733, l2: 0.00321\n",
            "Loss: 4.849065e-05, l1: 0.99727, l2: 0.00321\n",
            "Loss: 4.846990e-05, l1: 0.99725, l2: 0.00321\n",
            "Loss: 4.845958e-05, l1: 0.99727, l2: 0.00321\n",
            "Loss: 4.844277e-05, l1: 0.99726, l2: 0.00321\n",
            "Loss: 4.841409e-05, l1: 0.99721, l2: 0.00321\n",
            "Loss: 4.836866e-05, l1: 0.99705, l2: 0.00321\n",
            "Loss: 4.848617e-05, l1: 0.99707, l2: 0.00321\n",
            "Loss: 4.835515e-05, l1: 0.99706, l2: 0.00321\n",
            "Loss: 4.833022e-05, l1: 0.99694, l2: 0.00321\n",
            "Loss: 4.830992e-05, l1: 0.99691, l2: 0.00321\n",
            "Loss: 4.828344e-05, l1: 0.99691, l2: 0.00321\n",
            "Loss: 4.826006e-05, l1: 0.99693, l2: 0.00322\n",
            "Loss: 4.823509e-05, l1: 0.99693, l2: 0.00321\n",
            "Loss: 4.821576e-05, l1: 0.99697, l2: 0.00321\n",
            "Loss: 4.820378e-05, l1: 0.99696, l2: 0.00321\n",
            "Loss: 4.819257e-05, l1: 0.99695, l2: 0.00321\n",
            "Loss: 4.817866e-05, l1: 0.99696, l2: 0.00321\n",
            "Loss: 4.816553e-05, l1: 0.99699, l2: 0.00322\n",
            "Loss: 4.815218e-05, l1: 0.99699, l2: 0.00322\n",
            "Loss: 4.813970e-05, l1: 0.99699, l2: 0.00322\n",
            "Loss: 4.812772e-05, l1: 0.99701, l2: 0.00322\n",
            "Loss: 4.811293e-05, l1: 0.99700, l2: 0.00322\n",
            "Loss: 4.809531e-05, l1: 0.99702, l2: 0.00322\n",
            "Loss: 4.805792e-05, l1: 0.99705, l2: 0.00322\n",
            "Loss: 4.803514e-05, l1: 0.99706, l2: 0.00322\n",
            "Loss: 4.802732e-05, l1: 0.99701, l2: 0.00322\n",
            "Loss: 4.800245e-05, l1: 0.99702, l2: 0.00322\n",
            "Loss: 4.799237e-05, l1: 0.99700, l2: 0.00322\n",
            "Loss: 4.798007e-05, l1: 0.99695, l2: 0.00322\n",
            "Loss: 4.796564e-05, l1: 0.99687, l2: 0.00322\n",
            "Loss: 4.794956e-05, l1: 0.99681, l2: 0.00322\n",
            "Loss: 4.792579e-05, l1: 0.99674, l2: 0.00322\n",
            "Loss: 4.790917e-05, l1: 0.99672, l2: 0.00322\n",
            "Loss: 4.787803e-05, l1: 0.99670, l2: 0.00322\n",
            "Loss: 4.785274e-05, l1: 0.99676, l2: 0.00322\n",
            "Loss: 4.783180e-05, l1: 0.99677, l2: 0.00322\n",
            "Loss: 4.780864e-05, l1: 0.99681, l2: 0.00322\n",
            "Loss: 4.775521e-05, l1: 0.99690, l2: 0.00322\n",
            "Loss: 4.774244e-05, l1: 0.99705, l2: 0.00322\n",
            "Loss: 4.770773e-05, l1: 0.99697, l2: 0.00322\n",
            "Loss: 4.769168e-05, l1: 0.99693, l2: 0.00322\n",
            "Loss: 4.767975e-05, l1: 0.99692, l2: 0.00322\n",
            "Loss: 4.766407e-05, l1: 0.99691, l2: 0.00322\n",
            "Loss: 4.766606e-05, l1: 0.99699, l2: 0.00322\n",
            "Loss: 4.764803e-05, l1: 0.99695, l2: 0.00322\n",
            "Loss: 4.761572e-05, l1: 0.99686, l2: 0.00322\n",
            "Loss: 4.756908e-05, l1: 0.99679, l2: 0.00322\n",
            "Loss: 4.751952e-05, l1: 0.99670, l2: 0.00322\n",
            "Loss: 4.750761e-05, l1: 0.99673, l2: 0.00322\n",
            "Loss: 4.746675e-05, l1: 0.99674, l2: 0.00322\n",
            "Loss: 4.744381e-05, l1: 0.99679, l2: 0.00322\n",
            "Loss: 4.743116e-05, l1: 0.99686, l2: 0.00322\n",
            "Loss: 4.741901e-05, l1: 0.99698, l2: 0.00322\n",
            "Loss: 4.740539e-05, l1: 0.99703, l2: 0.00322\n",
            "Loss: 4.738054e-05, l1: 0.99713, l2: 0.00322\n",
            "Loss: 4.737623e-05, l1: 0.99703, l2: 0.00322\n",
            "Loss: 4.736475e-05, l1: 0.99724, l2: 0.00322\n",
            "Loss: 4.732576e-05, l1: 0.99714, l2: 0.00322\n",
            "Loss: 4.731157e-05, l1: 0.99710, l2: 0.00322\n",
            "Loss: 4.727489e-05, l1: 0.99706, l2: 0.00322\n",
            "Loss: 4.724973e-05, l1: 0.99713, l2: 0.00322\n",
            "Loss: 4.721769e-05, l1: 0.99720, l2: 0.00322\n",
            "Loss: 4.718674e-05, l1: 0.99736, l2: 0.00322\n",
            "Loss: 4.714147e-05, l1: 0.99732, l2: 0.00322\n",
            "Loss: 4.712190e-05, l1: 0.99731, l2: 0.00322\n",
            "Loss: 4.710861e-05, l1: 0.99723, l2: 0.00322\n",
            "Loss: 4.709177e-05, l1: 0.99724, l2: 0.00322\n",
            "Loss: 4.705901e-05, l1: 0.99725, l2: 0.00322\n",
            "Loss: 4.703679e-05, l1: 0.99724, l2: 0.00323\n",
            "Loss: 4.701251e-05, l1: 0.99726, l2: 0.00323\n",
            "Loss: 4.699709e-05, l1: 0.99724, l2: 0.00323\n",
            "Loss: 4.698223e-05, l1: 0.99725, l2: 0.00323\n",
            "Loss: 4.695962e-05, l1: 0.99731, l2: 0.00323\n",
            "Loss: 4.692778e-05, l1: 0.99740, l2: 0.00323\n",
            "Loss: 4.732711e-05, l1: 0.99782, l2: 0.00323\n",
            "Loss: 4.691479e-05, l1: 0.99746, l2: 0.00323\n",
            "Loss: 4.688786e-05, l1: 0.99754, l2: 0.00323\n",
            "Loss: 4.686091e-05, l1: 0.99759, l2: 0.00323\n",
            "Loss: 4.682676e-05, l1: 0.99761, l2: 0.00323\n",
            "Loss: 4.676482e-05, l1: 0.99758, l2: 0.00323\n",
            "Loss: 4.671859e-05, l1: 0.99751, l2: 0.00323\n",
            "Loss: 4.668970e-05, l1: 0.99747, l2: 0.00323\n",
            "Loss: 4.665078e-05, l1: 0.99743, l2: 0.00323\n",
            "Loss: 4.663248e-05, l1: 0.99745, l2: 0.00323\n",
            "Loss: 4.661267e-05, l1: 0.99740, l2: 0.00323\n",
            "Loss: 4.658910e-05, l1: 0.99739, l2: 0.00323\n",
            "Loss: 4.656404e-05, l1: 0.99740, l2: 0.00323\n",
            "Loss: 4.654413e-05, l1: 0.99743, l2: 0.00323\n",
            "Loss: 4.652545e-05, l1: 0.99745, l2: 0.00323\n",
            "Loss: 4.650773e-05, l1: 0.99747, l2: 0.00323\n",
            "Loss: 4.648249e-05, l1: 0.99748, l2: 0.00323\n",
            "Loss: 4.643440e-05, l1: 0.99749, l2: 0.00323\n",
            "Loss: 4.647053e-05, l1: 0.99746, l2: 0.00324\n",
            "Loss: 4.639716e-05, l1: 0.99748, l2: 0.00323\n",
            "Loss: 4.630822e-05, l1: 0.99750, l2: 0.00323\n",
            "Loss: 4.625537e-05, l1: 0.99752, l2: 0.00323\n",
            "Loss: 4.621302e-05, l1: 0.99760, l2: 0.00323\n",
            "Loss: 4.619113e-05, l1: 0.99766, l2: 0.00323\n",
            "Loss: 4.617216e-05, l1: 0.99767, l2: 0.00323\n",
            "Loss: 4.615066e-05, l1: 0.99773, l2: 0.00323\n",
            "Loss: 4.613643e-05, l1: 0.99782, l2: 0.00323\n",
            "Loss: 4.611674e-05, l1: 0.99792, l2: 0.00323\n",
            "Loss: 4.607500e-05, l1: 0.99805, l2: 0.00323\n",
            "Loss: 4.604835e-05, l1: 0.99814, l2: 0.00323\n",
            "Loss: 4.602087e-05, l1: 0.99821, l2: 0.00323\n",
            "Loss: 4.599194e-05, l1: 0.99819, l2: 0.00323\n",
            "Loss: 4.595780e-05, l1: 0.99806, l2: 0.00323\n",
            "Loss: 4.592746e-05, l1: 0.99812, l2: 0.00323\n",
            "Loss: 4.589470e-05, l1: 0.99812, l2: 0.00323\n",
            "Loss: 4.586613e-05, l1: 0.99829, l2: 0.00323\n",
            "Loss: 4.584814e-05, l1: 0.99829, l2: 0.00323\n",
            "Loss: 4.582574e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.580555e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.578155e-05, l1: 0.99843, l2: 0.00323\n",
            "Loss: 4.574565e-05, l1: 0.99853, l2: 0.00323\n",
            "Loss: 4.572462e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.570445e-05, l1: 0.99869, l2: 0.00323\n",
            "Loss: 4.567502e-05, l1: 0.99871, l2: 0.00323\n",
            "Loss: 4.565555e-05, l1: 0.99873, l2: 0.00323\n",
            "Loss: 4.560990e-05, l1: 0.99877, l2: 0.00323\n",
            "Loss: 4.559895e-05, l1: 0.99882, l2: 0.00323\n",
            "Loss: 4.556154e-05, l1: 0.99884, l2: 0.00323\n",
            "Loss: 4.554470e-05, l1: 0.99886, l2: 0.00323\n",
            "Loss: 4.552649e-05, l1: 0.99887, l2: 0.00323\n",
            "Loss: 4.551574e-05, l1: 0.99886, l2: 0.00323\n",
            "Loss: 4.547261e-05, l1: 0.99886, l2: 0.00323\n",
            "Loss: 4.544092e-05, l1: 0.99886, l2: 0.00323\n",
            "Loss: 4.539673e-05, l1: 0.99883, l2: 0.00323\n",
            "Loss: 4.538582e-05, l1: 0.99878, l2: 0.00323\n",
            "Loss: 4.535282e-05, l1: 0.99877, l2: 0.00323\n",
            "Loss: 4.533788e-05, l1: 0.99876, l2: 0.00323\n",
            "Loss: 4.532941e-05, l1: 0.99877, l2: 0.00323\n",
            "Loss: 4.531660e-05, l1: 0.99882, l2: 0.00323\n",
            "Loss: 4.529715e-05, l1: 0.99890, l2: 0.00323\n",
            "Loss: 4.527240e-05, l1: 0.99895, l2: 0.00323\n",
            "Loss: 4.523501e-05, l1: 0.99897, l2: 0.00323\n",
            "Loss: 4.549661e-05, l1: 0.99874, l2: 0.00323\n",
            "Loss: 4.522255e-05, l1: 0.99893, l2: 0.00323\n",
            "Loss: 4.518957e-05, l1: 0.99889, l2: 0.00323\n",
            "Loss: 4.516762e-05, l1: 0.99881, l2: 0.00323\n",
            "Loss: 4.514100e-05, l1: 0.99872, l2: 0.00323\n",
            "Loss: 4.515477e-05, l1: 0.99880, l2: 0.00323\n",
            "Loss: 4.512563e-05, l1: 0.99875, l2: 0.00323\n",
            "Loss: 4.509092e-05, l1: 0.99870, l2: 0.00323\n",
            "Loss: 4.507849e-05, l1: 0.99873, l2: 0.00323\n",
            "Loss: 4.507137e-05, l1: 0.99876, l2: 0.00323\n",
            "Loss: 4.506317e-05, l1: 0.99874, l2: 0.00323\n",
            "Loss: 4.505388e-05, l1: 0.99872, l2: 0.00323\n",
            "Loss: 4.504321e-05, l1: 0.99869, l2: 0.00323\n",
            "Loss: 4.502605e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.502683e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.501725e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.499946e-05, l1: 0.99857, l2: 0.00323\n",
            "Loss: 4.497726e-05, l1: 0.99859, l2: 0.00323\n",
            "Loss: 4.495426e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.493673e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.491611e-05, l1: 0.99866, l2: 0.00323\n",
            "Loss: 4.489770e-05, l1: 0.99869, l2: 0.00323\n",
            "Loss: 4.486969e-05, l1: 0.99871, l2: 0.00323\n",
            "Loss: 4.484605e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.485414e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.483218e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.480379e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.477303e-05, l1: 0.99853, l2: 0.00323\n",
            "Loss: 4.475234e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.473856e-05, l1: 0.99846, l2: 0.00323\n",
            "Loss: 4.472678e-05, l1: 0.99842, l2: 0.00323\n",
            "Loss: 4.471688e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.470759e-05, l1: 0.99828, l2: 0.00324\n",
            "Loss: 4.469755e-05, l1: 0.99824, l2: 0.00323\n",
            "Loss: 4.469062e-05, l1: 0.99820, l2: 0.00323\n",
            "Loss: 4.468174e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.466709e-05, l1: 0.99817, l2: 0.00323\n",
            "Loss: 4.465527e-05, l1: 0.99821, l2: 0.00323\n",
            "Loss: 4.463995e-05, l1: 0.99825, l2: 0.00324\n",
            "Loss: 4.461145e-05, l1: 0.99833, l2: 0.00324\n",
            "Loss: 4.459516e-05, l1: 0.99835, l2: 0.00324\n",
            "Loss: 4.457742e-05, l1: 0.99832, l2: 0.00324\n",
            "Loss: 4.458569e-05, l1: 0.99825, l2: 0.00324\n",
            "Loss: 4.456770e-05, l1: 0.99829, l2: 0.00324\n",
            "Loss: 4.455245e-05, l1: 0.99822, l2: 0.00324\n",
            "Loss: 4.453937e-05, l1: 0.99814, l2: 0.00324\n",
            "Loss: 4.452815e-05, l1: 0.99809, l2: 0.00324\n",
            "Loss: 4.451821e-05, l1: 0.99807, l2: 0.00324\n",
            "Loss: 4.450612e-05, l1: 0.99807, l2: 0.00324\n",
            "Loss: 4.449364e-05, l1: 0.99802, l2: 0.00324\n",
            "Loss: 4.448570e-05, l1: 0.99801, l2: 0.00324\n",
            "Loss: 4.447828e-05, l1: 0.99799, l2: 0.00324\n",
            "Loss: 4.446679e-05, l1: 0.99793, l2: 0.00324\n",
            "Loss: 4.445524e-05, l1: 0.99784, l2: 0.00324\n",
            "Loss: 4.443671e-05, l1: 0.99770, l2: 0.00324\n",
            "Loss: 4.441321e-05, l1: 0.99756, l2: 0.00324\n",
            "Loss: 4.442890e-05, l1: 0.99748, l2: 0.00324\n",
            "Loss: 4.439957e-05, l1: 0.99753, l2: 0.00324\n",
            "Loss: 4.437480e-05, l1: 0.99745, l2: 0.00324\n",
            "Loss: 4.436308e-05, l1: 0.99747, l2: 0.00323\n",
            "Loss: 4.435398e-05, l1: 0.99753, l2: 0.00323\n",
            "Loss: 4.433895e-05, l1: 0.99753, l2: 0.00323\n",
            "Loss: 4.432648e-05, l1: 0.99751, l2: 0.00323\n",
            "Loss: 4.432038e-05, l1: 0.99750, l2: 0.00323\n",
            "Loss: 4.431554e-05, l1: 0.99750, l2: 0.00323\n",
            "Loss: 4.433006e-05, l1: 0.99747, l2: 0.00323\n",
            "Loss: 4.431317e-05, l1: 0.99749, l2: 0.00323\n",
            "Loss: 4.430957e-05, l1: 0.99750, l2: 0.00323\n",
            "Loss: 4.430053e-05, l1: 0.99752, l2: 0.00323\n",
            "Loss: 4.428558e-05, l1: 0.99752, l2: 0.00323\n",
            "Loss: 4.426165e-05, l1: 0.99750, l2: 0.00323\n",
            "Loss: 4.663406e-05, l1: 0.99727, l2: 0.00322\n",
            "Loss: 4.425690e-05, l1: 0.99749, l2: 0.00323\n",
            "Loss: 4.422735e-05, l1: 0.99744, l2: 0.00323\n",
            "Loss: 4.419249e-05, l1: 0.99733, l2: 0.00323\n",
            "Loss: 4.421561e-05, l1: 0.99742, l2: 0.00323\n",
            "Loss: 4.418183e-05, l1: 0.99736, l2: 0.00323\n",
            "Loss: 4.416048e-05, l1: 0.99727, l2: 0.00323\n",
            "Loss: 4.414403e-05, l1: 0.99723, l2: 0.00323\n",
            "Loss: 4.411628e-05, l1: 0.99722, l2: 0.00323\n",
            "Loss: 4.408119e-05, l1: 0.99723, l2: 0.00323\n",
            "Loss: 4.405409e-05, l1: 0.99730, l2: 0.00323\n",
            "Loss: 4.403630e-05, l1: 0.99732, l2: 0.00323\n",
            "Loss: 4.402459e-05, l1: 0.99733, l2: 0.00323\n",
            "Loss: 4.400405e-05, l1: 0.99732, l2: 0.00323\n",
            "Loss: 4.399928e-05, l1: 0.99727, l2: 0.00323\n",
            "Loss: 4.396908e-05, l1: 0.99728, l2: 0.00323\n",
            "Loss: 4.395808e-05, l1: 0.99729, l2: 0.00323\n",
            "Loss: 4.392775e-05, l1: 0.99724, l2: 0.00323\n",
            "Loss: 4.390111e-05, l1: 0.99733, l2: 0.00323\n",
            "Loss: 4.389371e-05, l1: 0.99732, l2: 0.00323\n",
            "Loss: 4.385796e-05, l1: 0.99731, l2: 0.00323\n",
            "Loss: 4.384786e-05, l1: 0.99736, l2: 0.00323\n",
            "Loss: 4.383387e-05, l1: 0.99735, l2: 0.00323\n",
            "Loss: 4.383237e-05, l1: 0.99748, l2: 0.00323\n",
            "Loss: 4.382028e-05, l1: 0.99742, l2: 0.00323\n",
            "Loss: 4.380750e-05, l1: 0.99742, l2: 0.00323\n",
            "Loss: 4.378393e-05, l1: 0.99742, l2: 0.00323\n",
            "Loss: 4.376177e-05, l1: 0.99749, l2: 0.00323\n",
            "Loss: 4.373962e-05, l1: 0.99751, l2: 0.00323\n",
            "Loss: 4.375035e-05, l1: 0.99758, l2: 0.00323\n",
            "Loss: 4.373035e-05, l1: 0.99754, l2: 0.00323\n",
            "Loss: 4.372163e-05, l1: 0.99753, l2: 0.00323\n",
            "Loss: 4.371203e-05, l1: 0.99754, l2: 0.00323\n",
            "Loss: 4.370535e-05, l1: 0.99754, l2: 0.00323\n",
            "Loss: 4.369796e-05, l1: 0.99758, l2: 0.00323\n",
            "Loss: 4.369166e-05, l1: 0.99761, l2: 0.00323\n",
            "Loss: 4.368309e-05, l1: 0.99765, l2: 0.00323\n",
            "Loss: 4.367223e-05, l1: 0.99768, l2: 0.00323\n",
            "Loss: 4.367207e-05, l1: 0.99782, l2: 0.00323\n",
            "Loss: 4.366432e-05, l1: 0.99775, l2: 0.00323\n",
            "Loss: 4.365643e-05, l1: 0.99774, l2: 0.00323\n",
            "Loss: 4.364702e-05, l1: 0.99773, l2: 0.00323\n",
            "Loss: 4.363639e-05, l1: 0.99772, l2: 0.00323\n",
            "Loss: 4.361560e-05, l1: 0.99775, l2: 0.00323\n",
            "Loss: 4.367939e-05, l1: 0.99795, l2: 0.00323\n",
            "Loss: 4.360784e-05, l1: 0.99780, l2: 0.00323\n",
            "Loss: 4.359715e-05, l1: 0.99781, l2: 0.00323\n",
            "Loss: 4.359093e-05, l1: 0.99784, l2: 0.00323\n",
            "Loss: 4.358672e-05, l1: 0.99785, l2: 0.00323\n",
            "Loss: 4.357602e-05, l1: 0.99788, l2: 0.00323\n",
            "Loss: 4.356475e-05, l1: 0.99789, l2: 0.00323\n",
            "Loss: 4.356000e-05, l1: 0.99802, l2: 0.00323\n",
            "Loss: 4.354594e-05, l1: 0.99797, l2: 0.00323\n",
            "Loss: 4.353933e-05, l1: 0.99795, l2: 0.00323\n",
            "Loss: 4.352981e-05, l1: 0.99799, l2: 0.00323\n",
            "Loss: 4.351827e-05, l1: 0.99807, l2: 0.00323\n",
            "Loss: 4.351832e-05, l1: 0.99810, l2: 0.00323\n",
            "Loss: 4.351125e-05, l1: 0.99809, l2: 0.00323\n",
            "Loss: 4.353086e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.350499e-05, l1: 0.99813, l2: 0.00323\n",
            "Loss: 4.349154e-05, l1: 0.99819, l2: 0.00323\n",
            "Loss: 4.348280e-05, l1: 0.99817, l2: 0.00323\n",
            "Loss: 4.346611e-05, l1: 0.99814, l2: 0.00323\n",
            "Loss: 4.346062e-05, l1: 0.99810, l2: 0.00323\n",
            "Loss: 4.348956e-05, l1: 0.99838, l2: 0.00323\n",
            "Loss: 4.345649e-05, l1: 0.99817, l2: 0.00323\n",
            "Loss: 4.344928e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.343881e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.342874e-05, l1: 0.99826, l2: 0.00323\n",
            "Loss: 4.341589e-05, l1: 0.99831, l2: 0.00323\n",
            "Loss: 4.340597e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.339887e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.339388e-05, l1: 0.99826, l2: 0.00323\n",
            "Loss: 4.338917e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.337865e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.336566e-05, l1: 0.99816, l2: 0.00323\n",
            "Loss: 4.338770e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.336111e-05, l1: 0.99817, l2: 0.00323\n",
            "Loss: 4.334894e-05, l1: 0.99819, l2: 0.00323\n",
            "Loss: 4.334848e-05, l1: 0.99808, l2: 0.00323\n",
            "Loss: 4.334447e-05, l1: 0.99813, l2: 0.00323\n",
            "Loss: 4.333465e-05, l1: 0.99819, l2: 0.00323\n",
            "Loss: 4.332236e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.330694e-05, l1: 0.99834, l2: 0.00323\n",
            "Loss: 4.329069e-05, l1: 0.99835, l2: 0.00323\n",
            "Loss: 4.327705e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.325924e-05, l1: 0.99839, l2: 0.00323\n",
            "Loss: 4.324960e-05, l1: 0.99841, l2: 0.00323\n",
            "Loss: 4.323348e-05, l1: 0.99846, l2: 0.00323\n",
            "Loss: 4.322676e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.322238e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.321615e-05, l1: 0.99847, l2: 0.00323\n",
            "Loss: 4.320859e-05, l1: 0.99847, l2: 0.00323\n",
            "Loss: 4.319443e-05, l1: 0.99849, l2: 0.00323\n",
            "Loss: 4.317978e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.316184e-05, l1: 0.99861, l2: 0.00323\n",
            "Loss: 4.314582e-05, l1: 0.99866, l2: 0.00323\n",
            "Loss: 4.313029e-05, l1: 0.99870, l2: 0.00323\n",
            "Loss: 4.312722e-05, l1: 0.99876, l2: 0.00323\n",
            "Loss: 4.311523e-05, l1: 0.99873, l2: 0.00323\n",
            "Loss: 4.310915e-05, l1: 0.99871, l2: 0.00323\n",
            "Loss: 4.310090e-05, l1: 0.99869, l2: 0.00323\n",
            "Loss: 4.309319e-05, l1: 0.99866, l2: 0.00323\n",
            "Loss: 4.310451e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.308903e-05, l1: 0.99866, l2: 0.00323\n",
            "Loss: 4.308291e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.307615e-05, l1: 0.99864, l2: 0.00323\n",
            "Loss: 4.306527e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.304640e-05, l1: 0.99868, l2: 0.00323\n",
            "Loss: 4.303413e-05, l1: 0.99876, l2: 0.00323\n",
            "Loss: 4.301712e-05, l1: 0.99878, l2: 0.00323\n",
            "Loss: 4.301117e-05, l1: 0.99880, l2: 0.00323\n",
            "Loss: 4.300717e-05, l1: 0.99882, l2: 0.00323\n",
            "Loss: 4.300193e-05, l1: 0.99885, l2: 0.00323\n",
            "Loss: 4.299414e-05, l1: 0.99887, l2: 0.00323\n",
            "Loss: 4.298332e-05, l1: 0.99889, l2: 0.00323\n",
            "Loss: 4.303551e-05, l1: 0.99907, l2: 0.00323\n",
            "Loss: 4.298029e-05, l1: 0.99892, l2: 0.00323\n",
            "Loss: 4.297234e-05, l1: 0.99891, l2: 0.00323\n",
            "Loss: 4.296383e-05, l1: 0.99888, l2: 0.00323\n",
            "Loss: 4.295600e-05, l1: 0.99886, l2: 0.00323\n",
            "Loss: 4.294764e-05, l1: 0.99885, l2: 0.00323\n",
            "Loss: 4.293630e-05, l1: 0.99885, l2: 0.00323\n",
            "Loss: 4.291995e-05, l1: 0.99890, l2: 0.00323\n",
            "Loss: 4.290874e-05, l1: 0.99892, l2: 0.00323\n",
            "Loss: 4.290089e-05, l1: 0.99899, l2: 0.00323\n",
            "Loss: 4.289467e-05, l1: 0.99899, l2: 0.00323\n",
            "Loss: 4.288441e-05, l1: 0.99900, l2: 0.00323\n",
            "Loss: 4.287245e-05, l1: 0.99899, l2: 0.00323\n",
            "Loss: 4.285716e-05, l1: 0.99899, l2: 0.00323\n",
            "Loss: 4.285024e-05, l1: 0.99898, l2: 0.00323\n",
            "Loss: 4.283914e-05, l1: 0.99896, l2: 0.00323\n",
            "Loss: 4.283361e-05, l1: 0.99893, l2: 0.00323\n",
            "Loss: 4.282565e-05, l1: 0.99889, l2: 0.00323\n",
            "Loss: 4.281809e-05, l1: 0.99884, l2: 0.00323\n",
            "Loss: 4.280853e-05, l1: 0.99878, l2: 0.00323\n",
            "Loss: 4.279871e-05, l1: 0.99873, l2: 0.00323\n",
            "Loss: 4.278434e-05, l1: 0.99869, l2: 0.00323\n",
            "Loss: 4.278923e-05, l1: 0.99861, l2: 0.00323\n",
            "Loss: 4.277836e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.276785e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.275262e-05, l1: 0.99870, l2: 0.00323\n",
            "Loss: 4.273922e-05, l1: 0.99873, l2: 0.00323\n",
            "Loss: 4.272155e-05, l1: 0.99876, l2: 0.00323\n",
            "Loss: 4.271505e-05, l1: 0.99868, l2: 0.00323\n",
            "Loss: 4.269446e-05, l1: 0.99872, l2: 0.00323\n",
            "Loss: 4.268632e-05, l1: 0.99868, l2: 0.00323\n",
            "Loss: 4.267542e-05, l1: 0.99863, l2: 0.00323\n",
            "Loss: 4.266797e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.265528e-05, l1: 0.99857, l2: 0.00323\n",
            "Loss: 4.264495e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.263493e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.262526e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.261660e-05, l1: 0.99856, l2: 0.00324\n",
            "Loss: 4.261226e-05, l1: 0.99855, l2: 0.00324\n",
            "Loss: 4.260492e-05, l1: 0.99852, l2: 0.00324\n",
            "Loss: 4.259807e-05, l1: 0.99851, l2: 0.00324\n",
            "Loss: 4.258980e-05, l1: 0.99850, l2: 0.00324\n",
            "Loss: 4.258122e-05, l1: 0.99854, l2: 0.00324\n",
            "Loss: 4.259252e-05, l1: 0.99839, l2: 0.00323\n",
            "Loss: 4.257556e-05, l1: 0.99848, l2: 0.00324\n",
            "Loss: 4.256893e-05, l1: 0.99849, l2: 0.00323\n",
            "Loss: 4.256170e-05, l1: 0.99849, l2: 0.00323\n",
            "Loss: 4.255146e-05, l1: 0.99845, l2: 0.00323\n",
            "Loss: 4.253942e-05, l1: 0.99838, l2: 0.00323\n",
            "Loss: 4.253920e-05, l1: 0.99827, l2: 0.00323\n",
            "Loss: 4.253467e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.252644e-05, l1: 0.99828, l2: 0.00323\n",
            "Loss: 4.251920e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.251120e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.249841e-05, l1: 0.99820, l2: 0.00324\n",
            "Loss: 4.263820e-05, l1: 0.99798, l2: 0.00323\n",
            "Loss: 4.249602e-05, l1: 0.99818, l2: 0.00324\n",
            "Loss: 4.248764e-05, l1: 0.99819, l2: 0.00324\n",
            "Loss: 4.247702e-05, l1: 0.99823, l2: 0.00324\n",
            "Loss: 4.246659e-05, l1: 0.99828, l2: 0.00324\n",
            "Loss: 4.245845e-05, l1: 0.99830, l2: 0.00324\n",
            "Loss: 4.245219e-05, l1: 0.99833, l2: 0.00323\n",
            "Loss: 4.243877e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.243159e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.242396e-05, l1: 0.99834, l2: 0.00323\n",
            "Loss: 4.241739e-05, l1: 0.99835, l2: 0.00324\n",
            "Loss: 4.240832e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.239973e-05, l1: 0.99833, l2: 0.00323\n",
            "Loss: 4.238822e-05, l1: 0.99831, l2: 0.00323\n",
            "Loss: 4.238332e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.237595e-05, l1: 0.99829, l2: 0.00323\n",
            "Loss: 4.237096e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.236421e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.235581e-05, l1: 0.99833, l2: 0.00323\n",
            "Loss: 4.235081e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.234626e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.234236e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.233737e-05, l1: 0.99828, l2: 0.00323\n",
            "Loss: 4.233423e-05, l1: 0.99826, l2: 0.00323\n",
            "Loss: 4.232834e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.232567e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.232380e-05, l1: 0.99824, l2: 0.00323\n",
            "Loss: 4.232127e-05, l1: 0.99824, l2: 0.00323\n",
            "Loss: 4.231403e-05, l1: 0.99824, l2: 0.00323\n",
            "Loss: 4.232240e-05, l1: 0.99828, l2: 0.00323\n",
            "Loss: 4.230968e-05, l1: 0.99826, l2: 0.00323\n",
            "Loss: 4.230160e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.228824e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.227294e-05, l1: 0.99821, l2: 0.00323\n",
            "Loss: 4.226894e-05, l1: 0.99820, l2: 0.00323\n",
            "Loss: 4.225990e-05, l1: 0.99819, l2: 0.00323\n",
            "Loss: 4.225588e-05, l1: 0.99820, l2: 0.00324\n",
            "Loss: 4.225190e-05, l1: 0.99821, l2: 0.00324\n",
            "Loss: 4.224706e-05, l1: 0.99824, l2: 0.00324\n",
            "Loss: 4.224493e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.223832e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.223525e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.223056e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.222704e-05, l1: 0.99824, l2: 0.00323\n",
            "Loss: 4.221729e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.223817e-05, l1: 0.99838, l2: 0.00323\n",
            "Loss: 4.221447e-05, l1: 0.99829, l2: 0.00323\n",
            "Loss: 4.220921e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.220509e-05, l1: 0.99831, l2: 0.00323\n",
            "Loss: 4.219963e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.219222e-05, l1: 0.99833, l2: 0.00323\n",
            "Loss: 4.219228e-05, l1: 0.99835, l2: 0.00323\n",
            "Loss: 4.218897e-05, l1: 0.99834, l2: 0.00323\n",
            "Loss: 4.218470e-05, l1: 0.99834, l2: 0.00323\n",
            "Loss: 4.218110e-05, l1: 0.99833, l2: 0.00323\n",
            "Loss: 4.217679e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.216996e-05, l1: 0.99828, l2: 0.00323\n",
            "Loss: 4.216420e-05, l1: 0.99828, l2: 0.00323\n",
            "Loss: 4.215891e-05, l1: 0.99827, l2: 0.00323\n",
            "Loss: 4.214760e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.213759e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.212334e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.211497e-05, l1: 0.99815, l2: 0.00323\n",
            "Loss: 4.210541e-05, l1: 0.99816, l2: 0.00323\n",
            "Loss: 4.209917e-05, l1: 0.99818, l2: 0.00323\n",
            "Loss: 4.209344e-05, l1: 0.99821, l2: 0.00323\n",
            "Loss: 4.208459e-05, l1: 0.99825, l2: 0.00323\n",
            "Loss: 4.207433e-05, l1: 0.99830, l2: 0.00323\n",
            "Loss: 4.205895e-05, l1: 0.99834, l2: 0.00323\n",
            "Loss: 4.208803e-05, l1: 0.99853, l2: 0.00324\n",
            "Loss: 4.204940e-05, l1: 0.99841, l2: 0.00323\n",
            "Loss: 4.203117e-05, l1: 0.99840, l2: 0.00323\n",
            "Loss: 4.201345e-05, l1: 0.99837, l2: 0.00323\n",
            "Loss: 4.199923e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.198038e-05, l1: 0.99837, l2: 0.00324\n",
            "Loss: 4.199484e-05, l1: 0.99836, l2: 0.00323\n",
            "Loss: 4.197174e-05, l1: 0.99837, l2: 0.00324\n",
            "Loss: 4.195305e-05, l1: 0.99841, l2: 0.00324\n",
            "Loss: 4.193534e-05, l1: 0.99847, l2: 0.00324\n",
            "Loss: 4.192584e-05, l1: 0.99850, l2: 0.00323\n",
            "Loss: 4.191089e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.188216e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.216714e-05, l1: 0.99850, l2: 0.00323\n",
            "Loss: 4.187808e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.185972e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.184489e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.181799e-05, l1: 0.99850, l2: 0.00323\n",
            "Loss: 4.187648e-05, l1: 0.99843, l2: 0.00323\n",
            "Loss: 4.181087e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.179596e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.178147e-05, l1: 0.99849, l2: 0.00323\n",
            "Loss: 4.176695e-05, l1: 0.99852, l2: 0.00323\n",
            "Loss: 4.175495e-05, l1: 0.99855, l2: 0.00323\n",
            "Loss: 4.174666e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.176264e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.174267e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.173484e-05, l1: 0.99864, l2: 0.00323\n",
            "Loss: 4.172895e-05, l1: 0.99865, l2: 0.00323\n",
            "Loss: 4.172368e-05, l1: 0.99866, l2: 0.00323\n",
            "Loss: 4.171814e-05, l1: 0.99864, l2: 0.00323\n",
            "Loss: 4.171081e-05, l1: 0.99863, l2: 0.00323\n",
            "Loss: 4.170432e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.170209e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.169725e-05, l1: 0.99863, l2: 0.00323\n",
            "Loss: 4.169466e-05, l1: 0.99859, l2: 0.00323\n",
            "Loss: 4.168845e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.168344e-05, l1: 0.99863, l2: 0.00323\n",
            "Loss: 4.167930e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.167282e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.166566e-05, l1: 0.99857, l2: 0.00323\n",
            "Loss: 4.165983e-05, l1: 0.99855, l2: 0.00323\n",
            "Loss: 4.165251e-05, l1: 0.99855, l2: 0.00323\n",
            "Loss: 4.164164e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.163995e-05, l1: 0.99859, l2: 0.00323\n",
            "Loss: 4.163439e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.163108e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.162117e-05, l1: 0.99857, l2: 0.00323\n",
            "Loss: 4.161305e-05, l1: 0.99855, l2: 0.00323\n",
            "Loss: 4.160428e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.159520e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.158721e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.157858e-05, l1: 0.99856, l2: 0.00323\n",
            "Loss: 4.156701e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.155716e-05, l1: 0.99851, l2: 0.00323\n",
            "Loss: 4.155849e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.154879e-05, l1: 0.99855, l2: 0.00323\n",
            "Loss: 4.154100e-05, l1: 0.99850, l2: 0.00323\n",
            "Loss: 4.153642e-05, l1: 0.99851, l2: 0.00323\n",
            "Loss: 4.152800e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.151677e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.150764e-05, l1: 0.99864, l2: 0.00323\n",
            "Loss: 4.150465e-05, l1: 0.99868, l2: 0.00323\n",
            "Loss: 4.149655e-05, l1: 0.99867, l2: 0.00323\n",
            "Loss: 4.149220e-05, l1: 0.99864, l2: 0.00323\n",
            "Loss: 4.148744e-05, l1: 0.99862, l2: 0.00323\n",
            "Loss: 4.147936e-05, l1: 0.99861, l2: 0.00323\n",
            "Loss: 4.146764e-05, l1: 0.99860, l2: 0.00323\n",
            "Loss: 4.146001e-05, l1: 0.99861, l2: 0.00323\n",
            "Loss: 4.145121e-05, l1: 0.99859, l2: 0.00323\n",
            "Loss: 4.144595e-05, l1: 0.99858, l2: 0.00323\n",
            "Loss: 4.143926e-05, l1: 0.99857, l2: 0.00323\n",
            "Loss: 4.143285e-05, l1: 0.99854, l2: 0.00323\n",
            "Loss: 4.142578e-05, l1: 0.99851, l2: 0.00323\n",
            "Loss: 4.142176e-05, l1: 0.99845, l2: 0.00323\n",
            "Loss: 4.141397e-05, l1: 0.99847, l2: 0.00323\n",
            "Loss: 4.140814e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.140340e-05, l1: 0.99848, l2: 0.00323\n",
            "Loss: 4.139652e-05, l1: 0.99847, l2: 0.00323\n",
            "Loss: 4.138795e-05, l1: 0.99841, l2: 0.00323\n",
            "Loss: 4.137903e-05, l1: 0.99840, l2: 0.00323\n",
            "Loss: 4.137310e-05, l1: 0.99839, l2: 0.00323\n",
            "Loss: 4.136596e-05, l1: 0.99838, l2: 0.00323\n",
            "Loss: 4.136000e-05, l1: 0.99837, l2: 0.00323\n",
            "Loss: 4.134906e-05, l1: 0.99835, l2: 0.00323\n",
            "Loss: 4.133807e-05, l1: 0.99832, l2: 0.00323\n",
            "Loss: 4.132792e-05, l1: 0.99826, l2: 0.00323\n",
            "Loss: 4.131179e-05, l1: 0.99822, l2: 0.00323\n",
            "Loss: 4.129264e-05, l1: 0.99823, l2: 0.00323\n",
            "Loss: 4.127920e-05, l1: 0.99821, l2: 0.00323\n",
            "Loss: 4.126744e-05, l1: 0.99827, l2: 0.00323\n",
            "Loss: 4.123800e-05, l1: 0.99817, l2: 0.00323\n",
            "Loss: 4.122651e-05, l1: 0.99815, l2: 0.00323\n",
            "Loss: 4.121774e-05, l1: 0.99812, l2: 0.00323\n",
            "Loss: 4.119778e-05, l1: 0.99806, l2: 0.00323\n",
            "Loss: 4.118649e-05, l1: 0.99806, l2: 0.00323\n",
            "Loss: 4.117714e-05, l1: 0.99805, l2: 0.00322\n",
            "Loss: 4.116895e-05, l1: 0.99805, l2: 0.00322\n",
            "Loss: 4.116516e-05, l1: 0.99803, l2: 0.00322\n",
            "Loss: 4.116117e-05, l1: 0.99802, l2: 0.00322\n",
            "Loss: 4.115378e-05, l1: 0.99800, l2: 0.00322\n",
            "Loss: 4.114973e-05, l1: 0.99801, l2: 0.00322\n",
            "Loss: 4.114585e-05, l1: 0.99802, l2: 0.00322\n",
            "Loss: 4.114144e-05, l1: 0.99804, l2: 0.00322\n",
            "Loss: 4.113452e-05, l1: 0.99805, l2: 0.00322\n",
            "Loss: 4.112397e-05, l1: 0.99804, l2: 0.00322\n",
            "Loss: 4.111070e-05, l1: 0.99799, l2: 0.00322\n",
            "Loss: 4.110135e-05, l1: 0.99795, l2: 0.00322\n",
            "Loss: 4.109471e-05, l1: 0.99791, l2: 0.00322\n",
            "Loss: 4.108989e-05, l1: 0.99790, l2: 0.00322\n",
            "Loss: 4.108216e-05, l1: 0.99791, l2: 0.00322\n",
            "Loss: 4.107278e-05, l1: 0.99794, l2: 0.00322\n",
            "Loss: 4.105842e-05, l1: 0.99800, l2: 0.00322\n",
            "Loss: 4.104264e-05, l1: 0.99808, l2: 0.00322\n",
            "Loss: 4.103340e-05, l1: 0.99812, l2: 0.00322\n",
            "Loss: 4.102907e-05, l1: 0.99813, l2: 0.00322\n",
            "Loss: 4.102643e-05, l1: 0.99812, l2: 0.00322\n",
            "Loss: 4.102345e-05, l1: 0.99812, l2: 0.00322\n",
            "Loss: 4.101916e-05, l1: 0.99812, l2: 0.00322\n",
            "Loss: 4.101648e-05, l1: 0.99813, l2: 0.00322\n",
            "Loss: 4.101524e-05, l1: 0.99815, l2: 0.00322\n",
            "Loss: 4.100924e-05, l1: 0.99817, l2: 0.00322\n",
            "Loss: 4.100603e-05, l1: 0.99818, l2: 0.00322\n",
            "Loss: 4.100058e-05, l1: 0.99822, l2: 0.00322\n",
            "Loss: 4.099498e-05, l1: 0.99823, l2: 0.00322\n",
            "Loss: 4.098902e-05, l1: 0.99830, l2: 0.00322\n",
            "Loss: 4.098961e-05, l1: 0.99825, l2: 0.00322\n",
            "Loss: 4.098644e-05, l1: 0.99828, l2: 0.00322\n",
            "Loss: 4.098361e-05, l1: 0.99829, l2: 0.00322\n",
            "Loss: 4.098000e-05, l1: 0.99829, l2: 0.00322\n",
            "Loss: 4.097783e-05, l1: 0.99830, l2: 0.00322\n",
            "Loss: 4.097490e-05, l1: 0.99831, l2: 0.00322\n",
            "Loss: 4.097050e-05, l1: 0.99833, l2: 0.00322\n",
            "Loss: 4.098096e-05, l1: 0.99837, l2: 0.00322\n",
            "Loss: 4.096858e-05, l1: 0.99834, l2: 0.00322\n",
            "Loss: 4.096356e-05, l1: 0.99837, l2: 0.00322\n",
            "Loss: 4.095802e-05, l1: 0.99839, l2: 0.00322\n",
            "Loss: 4.095154e-05, l1: 0.99841, l2: 0.00322\n",
            "Loss: 4.094596e-05, l1: 0.99841, l2: 0.00322\n",
            "Loss: 4.093994e-05, l1: 0.99840, l2: 0.00322\n",
            "Loss: 4.093635e-05, l1: 0.99841, l2: 0.00322\n",
            "Loss: 4.093093e-05, l1: 0.99842, l2: 0.00322\n",
            "Loss: 4.092415e-05, l1: 0.99843, l2: 0.00322\n",
            "Loss: 4.091385e-05, l1: 0.99844, l2: 0.00322\n",
            "Loss: 4.091855e-05, l1: 0.99851, l2: 0.00322\n",
            "Loss: 4.090878e-05, l1: 0.99847, l2: 0.00322\n",
            "Loss: 4.090312e-05, l1: 0.99847, l2: 0.00322\n",
            "Loss: 4.088868e-05, l1: 0.99846, l2: 0.00322\n",
            "Loss: 4.087919e-05, l1: 0.99845, l2: 0.00322\n",
            "Loss: 4.087034e-05, l1: 0.99845, l2: 0.00322\n",
            "Loss: 4.086360e-05, l1: 0.99845, l2: 0.00322\n",
            "Loss: 4.086017e-05, l1: 0.99846, l2: 0.00322\n",
            "Loss: 4.085572e-05, l1: 0.99847, l2: 0.00322\n",
            "Loss: 4.085265e-05, l1: 0.99848, l2: 0.00322\n",
            "Loss: 4.084958e-05, l1: 0.99849, l2: 0.00322\n",
            "Loss: 4.084656e-05, l1: 0.99852, l2: 0.00322\n",
            "Loss: 4.084178e-05, l1: 0.99855, l2: 0.00322\n",
            "Loss: 4.083648e-05, l1: 0.99858, l2: 0.00322\n",
            "Loss: 4.083599e-05, l1: 0.99861, l2: 0.00322\n",
            "Loss: 4.083269e-05, l1: 0.99860, l2: 0.00322\n",
            "Loss: 4.082834e-05, l1: 0.99860, l2: 0.00322\n",
            "Loss: 4.082385e-05, l1: 0.99859, l2: 0.00322\n",
            "Loss: 4.082014e-05, l1: 0.99859, l2: 0.00322\n",
            "Loss: 4.081303e-05, l1: 0.99858, l2: 0.00322\n",
            "Loss: 4.080825e-05, l1: 0.99857, l2: 0.00322\n",
            "Loss: 4.080000e-05, l1: 0.99858, l2: 0.00322\n",
            "Loss: 4.079669e-05, l1: 0.99859, l2: 0.00322\n",
            "Loss: 4.079215e-05, l1: 0.99860, l2: 0.00322\n",
            "Loss: 4.078790e-05, l1: 0.99862, l2: 0.00322\n",
            "Loss: 4.078303e-05, l1: 0.99863, l2: 0.00322\n",
            "Loss: 4.077550e-05, l1: 0.99863, l2: 0.00322\n",
            "Loss: 4.076953e-05, l1: 0.99866, l2: 0.00322\n",
            "Loss: 4.076320e-05, l1: 0.99867, l2: 0.00322\n",
            "Loss: 4.075389e-05, l1: 0.99872, l2: 0.00322\n",
            "Loss: 4.075358e-05, l1: 0.99881, l2: 0.00321\n",
            "Loss: 4.074926e-05, l1: 0.99877, l2: 0.00322\n",
            "Loss: 4.074393e-05, l1: 0.99880, l2: 0.00322\n",
            "Loss: 4.074033e-05, l1: 0.99882, l2: 0.00322\n",
            "Loss: 4.073759e-05, l1: 0.99885, l2: 0.00322\n",
            "Loss: 4.073427e-05, l1: 0.99887, l2: 0.00322\n",
            "Loss: 4.072956e-05, l1: 0.99889, l2: 0.00322\n",
            "Loss: 4.072617e-05, l1: 0.99890, l2: 0.00322\n",
            "Loss: 4.072234e-05, l1: 0.99889, l2: 0.00322\n",
            "Loss: 4.071774e-05, l1: 0.99889, l2: 0.00322\n",
            "Loss: 4.071333e-05, l1: 0.99886, l2: 0.00322\n",
            "Loss: 4.071448e-05, l1: 0.99882, l2: 0.00322\n",
            "Loss: 4.070925e-05, l1: 0.99885, l2: 0.00322\n",
            "Loss: 4.070489e-05, l1: 0.99883, l2: 0.00322\n",
            "Loss: 4.069831e-05, l1: 0.99880, l2: 0.00322\n",
            "Loss: 4.069290e-05, l1: 0.99877, l2: 0.00321\n",
            "Loss: 4.068634e-05, l1: 0.99873, l2: 0.00321\n",
            "Loss: 4.068093e-05, l1: 0.99871, l2: 0.00321\n",
            "Loss: 4.067634e-05, l1: 0.99870, l2: 0.00321\n",
            "Loss: 4.067246e-05, l1: 0.99870, l2: 0.00321\n",
            "Loss: 4.066678e-05, l1: 0.99870, l2: 0.00321\n",
            "Loss: 4.066477e-05, l1: 0.99873, l2: 0.00321\n",
            "Loss: 4.065772e-05, l1: 0.99872, l2: 0.00321\n",
            "Loss: 4.065468e-05, l1: 0.99872, l2: 0.00321\n",
            "Loss: 4.064907e-05, l1: 0.99873, l2: 0.00321\n",
            "Loss: 4.064370e-05, l1: 0.99874, l2: 0.00321\n",
            "Loss: 4.064724e-05, l1: 0.99877, l2: 0.00321\n",
            "Loss: 4.064022e-05, l1: 0.99876, l2: 0.00321\n",
            "Loss: 4.063667e-05, l1: 0.99877, l2: 0.00321\n",
            "Loss: 4.063264e-05, l1: 0.99878, l2: 0.00321\n",
            "Loss: 4.062921e-05, l1: 0.99878, l2: 0.00321\n",
            "Loss: 4.067728e-05, l1: 0.99880, l2: 0.00321\n",
            "Loss: 4.062742e-05, l1: 0.99879, l2: 0.00321\n",
            "Loss: 4.062296e-05, l1: 0.99879, l2: 0.00321\n",
            "Loss: 4.061988e-05, l1: 0.99880, l2: 0.00321\n",
            "Loss: 4.061804e-05, l1: 0.99881, l2: 0.00321\n",
            "Loss: 4.061629e-05, l1: 0.99881, l2: 0.00321\n",
            "Loss: 4.061384e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.061013e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.060461e-05, l1: 0.99884, l2: 0.00321\n",
            "Loss: 4.059795e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.059311e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.058976e-05, l1: 0.99882, l2: 0.00321\n",
            "Loss: 4.058747e-05, l1: 0.99882, l2: 0.00321\n",
            "Loss: 4.058271e-05, l1: 0.99882, l2: 0.00321\n",
            "Loss: 4.057876e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.057180e-05, l1: 0.99884, l2: 0.00321\n",
            "Loss: 4.056478e-05, l1: 0.99884, l2: 0.00321\n",
            "Loss: 4.059390e-05, l1: 0.99886, l2: 0.00321\n",
            "Loss: 4.056299e-05, l1: 0.99885, l2: 0.00321\n",
            "Loss: 4.055688e-05, l1: 0.99883, l2: 0.00321\n",
            "Loss: 4.055389e-05, l1: 0.99882, l2: 0.00321\n",
            "Loss: 4.055077e-05, l1: 0.99879, l2: 0.00321\n",
            "Loss: 4.054680e-05, l1: 0.99876, l2: 0.00321\n",
            "Loss: 4.054060e-05, l1: 0.99871, l2: 0.00321\n",
            "Loss: 4.053714e-05, l1: 0.99866, l2: 0.00321\n",
            "Loss: 4.053088e-05, l1: 0.99867, l2: 0.00321\n",
            "Loss: 4.052699e-05, l1: 0.99868, l2: 0.00321\n",
            "Loss: 4.052406e-05, l1: 0.99869, l2: 0.00321\n",
            "Loss: 4.051904e-05, l1: 0.99867, l2: 0.00321\n",
            "Loss: 4.053561e-05, l1: 0.99868, l2: 0.00321\n",
            "Loss: 4.051570e-05, l1: 0.99867, l2: 0.00321\n",
            "Loss: 4.051039e-05, l1: 0.99863, l2: 0.00321\n",
            "Loss: 4.050326e-05, l1: 0.99857, l2: 0.00321\n",
            "Loss: 4.051011e-05, l1: 0.99846, l2: 0.00321\n",
            "Loss: 4.049859e-05, l1: 0.99853, l2: 0.00321\n",
            "Loss: 4.049233e-05, l1: 0.99849, l2: 0.00321\n",
            "Loss: 4.048435e-05, l1: 0.99845, l2: 0.00321\n",
            "Loss: 4.048055e-05, l1: 0.99843, l2: 0.00321\n",
            "Loss: 4.047743e-05, l1: 0.99842, l2: 0.00321\n",
            "Loss: 4.047502e-05, l1: 0.99842, l2: 0.00321\n",
            "Loss: 4.047250e-05, l1: 0.99842, l2: 0.00321\n",
            "Loss: 4.046600e-05, l1: 0.99843, l2: 0.00321\n",
            "Loss: 4.046554e-05, l1: 0.99847, l2: 0.00321\n",
            "Loss: 4.046405e-05, l1: 0.99845, l2: 0.00321\n",
            "Loss: 4.046028e-05, l1: 0.99844, l2: 0.00321\n",
            "Loss: 4.045715e-05, l1: 0.99843, l2: 0.00321\n",
            "Loss: 4.045217e-05, l1: 0.99841, l2: 0.00321\n",
            "Loss: 4.044619e-05, l1: 0.99835, l2: 0.00321\n",
            "Loss: 4.044708e-05, l1: 0.99826, l2: 0.00321\n",
            "Loss: 4.044297e-05, l1: 0.99831, l2: 0.00321\n",
            "Loss: 4.043727e-05, l1: 0.99826, l2: 0.00321\n",
            "Loss: 4.043078e-05, l1: 0.99821, l2: 0.00321\n",
            "Loss: 4.042512e-05, l1: 0.99817, l2: 0.00321\n",
            "Loss: 4.041719e-05, l1: 0.99813, l2: 0.00321\n",
            "Loss: 4.043105e-05, l1: 0.99808, l2: 0.00321\n",
            "Loss: 4.041403e-05, l1: 0.99812, l2: 0.00321\n",
            "Loss: 4.040628e-05, l1: 0.99809, l2: 0.00321\n",
            "Loss: 4.039887e-05, l1: 0.99808, l2: 0.00321\n",
            "Loss: 4.039259e-05, l1: 0.99807, l2: 0.00321\n",
            "Loss: 4.042088e-05, l1: 0.99807, l2: 0.00321\n",
            "Loss: 4.039182e-05, l1: 0.99807, l2: 0.00321\n",
            "Loss: 4.039023e-05, l1: 0.99807, l2: 0.00321\n",
            "Loss: 4.038720e-05, l1: 0.99808, l2: 0.00321\n",
            "Loss: 4.038347e-05, l1: 0.99803, l2: 0.00321\n",
            "Loss: 4.037953e-05, l1: 0.99802, l2: 0.00321\n",
            "Loss: 4.037686e-05, l1: 0.99800, l2: 0.00321\n",
            "Loss: 4.037364e-05, l1: 0.99798, l2: 0.00321\n",
            "Loss: 4.036647e-05, l1: 0.99793, l2: 0.00321\n",
            "Loss: 4.036164e-05, l1: 0.99790, l2: 0.00321\n",
            "Loss: 4.035613e-05, l1: 0.99787, l2: 0.00321\n",
            "Loss: 4.035361e-05, l1: 0.99784, l2: 0.00321\n",
            "Loss: 4.035032e-05, l1: 0.99786, l2: 0.00321\n",
            "Loss: 4.034812e-05, l1: 0.99786, l2: 0.00321\n",
            "Loss: 4.034496e-05, l1: 0.99785, l2: 0.00321\n",
            "Loss: 4.034044e-05, l1: 0.99781, l2: 0.00321\n",
            "Loss: 4.033354e-05, l1: 0.99778, l2: 0.00321\n",
            "Loss: 4.032884e-05, l1: 0.99773, l2: 0.00321\n",
            "Loss: 4.032607e-05, l1: 0.99772, l2: 0.00321\n",
            "Loss: 4.032347e-05, l1: 0.99771, l2: 0.00321\n",
            "Loss: 4.031849e-05, l1: 0.99771, l2: 0.00321\n",
            "Loss: 4.031348e-05, l1: 0.99769, l2: 0.00321\n",
            "Loss: 4.030648e-05, l1: 0.99766, l2: 0.00321\n",
            "Loss: 4.031044e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.030110e-05, l1: 0.99763, l2: 0.00321\n",
            "Loss: 4.029648e-05, l1: 0.99761, l2: 0.00321\n",
            "Loss: 4.029196e-05, l1: 0.99755, l2: 0.00321\n",
            "Loss: 4.028953e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.028651e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.028270e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.027827e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.027217e-05, l1: 0.99755, l2: 0.00321\n",
            "Loss: 4.027886e-05, l1: 0.99760, l2: 0.00321\n",
            "Loss: 4.026922e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.026282e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.025667e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.025234e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.024860e-05, l1: 0.99755, l2: 0.00321\n",
            "Loss: 4.025379e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.024726e-05, l1: 0.99756, l2: 0.00321\n",
            "Loss: 4.024434e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.023947e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.023610e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.023374e-05, l1: 0.99759, l2: 0.00321\n",
            "Loss: 4.023107e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.022625e-05, l1: 0.99755, l2: 0.00321\n",
            "Loss: 4.022212e-05, l1: 0.99754, l2: 0.00321\n",
            "Loss: 4.021575e-05, l1: 0.99754, l2: 0.00321\n",
            "Loss: 4.020712e-05, l1: 0.99757, l2: 0.00321\n",
            "Loss: 4.020773e-05, l1: 0.99765, l2: 0.00321\n",
            "Loss: 4.020285e-05, l1: 0.99761, l2: 0.00321\n",
            "Loss: 4.019713e-05, l1: 0.99763, l2: 0.00321\n",
            "Loss: 4.019213e-05, l1: 0.99765, l2: 0.00321\n",
            "Loss: 4.018777e-05, l1: 0.99765, l2: 0.00321\n",
            "Loss: 4.018037e-05, l1: 0.99765, l2: 0.00321\n",
            "Loss: 4.017544e-05, l1: 0.99763, l2: 0.00321\n",
            "Loss: 4.017023e-05, l1: 0.99760, l2: 0.00320\n",
            "Loss: 4.016705e-05, l1: 0.99759, l2: 0.00320\n",
            "Loss: 4.016518e-05, l1: 0.99760, l2: 0.00320\n",
            "Loss: 4.016300e-05, l1: 0.99760, l2: 0.00320\n",
            "Loss: 4.016023e-05, l1: 0.99763, l2: 0.00320\n",
            "Loss: 4.015844e-05, l1: 0.99768, l2: 0.00320\n",
            "Loss: 4.015232e-05, l1: 0.99769, l2: 0.00320\n",
            "Loss: 4.014962e-05, l1: 0.99770, l2: 0.00320\n",
            "Loss: 4.014502e-05, l1: 0.99771, l2: 0.00321\n",
            "Loss: 4.013850e-05, l1: 0.99775, l2: 0.00321\n",
            "Loss: 4.012763e-05, l1: 0.99781, l2: 0.00321\n",
            "Loss: 4.011847e-05, l1: 0.99785, l2: 0.00320\n",
            "Loss: 4.011192e-05, l1: 0.99788, l2: 0.00320\n",
            "Loss: 4.010855e-05, l1: 0.99788, l2: 0.00320\n",
            "Loss: 4.010481e-05, l1: 0.99788, l2: 0.00320\n",
            "Loss: 4.009670e-05, l1: 0.99788, l2: 0.00321\n",
            "Loss: 4.008757e-05, l1: 0.99794, l2: 0.00321\n",
            "Loss: 4.007722e-05, l1: 0.99802, l2: 0.00321\n",
            "Loss: 4.006929e-05, l1: 0.99808, l2: 0.00321\n",
            "Loss: 4.006417e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 4.006198e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 4.005887e-05, l1: 0.99818, l2: 0.00320\n",
            "Loss: 4.005504e-05, l1: 0.99819, l2: 0.00320\n",
            "Loss: 4.005228e-05, l1: 0.99820, l2: 0.00320\n",
            "Loss: 4.004709e-05, l1: 0.99822, l2: 0.00320\n",
            "Loss: 4.004558e-05, l1: 0.99825, l2: 0.00320\n",
            "Loss: 4.003858e-05, l1: 0.99826, l2: 0.00320\n",
            "Loss: 4.003547e-05, l1: 0.99826, l2: 0.00320\n",
            "Loss: 4.003173e-05, l1: 0.99829, l2: 0.00320\n",
            "Loss: 4.002703e-05, l1: 0.99830, l2: 0.00320\n",
            "Loss: 4.002023e-05, l1: 0.99833, l2: 0.00320\n",
            "Loss: 4.001181e-05, l1: 0.99839, l2: 0.00320\n",
            "Loss: 4.002377e-05, l1: 0.99837, l2: 0.00320\n",
            "Loss: 4.000977e-05, l1: 0.99838, l2: 0.00320\n",
            "Loss: 4.000488e-05, l1: 0.99839, l2: 0.00320\n",
            "Loss: 4.000093e-05, l1: 0.99839, l2: 0.00320\n",
            "Loss: 3.999697e-05, l1: 0.99838, l2: 0.00320\n",
            "Loss: 3.999231e-05, l1: 0.99839, l2: 0.00320\n",
            "Loss: 3.998577e-05, l1: 0.99840, l2: 0.00320\n",
            "Loss: 3.998094e-05, l1: 0.99842, l2: 0.00320\n",
            "Loss: 3.997619e-05, l1: 0.99845, l2: 0.00320\n",
            "Loss: 3.997290e-05, l1: 0.99852, l2: 0.00320\n",
            "Loss: 3.997001e-05, l1: 0.99850, l2: 0.00320\n",
            "Loss: 3.996845e-05, l1: 0.99850, l2: 0.00320\n",
            "Loss: 3.996473e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.995966e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.995056e-05, l1: 0.99851, l2: 0.00320\n",
            "Loss: 3.994598e-05, l1: 0.99853, l2: 0.00320\n",
            "Loss: 3.993466e-05, l1: 0.99855, l2: 0.00320\n",
            "Loss: 3.992852e-05, l1: 0.99854, l2: 0.00320\n",
            "Loss: 3.992219e-05, l1: 0.99854, l2: 0.00320\n",
            "Loss: 3.992598e-05, l1: 0.99845, l2: 0.00320\n",
            "Loss: 3.991913e-05, l1: 0.99850, l2: 0.00320\n",
            "Loss: 3.991464e-05, l1: 0.99850, l2: 0.00320\n",
            "Loss: 3.991108e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.990880e-05, l1: 0.99851, l2: 0.00320\n",
            "Loss: 3.990635e-05, l1: 0.99852, l2: 0.00320\n",
            "Loss: 3.990055e-05, l1: 0.99856, l2: 0.00320\n",
            "Loss: 3.989726e-05, l1: 0.99859, l2: 0.00320\n",
            "Loss: 3.989263e-05, l1: 0.99863, l2: 0.00320\n",
            "Loss: 3.988973e-05, l1: 0.99870, l2: 0.00320\n",
            "Loss: 3.988542e-05, l1: 0.99866, l2: 0.00320\n",
            "Loss: 3.988305e-05, l1: 0.99863, l2: 0.00320\n",
            "Loss: 3.988173e-05, l1: 0.99861, l2: 0.00320\n",
            "Loss: 3.987902e-05, l1: 0.99859, l2: 0.00320\n",
            "Loss: 3.987633e-05, l1: 0.99860, l2: 0.00320\n",
            "Loss: 3.987288e-05, l1: 0.99861, l2: 0.00320\n",
            "Loss: 3.987072e-05, l1: 0.99865, l2: 0.00320\n",
            "Loss: 3.986684e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.987443e-05, l1: 0.99881, l2: 0.00320\n",
            "Loss: 3.986549e-05, l1: 0.99874, l2: 0.00320\n",
            "Loss: 3.986188e-05, l1: 0.99876, l2: 0.00320\n",
            "Loss: 3.985887e-05, l1: 0.99875, l2: 0.00320\n",
            "Loss: 3.985538e-05, l1: 0.99874, l2: 0.00320\n",
            "Loss: 3.985262e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.984919e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.984517e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.984112e-05, l1: 0.99866, l2: 0.00320\n",
            "Loss: 3.983830e-05, l1: 0.99864, l2: 0.00320\n",
            "Loss: 3.983334e-05, l1: 0.99860, l2: 0.00320\n",
            "Loss: 3.983008e-05, l1: 0.99853, l2: 0.00320\n",
            "Loss: 3.982271e-05, l1: 0.99848, l2: 0.00320\n",
            "Loss: 3.981976e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.981688e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.981540e-05, l1: 0.99849, l2: 0.00320\n",
            "Loss: 3.981311e-05, l1: 0.99848, l2: 0.00320\n",
            "Loss: 3.980979e-05, l1: 0.99847, l2: 0.00320\n",
            "Loss: 3.980681e-05, l1: 0.99845, l2: 0.00320\n",
            "Loss: 3.980440e-05, l1: 0.99843, l2: 0.00320\n",
            "Loss: 3.980172e-05, l1: 0.99843, l2: 0.00320\n",
            "Loss: 3.979835e-05, l1: 0.99841, l2: 0.00320\n",
            "Loss: 3.979504e-05, l1: 0.99839, l2: 0.00320\n",
            "Loss: 3.979009e-05, l1: 0.99833, l2: 0.00320\n",
            "Loss: 3.979619e-05, l1: 0.99825, l2: 0.00320\n",
            "Loss: 3.978755e-05, l1: 0.99830, l2: 0.00320\n",
            "Loss: 3.978195e-05, l1: 0.99823, l2: 0.00320\n",
            "Loss: 3.977786e-05, l1: 0.99819, l2: 0.00320\n",
            "Loss: 3.977471e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.977085e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 3.976663e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.976250e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.975660e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.974778e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.973828e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.973178e-05, l1: 0.99818, l2: 0.00320\n",
            "Loss: 3.972723e-05, l1: 0.99818, l2: 0.00320\n",
            "Loss: 3.972410e-05, l1: 0.99817, l2: 0.00320\n",
            "Loss: 3.972074e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 3.971626e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.971136e-05, l1: 0.99813, l2: 0.00320\n",
            "Loss: 3.970693e-05, l1: 0.99813, l2: 0.00320\n",
            "Loss: 3.970126e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.969515e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 3.968977e-05, l1: 0.99813, l2: 0.00320\n",
            "Loss: 3.968832e-05, l1: 0.99818, l2: 0.00320\n",
            "Loss: 3.968161e-05, l1: 0.99810, l2: 0.00320\n",
            "Loss: 3.967950e-05, l1: 0.99810, l2: 0.00320\n",
            "Loss: 3.967611e-05, l1: 0.99811, l2: 0.00320\n",
            "Loss: 3.967283e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.967212e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 3.967003e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.966871e-05, l1: 0.99816, l2: 0.00320\n",
            "Loss: 3.966601e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.966238e-05, l1: 0.99815, l2: 0.00320\n",
            "Loss: 3.965853e-05, l1: 0.99812, l2: 0.00320\n",
            "Loss: 3.965592e-05, l1: 0.99812, l2: 0.00320\n",
            "Loss: 3.965195e-05, l1: 0.99813, l2: 0.00320\n",
            "Loss: 3.965012e-05, l1: 0.99812, l2: 0.00320\n",
            "Loss: 3.964694e-05, l1: 0.99810, l2: 0.00320\n",
            "Loss: 3.964292e-05, l1: 0.99806, l2: 0.00320\n",
            "Loss: 3.963753e-05, l1: 0.99802, l2: 0.00320\n",
            "Loss: 3.963881e-05, l1: 0.99797, l2: 0.00320\n",
            "Loss: 3.963352e-05, l1: 0.99799, l2: 0.00320\n",
            "Loss: 3.962666e-05, l1: 0.99796, l2: 0.00320\n",
            "Loss: 3.963662e-05, l1: 0.99798, l2: 0.00320\n",
            "Loss: 3.962463e-05, l1: 0.99797, l2: 0.00320\n",
            "Loss: 3.961996e-05, l1: 0.99797, l2: 0.00320\n",
            "Loss: 3.961745e-05, l1: 0.99799, l2: 0.00320\n",
            "Loss: 3.961337e-05, l1: 0.99803, l2: 0.00320\n",
            "Loss: 3.960828e-05, l1: 0.99807, l2: 0.00320\n",
            "Loss: 3.960745e-05, l1: 0.99814, l2: 0.00320\n",
            "Loss: 3.960091e-05, l1: 0.99813, l2: 0.00320\n",
            "Loss: 3.959867e-05, l1: 0.99809, l2: 0.00320\n",
            "Loss: 3.959554e-05, l1: 0.99807, l2: 0.00320\n",
            "Loss: 3.958981e-05, l1: 0.99806, l2: 0.00320\n",
            "Loss: 3.958267e-05, l1: 0.99801, l2: 0.00320\n",
            "Loss: 3.957744e-05, l1: 0.99803, l2: 0.00320\n",
            "Loss: 3.957115e-05, l1: 0.99802, l2: 0.00320\n",
            "Loss: 3.956484e-05, l1: 0.99798, l2: 0.00320\n",
            "Loss: 3.956007e-05, l1: 0.99795, l2: 0.00320\n",
            "Loss: 3.955468e-05, l1: 0.99787, l2: 0.00320\n",
            "Loss: 3.955304e-05, l1: 0.99788, l2: 0.00319\n",
            "Loss: 3.954575e-05, l1: 0.99788, l2: 0.00319\n",
            "Loss: 3.954133e-05, l1: 0.99789, l2: 0.00319\n",
            "Loss: 3.953711e-05, l1: 0.99791, l2: 0.00319\n",
            "Loss: 3.953227e-05, l1: 0.99794, l2: 0.00319\n",
            "Loss: 3.952845e-05, l1: 0.99795, l2: 0.00319\n",
            "Loss: 3.952575e-05, l1: 0.99795, l2: 0.00319\n",
            "Loss: 3.952242e-05, l1: 0.99795, l2: 0.00319\n",
            "Loss: 3.951971e-05, l1: 0.99795, l2: 0.00319\n",
            "Loss: 3.951528e-05, l1: 0.99798, l2: 0.00319\n",
            "Loss: 3.950822e-05, l1: 0.99800, l2: 0.00319\n",
            "Loss: 3.950425e-05, l1: 0.99807, l2: 0.00319\n",
            "Loss: 3.949790e-05, l1: 0.99806, l2: 0.00319\n",
            "Loss: 3.949489e-05, l1: 0.99805, l2: 0.00319\n",
            "Loss: 3.949273e-05, l1: 0.99805, l2: 0.00319\n",
            "Loss: 3.948914e-05, l1: 0.99805, l2: 0.00319\n",
            "Loss: 3.948464e-05, l1: 0.99808, l2: 0.00319\n",
            "Loss: 3.953297e-05, l1: 0.99793, l2: 0.00319\n",
            "Loss: 3.948345e-05, l1: 0.99806, l2: 0.00319\n",
            "Loss: 3.947995e-05, l1: 0.99809, l2: 0.00319\n",
            "Loss: 3.947689e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.947388e-05, l1: 0.99814, l2: 0.00319\n",
            "Loss: 3.947002e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.946545e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.946018e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.948800e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.945816e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.945300e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.944699e-05, l1: 0.99822, l2: 0.00319\n",
            "Loss: 3.944253e-05, l1: 0.99825, l2: 0.00319\n",
            "Loss: 3.943752e-05, l1: 0.99829, l2: 0.00319\n",
            "Loss: 3.943076e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.944946e-05, l1: 0.99864, l2: 0.00319\n",
            "Loss: 3.942704e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.942240e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.941497e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.941136e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.940521e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.940254e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.939813e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.939825e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.939622e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.939420e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.939040e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.938789e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.938135e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.938096e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.937783e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.937282e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.936909e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.938260e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.936793e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.936483e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.935866e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.935274e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.934916e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.934532e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.934234e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.933821e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.933952e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.933639e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.933354e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.932959e-05, l1: 0.99824, l2: 0.00319\n",
            "Loss: 3.932611e-05, l1: 0.99823, l2: 0.00319\n",
            "Loss: 3.932152e-05, l1: 0.99823, l2: 0.00319\n",
            "Loss: 3.931529e-05, l1: 0.99827, l2: 0.00319\n",
            "Loss: 3.931432e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.930492e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.930177e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.929761e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.929485e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.928934e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.928261e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.927420e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.927155e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.926633e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.925883e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.925178e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.924307e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.923520e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.923483e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.923257e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.923014e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.922777e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.922660e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.922440e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.922162e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.921606e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.922357e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.921300e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.920528e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.920040e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.919452e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.918812e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.918208e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.918361e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.917838e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.917258e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.916839e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.916402e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.916508e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.916175e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.915900e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.915548e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.915315e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.915106e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.914855e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.914689e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.914606e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.914280e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.914016e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.913628e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.913291e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.912805e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.912129e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.911472e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.910829e-05, l1: 0.99818, l2: 0.00318\n",
            "Loss: 3.911254e-05, l1: 0.99816, l2: 0.00318\n",
            "Loss: 3.910673e-05, l1: 0.99817, l2: 0.00318\n",
            "Loss: 3.910392e-05, l1: 0.99815, l2: 0.00318\n",
            "Loss: 3.909314e-05, l1: 0.99811, l2: 0.00318\n",
            "Loss: 3.908755e-05, l1: 0.99810, l2: 0.00318\n",
            "Loss: 3.908373e-05, l1: 0.99814, l2: 0.00318\n",
            "Loss: 3.907928e-05, l1: 0.99818, l2: 0.00318\n",
            "Loss: 3.907399e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.906788e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.906248e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.905895e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.905662e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.905444e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.905167e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.904604e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.904077e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.903656e-05, l1: 0.99821, l2: 0.00318\n",
            "Loss: 3.903121e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.902947e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.902791e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.902681e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.902435e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.902226e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.902061e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.901906e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.901754e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.901342e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.901552e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.901114e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.900813e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.900627e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.900327e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.900012e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.899566e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.898978e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.898382e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.897992e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.897282e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.898015e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.896946e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.896350e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.896020e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.895603e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.895243e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.895603e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.894985e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.894625e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.894303e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.894095e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.893666e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.894906e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.893558e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.893236e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.892976e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.892797e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.892607e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.892380e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.892084e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.891660e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.891303e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.891007e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.890775e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.890457e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.890470e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.890271e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.889954e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.889812e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.889546e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.889321e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.888847e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.888542e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.889029e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.888317e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.888078e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.887852e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.887705e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.887216e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.886727e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.886388e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.886059e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.885778e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.886437e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.885617e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.885316e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.884653e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.884187e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.883451e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.883132e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.882636e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.882461e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.882262e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.881859e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.881700e-05, l1: 0.99821, l2: 0.00318\n",
            "Loss: 3.881311e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.881112e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.880915e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.880337e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.880015e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.879617e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.879385e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.879247e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.879010e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.878798e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.878409e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.877945e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.877554e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.877708e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.877465e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.877233e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.877027e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.876816e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.876337e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.875975e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.875388e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.874749e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.874504e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.874314e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.874012e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.873817e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.873512e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.873835e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.873282e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.872955e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.872539e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.872329e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.872150e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.871952e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.871775e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.871530e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.871307e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.871063e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.870650e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.870254e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.871408e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.870110e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.869660e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.869356e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.869069e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.868897e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.868649e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.868409e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.868070e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.867719e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.867589e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.867438e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.867377e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.867314e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.867207e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.867020e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.866796e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.866572e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.866356e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.866440e-05, l1: 0.99822, l2: 0.00318\n",
            "Loss: 3.866258e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.866095e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.865747e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.865441e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.865062e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.864912e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.864436e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.864134e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.863881e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.870099e-05, l1: 0.99821, l2: 0.00318\n",
            "Loss: 3.863819e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.863626e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.863425e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.863013e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.862585e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.861956e-05, l1: 0.99822, l2: 0.00318\n",
            "Loss: 3.861339e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.860810e-05, l1: 0.99824, l2: 0.00318\n",
            "Loss: 3.860279e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.863342e-05, l1: 0.99816, l2: 0.00318\n",
            "Loss: 3.860123e-05, l1: 0.99822, l2: 0.00318\n",
            "Loss: 3.859805e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.859304e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.859086e-05, l1: 0.99823, l2: 0.00318\n",
            "Loss: 3.858696e-05, l1: 0.99822, l2: 0.00318\n",
            "Loss: 3.858474e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.858076e-05, l1: 0.99821, l2: 0.00318\n",
            "Loss: 3.857928e-05, l1: 0.99820, l2: 0.00318\n",
            "Loss: 3.857802e-05, l1: 0.99820, l2: 0.00318\n",
            "Loss: 3.857708e-05, l1: 0.99820, l2: 0.00318\n",
            "Loss: 3.857562e-05, l1: 0.99820, l2: 0.00318\n",
            "Loss: 3.857383e-05, l1: 0.99820, l2: 0.00318\n",
            "Loss: 3.857244e-05, l1: 0.99821, l2: 0.00318\n",
            "Loss: 3.856941e-05, l1: 0.99822, l2: 0.00318\n",
            "Loss: 3.856609e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.856246e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.856084e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.855899e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.855838e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.855658e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.855411e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.855149e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.854809e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.854793e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.854592e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.854319e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.854186e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.854071e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853976e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853863e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853798e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.853713e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853620e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853555e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.853458e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.853291e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.853160e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.853038e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.852947e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.852844e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.852713e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.852500e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.852418e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.852142e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.851993e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.851817e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.851713e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.851564e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.851459e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.851335e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.851017e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.850777e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.851225e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.850652e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.850471e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.850274e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.850017e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.849623e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.849529e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.849321e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.849230e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.849102e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.849197e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.849009e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.848851e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.848727e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.848586e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.848344e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.848016e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.847605e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.847263e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.847005e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.846755e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.846424e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.846503e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.846135e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.845692e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.845392e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.845121e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.844992e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.845526e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.844919e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.844764e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.845114e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.844687e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.844527e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.844376e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.844209e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.844088e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.843939e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.843755e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.843548e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.843190e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.842905e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.842727e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.842565e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.842266e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.842078e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.841725e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.841611e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.841037e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.840771e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.840555e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.840349e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.840243e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.840083e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.839984e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.839712e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.839341e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.838917e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.838624e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.838258e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.837911e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.837588e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.837306e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.837014e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.836732e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.836408e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.835985e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.841228e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.835828e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.835523e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.835299e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.835235e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.835070e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.834692e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.835893e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.834588e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.834267e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.833856e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.833477e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.833162e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.833549e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.833050e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.832861e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.832689e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.832603e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.832478e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.832300e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.832261e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.831935e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.831795e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.831665e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.831472e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.831316e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.831235e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.831075e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.830997e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.830848e-05, l1: 0.99829, l2: 0.00319\n",
            "Loss: 3.830688e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.830722e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.830581e-05, l1: 0.99829, l2: 0.00319\n",
            "Loss: 3.830338e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.830210e-05, l1: 0.99827, l2: 0.00319\n",
            "Loss: 3.830010e-05, l1: 0.99827, l2: 0.00319\n",
            "Loss: 3.829847e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.829512e-05, l1: 0.99824, l2: 0.00319\n",
            "Loss: 3.829297e-05, l1: 0.99823, l2: 0.00319\n",
            "Loss: 3.828919e-05, l1: 0.99820, l2: 0.00319\n",
            "Loss: 3.828753e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.828848e-05, l1: 0.99821, l2: 0.00319\n",
            "Loss: 3.828638e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.828532e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.828421e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.828321e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.828230e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.828097e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.828009e-05, l1: 0.99814, l2: 0.00319\n",
            "Loss: 3.827921e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827850e-05, l1: 0.99814, l2: 0.00319\n",
            "Loss: 3.827774e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827688e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827535e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827356e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827185e-05, l1: 0.99812, l2: 0.00319\n",
            "Loss: 3.828213e-05, l1: 0.99814, l2: 0.00319\n",
            "Loss: 3.827141e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.827072e-05, l1: 0.99812, l2: 0.00319\n",
            "Loss: 3.826870e-05, l1: 0.99812, l2: 0.00319\n",
            "Loss: 3.827020e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.826730e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.826517e-05, l1: 0.99812, l2: 0.00319\n",
            "Loss: 3.826060e-05, l1: 0.99812, l2: 0.00319\n",
            "Loss: 3.826683e-05, l1: 0.99814, l2: 0.00319\n",
            "Loss: 3.825880e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.825593e-05, l1: 0.99813, l2: 0.00319\n",
            "Loss: 3.825314e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.825228e-05, l1: 0.99815, l2: 0.00319\n",
            "Loss: 3.824951e-05, l1: 0.99816, l2: 0.00319\n",
            "Loss: 3.824813e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.824673e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824506e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824379e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824305e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824249e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824163e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.824025e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.823882e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.823767e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.823697e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.823665e-05, l1: 0.99819, l2: 0.00319\n",
            "Loss: 3.823485e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.823518e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.823399e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.823272e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.823173e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.823082e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.822972e-05, l1: 0.99817, l2: 0.00319\n",
            "Loss: 3.822710e-05, l1: 0.99818, l2: 0.00319\n",
            "Loss: 3.822567e-05, l1: 0.99822, l2: 0.00319\n",
            "Loss: 3.822222e-05, l1: 0.99821, l2: 0.00319\n",
            "Loss: 3.822033e-05, l1: 0.99821, l2: 0.00319\n",
            "Loss: 3.821786e-05, l1: 0.99822, l2: 0.00319\n",
            "Loss: 3.821491e-05, l1: 0.99823, l2: 0.00319\n",
            "Loss: 3.821042e-05, l1: 0.99825, l2: 0.00319\n",
            "Loss: 3.820860e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.820642e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.820438e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.820259e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.820081e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.819846e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.819754e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.819629e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.819520e-05, l1: 0.99827, l2: 0.00319\n",
            "Loss: 3.819307e-05, l1: 0.99827, l2: 0.00319\n",
            "Loss: 3.819194e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.819026e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.818911e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.818754e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.818544e-05, l1: 0.99825, l2: 0.00319\n",
            "Loss: 3.823635e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.818483e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.818355e-05, l1: 0.99826, l2: 0.00319\n",
            "Loss: 3.818129e-05, l1: 0.99828, l2: 0.00319\n",
            "Loss: 3.818007e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.817858e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.817728e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.817536e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.817385e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.817295e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.817054e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.816864e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.816638e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.816360e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.816289e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.816009e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.815693e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.815551e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.815334e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.815019e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.815166e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.814920e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.814766e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.814703e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.814587e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.814459e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.814163e-05, l1: 0.99857, l2: 0.00319\n",
            "Loss: 3.813941e-05, l1: 0.99861, l2: 0.00319\n",
            "Loss: 3.813754e-05, l1: 0.99862, l2: 0.00319\n",
            "Loss: 3.813559e-05, l1: 0.99863, l2: 0.00319\n",
            "Loss: 3.813422e-05, l1: 0.99866, l2: 0.00319\n",
            "Loss: 3.813201e-05, l1: 0.99866, l2: 0.00319\n",
            "Loss: 3.812838e-05, l1: 0.99865, l2: 0.00319\n",
            "Loss: 3.812620e-05, l1: 0.99865, l2: 0.00319\n",
            "Loss: 3.812432e-05, l1: 0.99866, l2: 0.00319\n",
            "Loss: 3.812259e-05, l1: 0.99868, l2: 0.00319\n",
            "Loss: 3.812180e-05, l1: 0.99868, l2: 0.00319\n",
            "Loss: 3.812098e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.812010e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.811879e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.813956e-05, l1: 0.99878, l2: 0.00320\n",
            "Loss: 3.811811e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.811692e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.811536e-05, l1: 0.99873, l2: 0.00320\n",
            "Loss: 3.811453e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.811291e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.811128e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.811059e-05, l1: 0.99873, l2: 0.00320\n",
            "Loss: 3.810778e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.810621e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.810302e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.810005e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.809664e-05, l1: 0.99870, l2: 0.00320\n",
            "Loss: 3.809311e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.809112e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.809152e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.808963e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.808886e-05, l1: 0.99870, l2: 0.00320\n",
            "Loss: 3.808734e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.808679e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.808526e-05, l1: 0.99872, l2: 0.00320\n",
            "Loss: 3.808243e-05, l1: 0.99870, l2: 0.00320\n",
            "Loss: 3.808518e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.808075e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.807725e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.807474e-05, l1: 0.99865, l2: 0.00320\n",
            "Loss: 3.807137e-05, l1: 0.99865, l2: 0.00320\n",
            "Loss: 3.806726e-05, l1: 0.99865, l2: 0.00320\n",
            "Loss: 3.806276e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.807659e-05, l1: 0.99864, l2: 0.00320\n",
            "Loss: 3.806095e-05, l1: 0.99866, l2: 0.00320\n",
            "Loss: 3.805706e-05, l1: 0.99869, l2: 0.00320\n",
            "Loss: 3.805484e-05, l1: 0.99873, l2: 0.00320\n",
            "Loss: 3.805190e-05, l1: 0.99871, l2: 0.00320\n",
            "Loss: 3.805002e-05, l1: 0.99870, l2: 0.00320\n",
            "Loss: 3.804740e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.804571e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.804373e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.804321e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.804128e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.804030e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.803904e-05, l1: 0.99868, l2: 0.00320\n",
            "Loss: 3.803767e-05, l1: 0.99867, l2: 0.00320\n",
            "Loss: 3.803704e-05, l1: 0.99865, l2: 0.00320\n",
            "Loss: 3.803465e-05, l1: 0.99864, l2: 0.00320\n",
            "Loss: 3.803381e-05, l1: 0.99864, l2: 0.00320\n",
            "Loss: 3.803274e-05, l1: 0.99864, l2: 0.00320\n",
            "Loss: 3.803123e-05, l1: 0.99863, l2: 0.00320\n",
            "Loss: 3.802942e-05, l1: 0.99861, l2: 0.00320\n",
            "Loss: 3.802752e-05, l1: 0.99858, l2: 0.00320\n",
            "Loss: 3.802709e-05, l1: 0.99857, l2: 0.00320\n",
            "Loss: 3.802329e-05, l1: 0.99857, l2: 0.00320\n",
            "Loss: 3.802107e-05, l1: 0.99855, l2: 0.00320\n",
            "Loss: 3.801867e-05, l1: 0.99853, l2: 0.00320\n",
            "Loss: 3.801555e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.801256e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.801084e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.800941e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.800792e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.800629e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.800460e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.800187e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.800097e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.799800e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.799697e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.799565e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.799485e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.799560e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.799359e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.799052e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.802225e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.799009e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.798757e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.798514e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.798280e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.798021e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.797683e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.797434e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.797256e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.797158e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.797032e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.796699e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.796586e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.796327e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.796132e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.795957e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.795823e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.795748e-05, l1: 0.99829, l2: 0.00319\n",
            "Loss: 3.795667e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.795578e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.795509e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.795388e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.795836e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.795318e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.795174e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.795038e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.794931e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.794760e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.794550e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.796250e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.794507e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.794375e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.794279e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.794122e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.794072e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.793924e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.793861e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.793795e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.793726e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.793574e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.793474e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.793361e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.793280e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.793192e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.793362e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.793104e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.792873e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.792710e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.792555e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.792383e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.792967e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.792291e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.792072e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.791933e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.791835e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.791711e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.791524e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.791419e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.791245e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.791192e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.791132e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.791066e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.790982e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.790851e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.790758e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.790711e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.790666e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.790629e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.790494e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.790413e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.790290e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.790353e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.790220e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.790086e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789955e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789880e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789791e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789684e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789599e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789700e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789552e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.789461e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.789405e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.789341e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.789240e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.789043e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.788896e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.788789e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.788710e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.788573e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.788445e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.788279e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.788323e-05, l1: 0.99857, l2: 0.00319\n",
            "Loss: 3.788241e-05, l1: 0.99856, l2: 0.00319\n",
            "Loss: 3.788147e-05, l1: 0.99856, l2: 0.00319\n",
            "Loss: 3.788045e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.787951e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.787867e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.787738e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.787606e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.787460e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.787263e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.787129e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.787031e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.786898e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.786760e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.786863e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.786714e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.786599e-05, l1: 0.99852, l2: 0.00319\n",
            "Loss: 3.786439e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.786336e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.786275e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.786201e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.786922e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.786169e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.786074e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.785888e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.785712e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.785585e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.785529e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.785312e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.785128e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.784955e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.784858e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.784678e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.784558e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.784490e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.784397e-05, l1: 0.99848, l2: 0.00319\n",
            "Loss: 3.784181e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.783989e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.783816e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.783694e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.783593e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.783501e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.783338e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.783316e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.783140e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.783072e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.782988e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.782906e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.782740e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.782651e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.782437e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.782280e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.782128e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.781882e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.781674e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.781452e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.781299e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.781078e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.780880e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.780635e-05, l1: 0.99836, l2: 0.00319\n",
            "Loss: 3.780273e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.780088e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779796e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.779614e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779506e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779333e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779183e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779029e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.778983e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.778848e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778814e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778753e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778713e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778645e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778549e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.778512e-05, l1: 0.99834, l2: 0.00319\n",
            "Loss: 3.778365e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.778273e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.778105e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.777943e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.777808e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.779720e-05, l1: 0.99830, l2: 0.00319\n",
            "Loss: 3.777770e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.777688e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.777596e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.778510e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.777554e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.777442e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.777230e-05, l1: 0.99831, l2: 0.00319\n",
            "Loss: 3.777085e-05, l1: 0.99832, l2: 0.00319\n",
            "Loss: 3.776959e-05, l1: 0.99833, l2: 0.00319\n",
            "Loss: 3.776876e-05, l1: 0.99835, l2: 0.00319\n",
            "Loss: 3.776791e-05, l1: 0.99837, l2: 0.00319\n",
            "Loss: 3.776709e-05, l1: 0.99838, l2: 0.00319\n",
            "Loss: 3.776493e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.776310e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.776178e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.776058e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.775924e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.775873e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.775795e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.775648e-05, l1: 0.99843, l2: 0.00319\n",
            "Loss: 3.775385e-05, l1: 0.99844, l2: 0.00319\n",
            "Loss: 3.775153e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.774874e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.774657e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.774566e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.774440e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.774192e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.774249e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.774081e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773880e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773742e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773628e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773518e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773271e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773155e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.773026e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.772936e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.772826e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.772507e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.772306e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.772163e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.772029e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.771907e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.771813e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.771611e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.771476e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.771382e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.771262e-05, l1: 0.99839, l2: 0.00319\n",
            "Loss: 3.771146e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.770947e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.770811e-05, l1: 0.99840, l2: 0.00319\n",
            "Loss: 3.770676e-05, l1: 0.99841, l2: 0.00319\n",
            "Loss: 3.770546e-05, l1: 0.99842, l2: 0.00319\n",
            "Loss: 3.770375e-05, l1: 0.99845, l2: 0.00319\n",
            "Loss: 3.770269e-05, l1: 0.99846, l2: 0.00319\n",
            "Loss: 3.770176e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.770088e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.770634e-05, l1: 0.99849, l2: 0.00319\n",
            "Loss: 3.770047e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769988e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769907e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769820e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769705e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769588e-05, l1: 0.99847, l2: 0.00319\n",
            "Loss: 3.769409e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.769294e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.769193e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.769093e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.768919e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768778e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.768721e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.768640e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768612e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768566e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768514e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768416e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768325e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768252e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.768115e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.768036e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.767953e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.767878e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.767720e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.767488e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.767598e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.767388e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.767165e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.767015e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.766946e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.766854e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.768179e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.766809e-05, l1: 0.99850, l2: 0.00319\n",
            "Loss: 3.766705e-05, l1: 0.99851, l2: 0.00319\n",
            "Loss: 3.766527e-05, l1: 0.99853, l2: 0.00319\n",
            "Loss: 3.766397e-05, l1: 0.99854, l2: 0.00319\n",
            "Loss: 3.766251e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.766636e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.766226e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.766146e-05, l1: 0.99855, l2: 0.00319\n",
            "Loss: 3.766060e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.765943e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765787e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765677e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.765557e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765407e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.765306e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.765317e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765237e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765118e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.765036e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.764965e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.764803e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.764613e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.764543e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.764500e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.764402e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.764361e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.764278e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.764189e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.764082e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.764229e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.764025e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.763818e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.763682e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.763585e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.763525e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.763381e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.763332e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.763263e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.763155e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.762975e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.764026e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.762936e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.762792e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.762644e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.762502e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.762368e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.762260e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.762182e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.762086e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.761997e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.761851e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.761702e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.761606e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.761512e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.761425e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.761319e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.761254e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.761024e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.760734e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.760460e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.760326e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.760144e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.760597e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.760074e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.759974e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.759676e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.759489e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.759268e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.759002e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.758786e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.758492e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.758311e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.758134e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.758000e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.757787e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.757689e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.757500e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.757306e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.757025e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.756779e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.756426e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.760549e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.756345e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.756030e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.755722e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.755444e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.755224e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.755425e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.755037e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.754733e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.754575e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.754535e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.754370e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.754284e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.754157e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.754070e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.753963e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.753836e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.753705e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.753608e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.753643e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.753502e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.753303e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.753198e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.753130e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.753036e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.752896e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.752845e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.752799e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.752751e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.752686e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.752558e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.752387e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.752264e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.752163e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.752072e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.751915e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.751737e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.751611e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.751508e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.751439e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.751309e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.751161e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.750998e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.750863e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.750714e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.750551e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.750701e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.750407e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.750238e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.750019e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.749795e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.749709e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.749600e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.749506e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.749427e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.749299e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.749140e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.749018e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.748900e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.748819e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.748751e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.748658e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.748593e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.748509e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.748363e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.748170e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.748036e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.747917e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.747855e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.747801e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.747770e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.747647e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.747623e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.747564e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.747512e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.747490e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.747415e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.747356e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.747286e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.747170e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.747142e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.747013e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746953e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746915e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746871e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746802e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.747084e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746777e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746689e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746635e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746529e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746894e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746507e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746420e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746335e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746194e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.746072e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.746049e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.745936e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.745835e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.745698e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.745581e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.745403e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.745217e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.745125e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.744978e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.744852e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.744724e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.744581e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.744391e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.744256e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.744130e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.744051e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.743984e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.743890e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.743756e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.743629e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.743499e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.743455e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.743240e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.743149e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.743060e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.742965e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.742923e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.742810e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.742758e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.742663e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.742611e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.742569e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.742506e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.742390e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.742424e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.742343e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.742257e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.742209e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.742134e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.742132e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.742047e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.742006e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.741934e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.741859e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741773e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.741741e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741639e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741531e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741656e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741483e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.741383e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.741328e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.741244e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.741103e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.740970e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.740847e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.740723e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.740643e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.740568e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.740500e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.740382e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.740271e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.740059e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.739908e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.739586e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.739413e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.739278e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.739128e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.738968e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.738764e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.738593e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.738449e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.738376e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.738271e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.738093e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.737955e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737856e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737738e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737603e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737553e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737470e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737404e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737297e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737236e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.737178e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737145e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737073e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.737025e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.736919e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.736800e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.736718e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.736640e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.736572e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.736471e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.736498e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.736361e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.736273e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.736167e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.736136e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.736178e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.736054e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.735899e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.735801e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.735756e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.735573e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.735530e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.735427e-05, l1: 0.99855, l2: 0.00318\n",
            "Loss: 3.735358e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.735196e-05, l1: 0.99851, l2: 0.00318\n",
            "Loss: 3.735184e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.735018e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.734955e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.734864e-05, l1: 0.99849, l2: 0.00318\n",
            "Loss: 3.734736e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.734585e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.734999e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.734505e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.734411e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.734147e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.734004e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.733850e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.733662e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.733402e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.733165e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.732989e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.732878e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.732778e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.732672e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.732570e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.732514e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.732475e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.732421e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.732307e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.732192e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.732100e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.731994e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.731904e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.731751e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.732429e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.731711e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.731614e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.731486e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.731401e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.731280e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.731154e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.731028e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.730976e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730885e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730840e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.730780e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.730685e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.731047e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.730610e-05, l1: 0.99831, l2: 0.00318\n",
            "Loss: 3.730536e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.730471e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730402e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730346e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.730273e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.730156e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730073e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.730002e-05, l1: 0.99828, l2: 0.00318\n",
            "Loss: 3.729897e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.729810e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.729709e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.729634e-05, l1: 0.99825, l2: 0.00318\n",
            "Loss: 3.729601e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.729547e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.729484e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.729373e-05, l1: 0.99826, l2: 0.00318\n",
            "Loss: 3.731165e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.729355e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.729296e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.729203e-05, l1: 0.99827, l2: 0.00318\n",
            "Loss: 3.729107e-05, l1: 0.99829, l2: 0.00318\n",
            "Loss: 3.728992e-05, l1: 0.99830, l2: 0.00318\n",
            "Loss: 3.728906e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.728853e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.728783e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.728710e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.728592e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.728497e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.728466e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.728372e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.728300e-05, l1: 0.99832, l2: 0.00318\n",
            "Loss: 3.728241e-05, l1: 0.99833, l2: 0.00318\n",
            "Loss: 3.728187e-05, l1: 0.99834, l2: 0.00318\n",
            "Loss: 3.728120e-05, l1: 0.99835, l2: 0.00318\n",
            "Loss: 3.727958e-05, l1: 0.99836, l2: 0.00318\n",
            "Loss: 3.727816e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.727570e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.727463e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.727425e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.727387e-05, l1: 0.99837, l2: 0.00318\n",
            "Loss: 3.727327e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.727709e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.727285e-05, l1: 0.99838, l2: 0.00318\n",
            "Loss: 3.727239e-05, l1: 0.99839, l2: 0.00318\n",
            "Loss: 3.727175e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.727161e-05, l1: 0.99840, l2: 0.00318\n",
            "Loss: 3.727087e-05, l1: 0.99841, l2: 0.00318\n",
            "Loss: 3.726989e-05, l1: 0.99842, l2: 0.00318\n",
            "Loss: 3.726946e-05, l1: 0.99843, l2: 0.00318\n",
            "Loss: 3.726845e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.726796e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.726755e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.726713e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.726653e-05, l1: 0.99844, l2: 0.00318\n",
            "Loss: 3.726568e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.726485e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.726479e-05, l1: 0.99847, l2: 0.00318\n",
            "Loss: 3.726392e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.726345e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.726256e-05, l1: 0.99845, l2: 0.00318\n",
            "Loss: 3.726122e-05, l1: 0.99846, l2: 0.00318\n",
            "Loss: 3.725971e-05, l1: 0.99848, l2: 0.00318\n",
            "Loss: 3.725787e-05, l1: 0.99850, l2: 0.00318\n",
            "Loss: 3.725670e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.725628e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.725589e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.725458e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.725259e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.725053e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.724908e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.724786e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.724719e-05, l1: 0.99859, l2: 0.00318\n",
            "Loss: 3.724648e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.724542e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.724431e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.724331e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.724248e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.724183e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.724099e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.724020e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.723962e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.723873e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.723830e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723757e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723691e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723632e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723523e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723435e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.723357e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.723292e-05, l1: 0.99865, l2: 0.00318\n",
            "Loss: 3.723184e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.723039e-05, l1: 0.99867, l2: 0.00318\n",
            "Loss: 3.722929e-05, l1: 0.99867, l2: 0.00318\n",
            "Loss: 3.722867e-05, l1: 0.99867, l2: 0.00318\n",
            "Loss: 3.722804e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722706e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722542e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722413e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722323e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722217e-05, l1: 0.99866, l2: 0.00318\n",
            "Loss: 3.722141e-05, l1: 0.99865, l2: 0.00318\n",
            "Loss: 3.722037e-05, l1: 0.99865, l2: 0.00318\n",
            "Loss: 3.721921e-05, l1: 0.99864, l2: 0.00318\n",
            "Loss: 3.721848e-05, l1: 0.99863, l2: 0.00318\n",
            "Loss: 3.721745e-05, l1: 0.99862, l2: 0.00318\n",
            "Loss: 3.721706e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.721593e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.721462e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.721335e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.721158e-05, l1: 0.99861, l2: 0.00318\n",
            "Loss: 3.720941e-05, l1: 0.99860, l2: 0.00318\n",
            "Loss: 3.721703e-05, l1: 0.99852, l2: 0.00318\n",
            "Loss: 3.720864e-05, l1: 0.99858, l2: 0.00318\n",
            "Loss: 3.720640e-05, l1: 0.99857, l2: 0.00318\n",
            "Loss: 3.720518e-05, l1: 0.99856, l2: 0.00318\n",
            "Loss: 3.720399e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720406e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720355e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720291e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720218e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720201e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720172e-05, l1: 0.99854, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720139e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720116e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720117e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720117e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "Loss: 3.720111e-05, l1: 0.99853, l2: 0.00318\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
            "  Objective function value: 0.000037\n",
            "  Number of iterations: 2993\n",
            "  Number of functions evaluations: 3218\n",
            "Error lambda_1: 0.147206%\n",
            "Error lambda_2: 0.172449%\n",
            "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x7f291309fae8> (for post_execute):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    305\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                                              stderr=subprocess.STDOUT)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 356\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex': 'latex'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# IPython >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf_mgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    392\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2588\u001b[0m                 \u001b[0martists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_title_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxison\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2536\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m                     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m                 \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    291\u001b[0m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[1;32m    292\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             ismath=\"TeX\" if self.get_usetex() else False)\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mmin_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlp_h\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlp_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linespacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0;32m--> 204\u001b[0;31m                 s, fontsize, renderer=self)\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._run_checked_subprocess(\n\u001b[1;32m    339\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[0;32m--> 340\u001b[0;31m                      texfile], tex)\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasefile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dvi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    308\u001b[0m             raise RuntimeError(\n\u001b[1;32m    309\u001b[0m                 \u001b[0;34m'Failed to process string with tex because {} could not be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 'found'.format(command[0])) from exc\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    305\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                                              stderr=subprocess.STDOUT)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 356\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex': 'latex'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                            else suppress())\n\u001b[1;32m   2099\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2588\u001b[0m                 \u001b[0martists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_title_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxison\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2536\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m                     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m                 \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    291\u001b[0m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[1;32m    292\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             ismath=\"TeX\" if self.get_usetex() else False)\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mmin_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlp_h\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlp_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linespacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0;32m--> 204\u001b[0;31m                 s, fontsize, renderer=self)\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._run_checked_subprocess(\n\u001b[1;32m    339\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[0;32m--> 340\u001b[0;31m                      texfile], tex)\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasefile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dvi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    308\u001b[0m             raise RuntimeError(\n\u001b[1;32m    309\u001b[0m                 \u001b[0;34m'Failed to process string with tex because {} could not be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 'found'.format(command[0])) from exc\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 388.543x336.186 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "P9CE3g-5Xdnw",
        "outputId": "c79edc62-da79-4cfd-fd45-ec0077c66f62"
      },
      "source": [
        "    ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################    \n",
        "    \n",
        "    fig, ax = newfig(1.0, 1.4)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    ####### Row 0: u(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')    \n",
        "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])    \n",
        "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 3: Identified PDE ##################    \n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "    ax = plt.subplot(gs2[:, :])\n",
        "    ax.axis('off')\n",
        "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "    s3 = r'Identified PDE (1\\% noise) & '\n",
        "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "    s5 = r'\\end{tabular}$'\n",
        "    s = s1+s2+s3+s4+s5\n",
        "    ax.text(0.1,0.1,s)\n",
        "        \n",
        "    savefig('./figures/Burgers_identification')  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 388.543x336.186 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}