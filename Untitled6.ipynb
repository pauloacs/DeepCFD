{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lhEn6qcwbmwV"
      ],
      "toc_visible": true,
      "mount_file_id": "11fbFTcssPYcEsrqyzGR6ZX2JQv4cVYzp",
      "authorship_tag": "ABX9TyM+gmQ9b93PAGd/MEBjBPuj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloacs/DeepCFD/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhEn6qcwbmwV"
      },
      "source": [
        "# CUSTOM_lbfgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts-MsMIzbbnB"
      },
      "source": [
        "#%% Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p64VmVabtvg"
      },
      "source": [
        "# NN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqLA1Zab7ms"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, hp, logger, ub, lb):\n",
        "\n",
        "        layers = hp[\"layers\"]\n",
        "\n",
        "        # Setting up the optimizers with the hyper-parameters\n",
        "        self.nt_config = Struct()\n",
        "        self.nt_config.learningRate = hp[\"nt_lr\"]\n",
        "        self.nt_config.maxIter = hp[\"nt_epochs\"]\n",
        "        self.nt_config.nCorrection = hp[\"nt_ncorr\"]\n",
        "        self.nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
        "        self.tf_epochs = hp[\"tf_epochs\"]\n",
        "        self.tf_optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp[\"tf_lr\"],\n",
        "            beta_1=hp[\"tf_b1\"],\n",
        "            epsilon=hp[\"tf_eps\"])\n",
        "\n",
        "        self.dtype = \"float32\"\n",
        "        # Descriptive Keras model\n",
        "        tf.keras.backend.set_floatx(self.dtype)\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "        self.model.add(tf.keras.layers.Lambda(lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "        for width in layers[1:-1]:\n",
        "            self.model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh,kernel_initializer=\"glorot_normal\"))\n",
        "        self.model.add(tf.keras.layers.Dense(layers[-1], activation=None, kernel_initializer=\"glorot_normal\"))\n",
        "\n",
        "        # Computing the sizes of weights/biases for future decomposition\n",
        "        self.sizes_w = []\n",
        "        self.sizes_b = []\n",
        "        for i, width in enumerate(layers):\n",
        "            if i != 1:\n",
        "                self.sizes_w.append(int(width * layers[1]))\n",
        "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "        self.logger = logger\n",
        "\n",
        "    # Defining custom loss\n",
        "    @tf.function\n",
        "    def loss(self, u_pred, v_pred, u, v):\n",
        "        return tf.reduce_mean(tf.square(u - u_pred)) + tf.reduce_mean(tf.square(v - v_pred))\n",
        "\n",
        "    @tf.function\n",
        "    def grad(self, X, u, v):\n",
        "        psi_and_p = self.model(X, self.weights, self.biases)\n",
        "        psi = psi_and_p[:,0:1]     \n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            u_pred = tape.gradient(psi, X[0])\n",
        "            v_pred = -tape.gradient(psi, X[1])\n",
        "            loss_value = self.loss(u_pred, v_pred, u , v)\n",
        "        grads = tape.gradient(loss_value, self.wrap_training_variables())\n",
        "        return loss_value, grads\n",
        "\n",
        "    def wrap_training_variables(self):\n",
        "        var = self.model.trainable_variables\n",
        "        return var\n",
        "\n",
        "    def get_params(self, numpy=False):\n",
        "        return []\n",
        "\n",
        "    def get_weights(self, convert_to_tensor=True):\n",
        "        w = []\n",
        "        for layer in self.model.layers[1:]:\n",
        "            weights_biases = layer.get_weights()\n",
        "            weights = weights_biases[0].flatten()\n",
        "            biases = weights_biases[1]\n",
        "            w.extend(weights)\n",
        "            w.extend(biases)\n",
        "        if convert_to_tensor:\n",
        "            w = self.tensor(w)\n",
        "        return w\n",
        "\n",
        "    def set_weights(self, w):\n",
        "        for i, layer in enumerate(self.model.layers[1:]):\n",
        "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "            weights = w[start_weights:end_weights]\n",
        "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "            weights_biases = [weights, biases]\n",
        "            layer.set_weights(weights_biases)\n",
        "\n",
        "    def get_loss_and_flat_grad(self, X, u, v):\n",
        "        def loss_and_flat_grad(w):\n",
        "\n",
        "            psi_and_p = self.model(X, self.weights, self.biases)\n",
        "            psi = psi_and_p[:,0:1]     \n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                self.set_weights(w)\n",
        "                u_pred = tape.gradient(psi, X[0])\n",
        "                v_pred = -tape.gradient(psi, X[1])\n",
        "                loss_value = self.loss(u_pred, v_pred, u, v)\n",
        "            grad = tape.gradient(loss_value, self.wrap_training_variables())\n",
        "            grad_flat = []\n",
        "            for g in grad:\n",
        "                grad_flat.append(tf.reshape(g, [-1]))\n",
        "            grad_flat = tf.concat(grad_flat, 0)\n",
        "            return loss_value, grad_flat\n",
        "\n",
        "        return loss_and_flat_grad\n",
        "\n",
        "    def tf_optimization(self, X_u, u, v):\n",
        "        self.logger.log_train_opt(\"Adam\")\n",
        "        for epoch in range(self.tf_epochs):\n",
        "            loss_value = self.tf_optimization_step(X_u, u, v)\n",
        "            self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    @tf.function\n",
        "    def tf_optimization_step(self, X_u, u, v):\n",
        "        loss_value, grads = self.grad(X_u, u, v)\n",
        "        self.tf_optimizer.apply_gradients(\n",
        "                zip(grads, self.wrap_training_variables()))\n",
        "        return loss_value\n",
        "\n",
        "    def nt_optimization(self, X_u, u, v):\n",
        "        self.logger.log_train_opt(\"LBFGS\")\n",
        "        loss_and_flat_grad = self.get_loss_and_flat_grad(X_u, u, v)\n",
        "        # tfp.optimizer.lbfgs_minimize(\n",
        "        #   loss_and_flat_grad,\n",
        "        #   initial_position=self.get_weights(),\n",
        "        #   num_correction_pairs=nt_config.nCorrection,\n",
        "        #   max_iterations=nt_config.maxIter,\n",
        "        #   f_relative_tolerance=nt_config.tolFun,\n",
        "        #   tolerance=nt_config.tolFun,\n",
        "        #   parallel_iterations=6)\n",
        "        self.nt_optimization_steps(loss_and_flat_grad)\n",
        "\n",
        "    def nt_optimization_steps(self, loss_and_flat_grad):\n",
        "        lbfgs(loss_and_flat_grad,\n",
        "              self.get_weights(),\n",
        "              self.nt_config, Struct(), True,\n",
        "              lambda epoch, loss, is_iter:\n",
        "              self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    def fit(self, X_u, u, v):\n",
        "        self.logger.log_train_start(self)\n",
        "\n",
        "        # Creating the tensors\n",
        "        X_u = self.tensor(X_u)\n",
        "        u = self.tensor(u)\n",
        "        v = self.tensor(v)\n",
        "\n",
        "        # Optimizing\n",
        "        self.tf_optimization(X_u, u, v)\n",
        "        self.nt_optimization(X_u, u, v)\n",
        "\n",
        "        self.logger.log_train_end(self.tf_epochs + self.nt_config.maxIter)\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        u_pred = self.model(X_star)\n",
        "        return u_pred.numpy()\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def tensor(self, X):\n",
        "        return tf.convert_to_tensor(X, dtype=self.dtype)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9NYRbjDdGlA"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-_yerEMedfH"
      },
      "source": [
        "# Load Data\n",
        "import scipy.io\n",
        "\n",
        "def prep_data(path, N_train):\n",
        "\n",
        "  data = scipy.io.loadmat(path)\n",
        "        \n",
        "  U_star = data['U_star'] # N x 2 x T\n",
        "  P_star = data['p_star'] # N x T\n",
        "  t_star = data['t'] # T x 1\n",
        "  X_star = data['X_star'] # N x 2\n",
        "\n",
        "  N = X_star.shape[0]\n",
        "  T = t_star.shape[0]\n",
        "\n",
        "  # Rearrange Data \n",
        "  XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "  YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "  TT = np.tile(t_star, (1,N)).T # N x T\n",
        "\n",
        "  UU = U_star[:,0,:] # N x T\n",
        "  VV = U_star[:,1,:] # N x T\n",
        "  PP = P_star # N x T\n",
        "\n",
        "  x = XX.flatten()[:,None] # NT x 1\n",
        "  y = YY.flatten()[:,None] # NT x 1\n",
        "  t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "  u = UU.flatten()[:,None] # NT x 1\n",
        "  v = VV.flatten()[:,None] # NT x 1\n",
        "  p = PP.flatten()[:,None] # NT x 1\n",
        "  X = np.concatenate([x, y, t], 1)\n",
        "        \n",
        "  lb = X.min(0)\n",
        "  ub = X.max(0)\n",
        "\n",
        "  #TRaining DAta\n",
        "  idx = np.random.choice(N*T, N_train, replace=False)\n",
        "  x_train = x[idx,:]\n",
        "  y_train = y[idx,:]\n",
        "  t_train = t[idx,:]\n",
        "  u_train = u[idx,:]\n",
        "  v_train = v[idx,:]\n",
        "  X_train= np.concatenate([x_train, y_train, t_train], 1)\n",
        "\n",
        "  # Boudanry data\n",
        "  x_1 = np.vstack((lb, ub))\n",
        "  \n",
        "  #Test DATA\n",
        "  snap = np.array([100])\n",
        "  x_star = X_star[:,0:1]\n",
        "  y_star = X_star[:,1:2]\n",
        "  t_star = TT[:,snap]\n",
        "\n",
        "  u_star = U_star[:,0,snap]\n",
        "  v_star = U_star[:,1,snap]\n",
        "  p_star = P_star[:,snap]\n",
        "  X_star= np.concatenate([x_train, y_train, t_train], 1)\n",
        "\n",
        "  return X, X_train, u_train , v_train , x_1, X_star, u_star, v_star, p_star, ub, lb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6DYzM3GdMfh"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7iYtOUtg89-"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsWWHtwtdAR0"
      },
      "source": [
        "#HYPER PARAMETERS\n",
        "hp = {}\n",
        "# Data size on the solution u\n",
        "hp[\"N_u\"] = 5000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "hp[\"layers\"] = [3, 20, 20, 20, 20, 20, 20, 20, 20, 2]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "hp[\"tf_epochs\"] = 100\n",
        "hp[\"tf_lr\"] = 0.001\n",
        "hp[\"tf_b1\"] = 0.9\n",
        "hp[\"tf_eps\"] = None\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "hp[\"nt_epochs\"] = 500\n",
        "hp[\"nt_lr\"] = 0.8\n",
        "hp[\"nt_ncorr\"] = 50\n",
        "hp[\"log_frequency\"] = 10"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly9bdTk2dV3J"
      },
      "source": [
        "#DEFINE THE MODEL\n",
        "\n",
        "class NavierStokesInformedNN(NeuralNetwork):\n",
        "  def __init__(self, hp, logger, ub, lb):\n",
        "    super().__init__(hp, logger, ub, lb)\n",
        "\n",
        "    # Defining the two additional trainable variables for identification\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([0.0], dtype=self.dtype)\n",
        "\n",
        "  def f_model(self,X):\n",
        "\n",
        "    l1, l2 = self.get_params()\n",
        "\n",
        "    psi_and_p = self.model(X)\n",
        "    psi = psi_and_p[:,0:1]\n",
        "    p = psi_and_p[:,1:2]\n",
        "    \n",
        "    with tf.GradientTape(persistent=False) as tape:\n",
        "      u = tape.gradient(psi, y)\n",
        "      v = -tape.gradient(psi, x)\n",
        "      u_t = tape.gradient(u, t)\n",
        "      u_x = tape.gradient(u, x)\n",
        "      u_y = tape.gradient(u, y)\n",
        "      u_xx = tape.gradient(u_x, x)\n",
        "      u_yy = tape.gradient(u_y, y)\n",
        "    \n",
        "      v_t = tape.gradient(v, t)\n",
        "      v_x = tape.gradient(v, x)\n",
        "      v_y = tape.gradient(v, y)\n",
        "      v_xx = tape.gradient(v_x, x)\n",
        "      v_yy = tape.gradient(v_y, y)\n",
        "    \n",
        "      p_x = tape.gradient(p, x)\n",
        "      p_y = tape.gradient(p, y)\n",
        "    \n",
        "    f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
        "    f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy)\n",
        "    \n",
        "    return u, v, p, f_u, f_v\n",
        "\n",
        "  def loss(self, X, u, v):\n",
        "    u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.f_model(self.X)\n",
        "    return tf.reduce_sum(tf.square(self.u - self.u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.v - self.v_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_v_pred))\n",
        "\n",
        "\n",
        "  def wrap_training_variables(self):\n",
        "    var = self.model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = super().get_weights(convert_to_tensor=False)\n",
        "    w.extend(self.lambda_1.numpy())\n",
        "    w.extend(self.lambda_2.numpy())\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    super().set_weights(w)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "        return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def fit(self, X_u, u, v):\n",
        "    self.X_u =  tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    super().fit(X_u, u, v)\n",
        "\n",
        "  def predict(self, x_star, y_star, t_star):\n",
        "    u_star, v_star, p_star, _ , _ = self.f_model(self, x_star, y_star, t_star)\n",
        "    return u_star, v_star, p_star\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHFdWPTz0Y4"
      },
      "source": [
        "# LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csR7qBcaz2fV"
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, hp):\n",
        "        print(\"Hyperparameters:\")\n",
        "        print(json.dumps(hp, indent=2))\n",
        "        print()\n",
        "\n",
        "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.prev_time = self.start_time\n",
        "        self.frequency = hp[\"log_frequency\"]\n",
        "\n",
        "    def get_epoch_duration(self):\n",
        "        now = time.time()\n",
        "        edur = datetime.fromtimestamp(now - self.prev_time) \\\n",
        "            .strftime(\"%S.%f\")[:-5]\n",
        "        self.prev_time = now\n",
        "        return edur\n",
        "\n",
        "    def get_elapsed(self):\n",
        "        return datetime.fromtimestamp(time.time() - self.start_time) \\\n",
        "                .strftime(\"%M:%S\")\n",
        "\n",
        "    def get_error_u(self):\n",
        "        return self.error_fn()\n",
        "\n",
        "    def set_error_fn(self, error_fn):\n",
        "        self.error_fn = error_fn\n",
        "\n",
        "    def log_train_start(self, model, model_description=False):\n",
        "        print(\"\\nTraining started\")\n",
        "        print(\"================\")\n",
        "        self.model = model\n",
        "        if model_description:\n",
        "            print(model.summary())\n",
        "\n",
        "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "        if epoch % self.frequency == 0:\n",
        "            name = 'nt_epoch' if is_iter else 'tf_epoch'\n",
        "            print(f\"{name} = {epoch:6d}  \" +\n",
        "                  f\"elapsed = {self.get_elapsed()} \" +\n",
        "                  f\"(+{self.get_epoch_duration()})  \" +\n",
        "                  f\"loss = {loss:.4e}  \" + custom)\n",
        "\n",
        "    def log_train_opt(self, name):\n",
        "        print(f\"-- Starting {name} optimization --\")\n",
        "\n",
        "    def log_train_end(self, epoch, custom=\"\"):\n",
        "        print(\"==================\")\n",
        "        print(f\"Training finished (epoch {epoch}): \" +\n",
        "              f\"duration = {self.get_elapsed()}  \" +\n",
        "              f\"error = {self.get_error_u():.4e}  \" + custom)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojJ1FJVRgtyN"
      },
      "source": [
        "import scipy.io\n",
        "path='/content/cylinder_nektar_wake.mat'\n",
        "data = scipy.io.loadmat(path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dJ1CESikfZq"
      },
      "source": [
        "# TRAIN THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P-XtP31Ckhmx",
        "outputId": "7e3f1be8-dc35-40aa-dba7-fb46df61c9ea"
      },
      "source": [
        "# Getting the data\n",
        "path='/content/cylinder_nektar_wake.mat'\n",
        "X, X_train, u_train , v_train , x_1, X_star, u_star, v_star, p_star, ub, lb = prep_data(path, hp[\"N_u\"])\n",
        "lambdas_star = (1.0, 100) #true values\n",
        "\n",
        "# Creating the model\n",
        "logger = Logger(hp)\n",
        "pinn = NavierStokesInformedNN(hp, logger, ub, lb)\n",
        "\n",
        "\n",
        "# Defining the error function and training\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_train, u_train, v_train)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "{\n",
            "  \"N_u\": 5000,\n",
            "  \"layers\": [\n",
            "    3,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    20,\n",
            "    2\n",
            "  ],\n",
            "  \"tf_epochs\": 100,\n",
            "  \"tf_lr\": 0.001,\n",
            "  \"tf_b1\": 0.9,\n",
            "  \"tf_eps\": null,\n",
            "  \"nt_epochs\": 500,\n",
            "  \"nt_lr\": 0.8,\n",
            "  \"nt_ncorr\": 50,\n",
            "  \"log_frequency\": 10\n",
            "}\n",
            "\n",
            "TensorFlow version: 2.4.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: False\n",
            "\n",
            "Training started\n",
            "================\n",
            "-- Starting Adam optimization --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-36aa9f2be77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_lambda_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merror_lambda_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5ea8356e77e7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, v)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_u\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b6fcc6de96d3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_u, u, v)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# Optimizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b6fcc6de96d3>\u001b[0m in \u001b[0;36mtf_optimization\u001b[0;34m(self, X_u, u, v)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_optimization_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3885\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    <ipython-input-2-b6fcc6de96d3>:116 tf_optimization_step  *\n        loss_value, grads = self.grad(X_u, u, v)\n    <ipython-input-2-b6fcc6de96d3>:48 grad  *\n        psi_and_p = self.model(X, self.weights, self.biases)\n\n    AttributeError: 'NavierStokesInformedNN' object has no attribute 'weights'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZqznCdoywci"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}